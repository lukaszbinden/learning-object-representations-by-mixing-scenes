\documentclass[12pt,a4paper]{article}

\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{appendix}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage[boxed]{algorithm2e}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii,
sorting=nty,
%style=alphabetic
%style=reading
style=ieee,
backref,
citestyle=ieee
]{biblatex}
\DefineBibliographyStrings{english}{%
  backrefpage = {page}, % originally "cited on page"
  backrefpages = {pages}, % originally "cited on pages"
}
\addbibresource{bibliography.bib}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{  \tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{Learning object representations by mixing scenes}

\author{Lukas Zbinden\\
Computer Vision Group\\
Computer Science Department, University of Bern\\ 
2019\\
{\tt\small lukas.zbinden@unifr.ch}
}

\maketitle
%\thispagestyle{empty}

%=========================================================================
%%%%%%%%% ABSTRACT
\begin{abstract}
   Our project 
   abstract: in 4 sentences: state the problem; say why it's an interesting problem; say what your solution achieves; say what follows from your solution
   Do not use custom terminology in the abstract;
use terms and concepts that are widely accepted. Make it easy to see the main point!
\end{abstract}

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
% \listoftables
% \newpage

%%%%%%%%% BODY TEXT
% \chapter{Introduction}
\section{Motivation}
The dawn of deep learning in 2012 with the introduction of the groundbreaking AlexNet neural network reignited the idea of an intelligent computer that is on a par with the human brain. After much disappointment in the late 21st century, the decade old research field of artificial intelligence and machine learning in particular was revived around the globe. While today research is still far away from the lofty aim of a truly intelligent machine, impressive achievements have been published continuously (cite?). One research area in machine learning where progress has stood out in recent years is computer vision. The field of computer vision aims at equipping a machine with artificial eyesight to process and interpret visual signals such as images and video. For instance, the computer has learned to classify objects in images with an accuracy that exceeds human performance or detects skin cancer with a higher precision than doctors. Other significant applications such as autonomous driving or medical image analysis have emerged from the ability to artificially understand visual data and presumably many more are yet to be discovered. Within computer vision, one area of research focuses on the representation of visual data such as objects or attributes thereof in lower dimensional space, the latent or feature space. Considering that an image resides in a high dimensional space, the aim of representation learning in the context of computer vision is to learn a model that successfully transforms a given image into a lower dimensional representation that is meaningful and captures the significant image features in condensed form. This type of dimensionality reduction opens up an array of new possibilities for downstream applications that make use of latent space~\footnote{or embedding space} representations. Furthermore, some research imposes additional constraints on the low dimensional representation to lay the groundwork for more fine-grained methods. This research is called disentangling factors of variation. A factor of variation represents an image object or image attribute in latent space. This research aims to learn a latent space representation that is not only low dimensional but has an inherent structure that includes disentangled, independent factors of variation. With such a structure at hand, latent space interpolations become possible where objects or attributes are swapped, altered or used to perform basic arithmetic operations. This work centers around the challenging problem of learning disentangled representations and disentangling factors of variation, respectively, given image data.

TODO: argue about unsupervised learning.
TODO: argue why DL and not explicit math for this problem (-> no rigidity, no formalism, no direct math applicable)

We propose a novel machine learning model capable of learning object representations by mixing scenes. Given two real images containing scenes, the model is expected to generate a new realistic scene that mixes the content of the two inputs. For instance, the two scenes could be pictures taken in a park with several persons and dogs in each. The generated image will in turn also represent a scene in a similar looking park with people and dogs mixed together from both input scenes. The model extracts objects from both scenes and incorporates them in a new made-up scene. The mixing of the objects is done in the latent space based on the extracted object representations.

The proposed solution is expected to generate based on the input images a new scene containing a mixture of the input content including objects and background. The model is able to perform this operation because it has learned to extract objects from images into well structured representations. An object representation consist of multiple independent factors of variation, each representing an image attribute in latent space. This allows the model to compose a crossbreed representation with selectively chosen factors of variation from multiple extracted object representations. This mixed representation is then decoded into a new scene image. Thus the model encodes a given image into an object representation consisting of disentangled factors of variation which makes it possible to swap or move objects around within it or mix it with other representations into a crossbreed and thereby manipulating and controlling the content of the subsequent generated image. 
Intuitively, the key idea is that...

The learned object representations open up a variety of possible downstream applications such as image search, latent space interpolations or visualization of high-dimensional data. The proposed model could be used for fine-grained image search using nearest neighbor search. The algorithm would receive an image showing some objects and using the model would extract the objects in the latent space as object representations and features, respectively. Continuing, the algorithm would run the search in the feature space for images having a similar feature. This search would work without requiring any labels and find images containing similar objects as the query image. Further, latent space interpolation AT WORK WED 02.01...

The model is trained in an unsupervised fashion without any human interference. Our work aims to produce a representation that is made up of well disentangled factors of variation representing image objects and object attributes, respectively. Experiments showcase the potential of the model through qualitative and quantitative evaluation of the generated samples.

\subsection{Challenges}
TBD list expected challenges for the model
\begin{enumerate}
  \item unsupervised disentanglement
  \item a dataset with aligned data is a desirable property for a machine learning model because TODO: why?. For instance, CelebA is a face dataset where an image contains exactly one face which is often center aligned. In contrast, the proposed model is applied to a natural, heterogenous dataset in which the data (e.g. objects) is inherently not aligned. By design however, the model makes the assumption that a given image contains up to four objects where each object is approximately aligned in one of the quadrants. The underlying dataset will therefore pose a substantial challenge to the model in the learning process
\end{enumerate}

\subsection{Contributions}
The contributions of our work are summarized as follows:
\begin{enumerate}
  \item a novel architecture to learn object representations in an unsupervised framework
  \item a novel visual similarity detection algorithm
  \item neural architecture engineering (e.g. autoencoder with DenseBlocks, instance and spectral normalization), hyperparameter tuning
  \item An implementation of the proposed dataset based on MS COCO
  \item An implementation of the proposed method
  \item A study of the capabilities and limitations of the proposed model
\end{enumerate}

% ...........................................................................................
% The introduction serves a twofold purpose. Firstly, it
% gives the background on and motivation for your research,
% establishing its importance. Secondly, it gives a summary
% and outline of your paper. When you write the background review, you should
% consider including technological trends of the area, open
% problems and recent promising developments.
% A proper flow is to first set the context,
% then present your proposal, then provide the verification,
% and lastly wrap up with conclusions
% 
% - says "I am going to look at the following things"
% - introduce area of study - representation learning, disentangling factors of variation
% - Describe the problem, state your contributions and the implications
% - say what is really new in your work
% - tell what your work is about, what problem it solves, why the problem is interesting, what is really new in your work, why it's so neat
% - set the proper context for understanding the rest of your paper!
% - introduce your idea using EXAMPLES and only then present the general case, explain it as if you were speaking to someone using a whiteboard;
% ...........................................................................................

learned representations, data space

disentangling factors of variation in image data. 

feature representation vs object representation
feature = numeric representation of raw data
feature space is a vector space
feature = high dimensional vector
a collection of data points then becomes a point cloud in the feature space 
model = mathematical "summary" of features (summary -> a gemoetric shape)
model = geometric summary of point cloud
point cloud = approximate geometric shape
disentangling image features into semantically meaningful properties

"a separate set of dimensions" = a feature chunk

Devise a manifold learning algorithm that can discover and capture independent factors of variation.

\par Concretely, we used a ... to model the distribution over image features and the latent factors of variation.

% - from [12]
Representation learning: Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsupervised learning algorithms such as autoencoders seek efficient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables. 

% -- from [26]
While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. For example, for a dataset of faces, a useful disentangled representation may allocate a separate set of dimensions for each of the following attributes: facial expression, eye color, hairstyle, presence or absence of eyeglasses, and the identity of the corresponding person. A disentangled representation can be useful for natural tasks that require knowledge of the salient attributes of the data, which include tasks like face recognition and object recognition.

%=========================================================================
% \chapter{Background}
\section{Related work}
% Secondly, it provides a critique of the
% approaches in the literatureâ€”necessary to establish the
% contribution and importance of your paper. 
% Critiquing the major approaches of the background
% work will enable you to identify the limitations of the
% other works and show that your research picks up where
% the others left off.
In this section, we first give a brief overview of neural networks. Firstly, we start with MLPs followed by autoencoders and CNNs as two special types of neural networks. Secondly, we give an introduction to GANs and a number of variations thereof. Finally, we review the two research areas this work is rooted in, namely \textit{representation learning} in general and \textit{unsupervised disentangling factors of variation} in particular. For the latter, we give an overview of previous major approaches along with their characteristics to establish the contribution and importance of our work.


\subsection{Multilayer perceptron (MLP)}\label{subsec:mlp}
The multiplayer perceptron (MLP) is a generic and probably the simplest type of neural network that consists of an input and an output layer and one or more hidden layers in between. Each layers neurons are connected with all neurons of the previous layer. An important capability of an MLP is stated by the universal approximation theorem~\cite{mlpUnivApprox} which basically says a feed forward neural network with at least one hidden layer, sufficient neurons and a suitable activation function (i.e. an MLP) can approximate any possible continuous function. We can thus exploit the expressive power of neural networks to tackle problems and tasks yet unsolved and unachieved, respectively. The MLP is a typical example of a deep learning model.
TODO mention: MLPs are usually trained with stochastic gradient descent.

A reason why we do not always go to an MLP to accomplish a task is their exponential demand in computational and memory resources as the size of the network grows (i.e. number of neurons). For instance, each layer has a fixed number of neurons each of which is connected to all neurons of the previous layer which results in a considerable amount of weights per neuron and equally many computations in the forward as well as the backward pass while training. While the architecture is simple in its structure it is inflexible in some respects. Every neurons' receptive field spans the entire input and it is not possible to limit the receptive field of a neuron (cf. \ref{sec:rf}). MLPs are great for approximating many non-linear functions but not necessarily a good choice for computationally and memory intensive tasks such as image processing. For the latter case, a convolutional neural network (CNN) is a more suitable choice. We describe more specific types of neural networks such as autoencoders, CNNs and GANs next.


\subsection{Autoencoder}
\subsubsection{Overview}
Autoencoder is a type of neural network used for unsupervised learning. More specifically, an autoencoder learns in a self-supervised manner where the labels (or targets) are generated from the input data. In a standard autoencoder the ground truth is exactly the input because the autoencoder tries to reproduce the input as close as possible. TODO what is it used for? what do we use it for? -> "In our work we also use them as feature extractors.". An autoencoder consists of an encoder network $f_{\theta}$ and a decoder network  $g_{\theta}$ as depicted in Figure~\ref{fig:autoencoder}. The encoder takes a high dimensional input instance $x$ and encodes it into a lower dimensional latent space representation $z$ (or compressed representation). Its challenge is to encode as much of the relevant information from the input into the compressed representation $z$ (while leaving the irrelevant information out) in order to allow the decoder to reconstruct the input into $\hat{x}$ as accurately as possible.
\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/autoencoder_schema.png}
\caption{Overview of an autoencoder \cite{chollet_autoencoders}.}
\label{fig:autoencoder}
\end{figure}
A common objective function for an autoencoder is to minimize the pixel-wise reconstruction loss $\Vert x - \hat{x} \Vert^2$. Both networks are trained jointly using stochastic gradient descent. Note that this architecture does not enforce any kind of structure in the latent representation.

TODO mention that encoder/decoder can be impl by MLPs or convnets
TODO perhaps mention "embedding space"

Autoencoder plays an important role in the work by Hu \textit{et al.}~\cite{DisentFacOfVarByMixTh}. In particular, they make use of a \textit{mixing autoencoder} which is described next.

\subsubsection{Mixing Autoencoder}\label{subsec:mixAE}
A mixing autoencoder, a term coined in \cite{DisentFacOfVarByMixTh} and introduced by Reed \textit{et al.}~\cite{DeepVisAnaMak}, refers to a neural network architecture with an autoencoder where multiple instances of the encoder are used in parallel to project a set of data points to latent space. Notably all encoder instances share one set of weights. The latent space representations are then "mixed" in some way and finally projected back onto data space using the decoder. As with the encoder there can exist multiple instances of the decoder and with shared weights. An example is shown in figure \ref{fig:mixing_autoencoder}. 
\begin{figure}[ht]
\centering
\includegraphics[scale=0.7]{images/mixing_autoencoder.png}
\caption{Schematic of a mixing autoencoder \cite{DisentFacOfVarByMixTh}.}
\label{fig:mixing_autoencoder}
\end{figure}
Mixing autoencoders provide an architectural prior TODO korrekt? for disentangling latent representations and is used as part of the respective method in \cite{DeepVisAnaMak}, \cite{DisentFacOfVarByMixTh} as well as in our work.


%=========================================================================
%=========================================================================
% CONVOLUTIONAL NEURAL NETWORK (CNN)
\subsection{Convolutional Neural Network (CNN)}
\subsubsection{Evolution}
%-- CNN start -------------------------------
Convolutional neural networks (CNNs), also called \textit{convnets}, are among the most common type of neural networks. With respect to multilayer perceptron (MLP) networks and image processing, CNNs are much more time and space efficient as described later. As a result, CNN are ubiquitous in computer vision applications. In 1979, Fukushima~\cite{FukushimaCnn79} and in 1989, Le Cun \textit{et al.}~\cite{CunGrounworkCNNs} laid significant groundwork for CNNs~\footnote{this is not a complete account of the history of CNNs}. But it was not until 2012, when Krizhevsky \textit{et al.}~\cite{AlexNet} released a CNN model which went on to win the annual \textit{ImageNet} large-scale visual object recognition challenge~\cite{Imagenet} with an unprecedented accuracy and by a significant margin, that CNNs gained widespread research attention in fields such as object detection [17, 2], image recognition [10, 8], semantic segmentation [12, 1] and image captioning [20] and therein achieved new state-of-the-art performances over hand-crafted techniques. CNNs continue to do so today (from [59]).
\par [62] Mention "feature extractor, object detector"

\subsubsection{Architecture and design}
The design of a CNN architecture is not a rigorously defined procedure but rather an art depending on the task to solve. For instance, given an image recognition task a CNN is often followed by a classifier in the form of a small MLP MENTION-ALTERNATIVE. A CNN architecture is the result of an assembly of a number of individual components and layers, respectively. Those are (re-)used across a wide range of different types of CNNs. Considering Figure~\ref{fig:simple_cnn_arch}, we describe relevant CNN components along with their roles within the architecture  next. 
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/simple_cnn_architecture.png}
\caption{A simple CNN architecture \cite{cnn_arch_article}.}
\label{fig:simple_cnn_arch}
\end{figure}

\paragraph{Convolutional layer} Performs the convolution operation between its input, which is either the input image itself or the output of the previous layer, and its kernels. The kernels are the set of weights that belong to a layer. The result of the convolution is called the feature map or activation map and stores the information if and where a certain feature occured in the input. The orchestration of a number of convolutional layers will result in the ability of the CNN to detect higher level features such as objects. See Section~\ref{subsec:innerworkconvl} for more details. A convolutional layer can learn a linear function ELABORATE.

\paragraph{Kernel/filter            } Holds the weights of a convolutional layer. The kernel is shared between all neurons of the convolutional layer unlike in a fully connected layer where each neuron has its own sets of weights. The number of kernels a layer has (a hyperparameter) depends on the chosen depth of the output volume such that the number of kernels equals the number of output channels. The kernel is applied to the input by using a dot product. One dot product results in a scalar which represents exactly one value in the resulting feature map. The size of the kernel is a hyperparameter and is often 1, 3 or 5. Intuitively, each kernel is used to learn to detect a certain feature across the input volume.

\paragraph{Stride                   } Defines the step size of the kernel as it slides across the input volume. It is often 1 or 2. With a stride greater than 1 the spatial output dimension will always be smaller than the input. Thus a convolutional layer can also be used to reduce the spatial dimension of an input as an alternative to the pooling layer.

\paragraph{Padding					} The padding controls the output size of the feature map of a convolutional layer. If the padding is zero (in ML frameworks often denoted by \textit{VALID}), the kernel stays inside the input dimension and the output size will be smaller. A padding greater than zero (in ML frameworks often denoted by \textit{SAME}) causes the input volume to be padded with zeros such that the output size of the convolution is the same as the input size.

\paragraph{Normalization layer      } Oftentimes a convolutional layer is followed by a normalization layer before the activation function. Normalization of the activations of a convolutional layer helps to make the learning process of the network more effective ELABORATE. Common normalization types are batch norm REF, instance norm REF and spectral norm \cite{SNGAN}. Which type to use depends on the problem task at hand. Note that not always is the use of a normalization layer effective. For instance, in an encoder aiming at disentangling factor of variation, batch normalization will be counterproductive as it ... ELABORATE. Instance normalization might be more appropriate in that case.

\paragraph{Non-linearity layer (activation function)} A non-linearity layer consist of an activation function which takes the feature maps of a convolutional layer and processes them element-wise to produce the activation map~\footnote{feature map and activation map are often used interchangeably in the literature}. The activation function enables the CNN to learn non-linear functions ELABORATE. Without the activation function, a sequence of convolutional layers would collapse into one linear function. Common activation function types are Sigmoid, Tanh, Softmax and ReLU. 

\paragraph{Pooling layer            } The pooling layer's sole responsibility is to reduce the spacial size of the activation maps. The idea is to drop irrelevant information so as to keep the most important activations only, thereby to focus on the "essence" of a feature and getting rid of the rest to ultimately perform tasks based on very compact but highly decisive information. Common types are max pooling and average pooling. The output of a pooling layer will be the same independently of where a specific feature is located inside its pooling region as the maximum or the average will be the same. This contributes to the translation-invariant property of convnets.
\par Note that the pooling operation also helps to build the abstraction hierarchy in the convnet as it drops irrelevant information and thus makes the retained information more "abstract". 

\paragraph{Fully connected layer    } As illustrated in Figure~\ref{fig:simple_cnn_arch} the feature maps resulting from the convolutional layers are passed to a fully connected layer. In fact the last three layers represent an MLP as described in~\ref{subsec:mlp} except that the input is not a vector but the activation maps (a 3D tensor) from the previous layer. The fully connected layers act as a classifier that receives the high level features from the convolutional layers as input and outputs a class probability distribution representing the class predictions for the input image. Note that a fully connected layer with $n$ neurons is equivalent to a convolutional layer with $n$ 1x1 kernels.

ELABORATE Why the CNN is more time and space efficient.


\subsubsection{Inner workings of a convolutional layer}\label{subsec:innerworkconvl}
The convolutional layer is at the core of a CNN. For simplicity we will only consider image data as input to a CNN. This implies that any input is three dimensional $h \times w \times c$ where $h$ is the height of the image, $w$ the width and $c$ the number of channels, which for colored images is 3 and for grey images 1.  In subsequent layers however $c$ can take on any number and represents the number of feature and activation maps, respectively, that a layer outputs. The convolutional layer is the main building block of a CNN. It receives as input a three dimensional tensor and produces as output a three dimensional tensor. Note that each dimension in the output can be different from the respective dimension in the input. However, often the spatial dimensions are kept the same, $h = w$. To showcase the inner workings of a convolutional layer $i$ consider Figure~\ref{fig:conv_layer}. 
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/convolutional_layer.png}
\caption{A convolutional layer $i$ with its input and output volume. By convolving each of the 15 kernels $k_i$ across the input volume the 15 activation maps of the output volume result. Note that as depicted one convolution operation $f * k$ produces exactly one value in one activation map. $f$ is the receptive field of $s$ and equals the size of $k$.}
\label{fig:conv_layer}
\end{figure}
Layer~$i$ receives as input the output of the previous layer~$i-1$ and produces itself an output which becomes the input to the subsequent layer $i+1$. $c$ in the output volume refers to the number of output channels and activation maps, respectively. Each activation map (also feature map) represents the result of convolving (or sliding) each of the kernels (also filter) across each input channel of dimension $h \times w$ and captures the feature the kernel has learned to detect (e.g. an edge, a texture, an object). If a kernel detects edges, the activation map will only be non-zero if the input volume contains an edge that activates the kernel. Because a convolutional layer is \textit{translation-invariant}, that is, the same kernel is convolved over the entire spatial dimension of the input volume, it will detect any edge irrelevant of its spatial location. Note that the depth of the kernel always equals the depth of the input volume because the convolution operation, that is the dot product of the receptive field of the kernel in the input volume with the kernel, always spans the entire depth dimension. 

With each subsequent layer, the convnet can detect more elaborate features. In the first few layers of the network the features detected are low level such as edges or curves. As these features are combined and processed together in subsequent layers, the features learned become more high level such as objects and even later represent abstract concepts such as faces, arms, legs \cite{DeepVis}.

A sequence of convolutional layers as in Figure~\ref{fig:conv_layer} represents a series of linear functions which is again a linear function. In order to allow it to model non-linear functions, each convolutional layer is followed by an \textit{activation function}, of which \textit{Rectified Linear Unit (ReLU)} is a prevalent example.

\par TODO: Where exactly lies the computational and memory advantage? CNNs: As most of parameters exist in fully connected (FC) layers

\subsubsection{AlexNet: a seminal CNN OR DENSENET?}
As a complete example, consider the convnet called \textit{AlexNet}~\cite{AlexNet} in Figure~\ref{fig:AlexNet}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/alexnet-cnn-architecture-layers.png}
\caption{The AlexNet convnet \cite{AlexNet}.}
\label{fig:AlexNet}
\end{figure}
AlexNet consists of 8 layers with weights where the first 5 are convolutional layers followed by a classifier consisting of 3  fully connected layers. The input is an image of size $224 \times 224 \times 3$. The spatial dimension of the input is reduced to $13 \times 13$ before it is passed to the classifier. 

\subsubsection{Receptive field and its implications}\label{sec:rf}
In fully connected networks, the output value of any neuron depends on the entire input to the network, that is all the pixels in case of an image input. In contrast, the output value of a neuron~\footnote{a CNN neuron is equivalent to the dot product of a filter of a convolutional layer with the input tensor} in a CNN only depends on a region of the input image, the so called \textit{receptive field} (RF), or \textit{field of view}, for that neuron. Only the region defined by the receptive field of a neuron impacts the output value of that neuron. Anywhere outside that region does not affect the outcome. Note that in case of an MLP the receptive field of any neuron always spans the entire input image. We assume the RF is square shaped and the size of each side is given in pixels. The receptive field of a neuron can be considered with respect to the previous layer or with respect to the input image (or any of the previous layers). Considering only the previous layer, the receptive field of the current layer is equal to its filter size. It follows that only if the filter size of a layer is greater than 1x1 will the receptive field increase. Assuming that is the case for all layers, the network starts off with a very small receptive field that allows it to learn only low level features such as edges or curves. The next layer then looks at a combination of receptive fields from the previous layer which means a larger receptive field. As the network goes deeper, the receptive field continues to increase and so each neuron of the next layer looks at yet a larger region of the input image and is therefore capable of learning more abstract features such that the network can eventually learn high level features such as objects or regions. In conclusion, with respect to the input image, the receptive field size of any layer in the CNN is a function of the number of previous layers along with the filter size and stride of each layer \cite{li_recept_field_2017}.

What needs to be ensured wit respect to the receptive field when designing a convolutional network is that it covers the entire relevant region of the input image. What the relevant region is depends on the application.
In case of a classification, object detection or a segmentation problem the receptive field of the CNN should span the entire input image.  The size therefore can be too small when important information to the problem is left out, for instance smaller than the size of the object to detect. Likewise, the field can be as big as the input image or larger such that each output value in the last layer is affected by the whole input image. With such a large field it might be difficult for a CNN to disentangle multiple objects in the input image if every pixel of the input image maps to every element of the feature vector. 

In short it is crucial to consider the receptive field and its size, respectively, with respect to the problem at hand when designing a convolution network.


\subsubsection{Unsupervised training of CNNs}\label{subsec:deepcluster}
CNNs are often trained in a supervised setting where human annotated labels are available to drive the learning process. A major challenge in the realm of unsupervised learning is the lack of labels to direct the learning process. In supervised learning the availability of labels as ground truth (usually by human annotation) allows us to define loss functions that measure the accuracy of the prediction with respect to the label and thereby providing an error signal to perform backpropagation with and realize the learning process, respectively. Using labels we can easily train a CNN as a regression or classification task. In unsupervised learning however this is not as straightforward since ground truth is nonexistent. The task is still to train a CNN but in an unsupervised manner. A widespread method in unsupervised learning is clustering. A recent approach using clustering is presented in the work by Caron \textit{et al.} \cite{DeepCluster}. Their novel \textit{DeepCluster} technique is used to perform unsupervised training of CNNs. In a first step, a (randomly initialized) convnet without the classifier is taken to produce the features for all images from a large dataset by simple forward passing. Those features are then grouped using the clustering algorithm \textit{k}-means into \textit{k} clusters. Notice that when comparing the images of the features, the features belonging to the same cluster will also appear similar in the image space on a semantic level but not necessarily on a pixel level. Each of those clusters then becomes the pseudo-label for all the features belonging to that cluster. In a second step, with the classifier appended to the convnet, the convnet is trained in a supervised manner using the pseudo-labels, cross-entropy loss and backpropagation. The convnet has to predict the correct cluster for a given image. After each epoch, this two step process is repeated for a number of times. 
\par What results is a trained convnet that is capable of extracting visual features from images and achieves notable performance in many standard transfer tasks such as classification or object detection.
%-- CNN end -------------------------------


%=========================================================================
% GAN
%=========================================================================
\subsection{Generative Adversarial Network (GAN)}
%-- GAN start -------------------------------
This type of neural network, the so-called Vanilla GAN, was introduced in 2014 by \cite{1406.2661} and has inspired a significant amount of research and progress. It is called ``the coolest idea in deep learning in the last 20 years'' by AI pioneer Yann LeCun and ``a significant and fundamental advance'' by AI luminary Andrew Ng. The GAN network defines a learning framework for synthetic image generation in which two neural networks, usually convolutional, namely the generator $G$ (a generative model) and the discriminator $D$ (a discriminative model), compete against each other in a so called minimax game. The framework includes a dataset from which two things result. Firstly, the dataset's images are used by the discriminator to learn to discriminate real from fake images. Real images come from the dataset, fake images come from the generator. Secondly, the dataset is defined by an implicit probability data distribution $p_d$ which all its samples follow. The generator's objective is to learn and approximate, respectively, the data distribution $p_d$ and by that generate new, realistic images that look similar to samples from the dataset. Thus the generator implicitly defines a probability model distribution $p_g$ as the distribution of the samples obtained from the generator given some input $z$ (e.g. a Gaussian noise). In other words, the discriminator learns to determine whether a sample is from $p_d$ or $p_g$ and the generator learns to invent samples from $p_g$ that closely ressemble samples from $p_d$ and ultimately confuse $D$ in its decision.
\par \textbf{Minimax game} The minimax game comes into play in the learning phase. The two models are simultaneously trained so that $G$ learns to generate samples that are hard to classify by $D$, while $D$ learns to discriminate the samples generated by $G$. As training evolves, $D$ forces $G$ to produce even better samples. This setting can be viewed as a two-player game where both players (models) try to minimize their own loss and the final state of the game is the Nash equilibrium where both players cannot improve their loss anymore. Ideally, when $G$ is fully trained, it should not be possible for $D$ to perform any better than randomly guessing. The two-player minimax game is formally defined as
\begin{equation} \label{eq:1}
\min\limits_{G} \max\limits_{D} V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}\big[log\, D(x)\big] + \mathbb{E}_{z\sim p_{z} (z)}\big[log(1 - D(G(z)))\big]
\end{equation}
where $z$ is a random noise. $G$ tries to minimize $log(1 - D(G(z))$ (or maximize $D(G(z))$) such that $D$ gives a high probability for input $G(z)$, while $D$ tries to maximize $log(D(x)) + log(1 - D(G(z))$ with a high probability for $x$ and a low probability for $G(z)$. The two networks are trained in an alternating manner: for every training batch, one network is fixed and the other is updated so that backpropagation happens in one network at a time exclusively. In an ideal convergence scenario, $G$ has learned to produce samples so realistic that $D$ cannot discern them from real images and assigns probability 0.5 to every image (Nash equilibrium). In this state, $D$ cannot teach $G$ anymore how to improve, convergence has set in. Note that the expectation value is practically the average over the training images x.

\par \textbf{GAN loss functions} 
Multiple formulations of the GAN loss functions exist (\cite{GANLandscape}). In this work we focus on the non-saturating GAN loss introduced in \cite{1406.2661} as the empirical evaluation in \cite{GANLandscape} suggests to favor this version when applying GANs to a new dataset. The hinge loss version has also shown good performance in \cite{SNGAN} but \cite{GANLandscape} concludes that it performs very similar to the non-saturating loss.

The discriminator loss function is
\begin{equation} \label{eq:d_loss}
    L_{D} = -\mathbb{E}_{x\sim p_{data} (x)}\big[log(D(x))\big] - \mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(1 - D(\hat{x}))\big]
\end{equation}
where $p_{data}$ denotes the (true) data distribution and $p_{model}$ the model distribution. It is equivalent to the binary cross entropy between the data and the model distributions of real and generated images ($D$ has to discriminate between two classes).

The generator loss function is
\begin{equation} \label{eq:g_loss}
    L_{G} = -\mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(D(\hat{x}))\big]
\end{equation}
where the generator creates samples $\hat{x} = G(z)$ which the discriminator believes to be real $D(\hat{x}) \approx 1$ and thereby trying to approach $log(1) = 0$.
%-- GAN end -------------------------------

\par \textbf{Joint loss function}
Similar to our work, a number of other methods (\cite{1511.05440},\cite{DisentFacOfVarByMixTh},\cite{1604.07379}) use a joint loss function for the generator which is composed of two or more losses and oftentimes includes a reconstruction loss and an adversarial loss $L_G = \lambda_{rec}L_{rec} + \lambda_{adv}L^G_{adv} + \lambda_{i}L_{i}$ where $i$ is a method specific additional loss. To look up the weightings in other works proves useful to find reasonable values more efficiently. For instance, \cite{1604.07379} uses $\lambda_{rec} = 0.999$ and $\lambda_{adv} = 0.001$ on ImageNet whereas \cite{DisentFacOfVarByMixTh} deploys $\lambda_{rec} = 30$ and $\lambda_{adv} = 1$ on CelebA.

%-- [24] DCGAN start -------------------------------
\par \textbf{DCGAN} \cite{1511.06434} introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate for the first time that they are a strong candidate for purely unsupervised learning. The authors propose a more stable set of architectures for training generative adversarial networks and give evidence that adversarial networks learn good representations of images. According to Ian Goodfellow, DCGAN defines a quasi-standard generator network architecture. 
%-- [24] DCGAN end -------------------------------

%-- [26] INFOGAN start -------------------------------
\par\textbf{INFOGAN} \cite{1606.03657} describes InfoGAN, a type of GAN that learns disentangled representations in an unsupervised manner (no supervision of any kind) with the addition of maximizing the mutual information between a small subset of the latent variables and the observation. InfoGAN can disentangle both discrete and continuous latent factors, scale to complicated datasets, and typically requires no more training time than regular GANs.  The effective results suggest that generative modelling augmented with a mutual information cost be a fruitful approach for learning disentangled representations. They used DCGAN for implementation and stable GAN training, respectively. Gist: concat noise vector $z$ with latent vector $c$ to form input for generator G. Additionally, extend discriminator D with Q-network on top to generate $\hat{c}$ to then calculate the reconstruction loss with $c$ and train entire network accordingly using backprob (aside the standard GAN training). (In [29] gibts auch eine kurze Zusammenfassung) TODO In this work, we share a similar goal, but we
directly train a...
%-- [26] INFOGAN end -------------------------------

%-- [32] SAGAN start -------------------------------
\par \textbf{SAGAN} \cite{1805.08318} introduces self-attention to the GAN framework, enabling both the generator and the discriminator to efficiently model relationships between widely separated spatial regions. Convolution processes the information in a local neighborhood only, thus using convolutional layers alone is computationally inefficient for modeling long-range dependencies in images. TODO We share a similar motivation when...
%-- [32] SAGAN end -------------------------------

%-- [29] DNA-GAN start -------------------------------
\par \textbf{DNAGAN} In \cite{DnaGan}, Xiao \textit{et al.} propose the supervised DNA-GAN by using an analogy with the DNA double helix structure, where different kinds of traits are encoded in different DNA pieces. In the same way, they argue that different visual attributes in an image are controlled by different pieces in the latent representation of that image. Similar to our model, for two parallel input images, they use an encoder to retrieve their representations and a decoder to generate new images whose realism is judged by a discriminator (GAN). Additionally, they use labels to support the disentanglement in the latent space where image pairs are required to have different labels for a certain attribute (e.g. smiling or not). The respective part of a label in the latent representations of the image pair is then swapped to obtain two crossbreed representations and images, respectively. TOOD Our model can also...
%-- [29] DNA-GAN end -------------------------------

%-- [31] SNGAN start -------------------------------
\par \textbf{SNGAN} \cite{SNGAN} presents a highly effective method for stabilizing GAN training and helping to prevent mode collapse, respectively, the \textit{spectral normalization}. TODO 19.02 elaborate
%-- [29] SNGAN end -------------------------------

TODO 26.02 evtl add style gan

%=========================================================================
%=========================================================================
% Representation learning
\subsection{Representation learning}
Representation learning~\cite{ReprLearning} or feature learning~\footnote{we use \textit{representation}, \textit{feature} and \textit{feature vector} interchangeably} is a field within machine learning that aims at automatically discovering and extracting representations from raw data~\footnote{i.e. real world data such as images, video and sensor data} essential for many downstream tasks in AI. A good representation is one which contains the discriminative information from the input data in a well organized way and in a much lower dimension. Such a representation will leverage predictors or classifiers as they can extract decisive information more easily. With respect to real world data, attempts to extract features with rigorous explicit algorithms have not been as promising as approximative approaches with neural networks. Significant in using deep learning is that the system can automatically perform feature engineering which is a labor-intensive process (including feature extraction, feature importance and feature selection) when hand-crafted. This allows to construct new applications faster and allows for more progress towards an artificial intelligence system that can understand the world around it.  

In short, some essential properties of a good representation are \textit{distributed}, \textit{invariant} and \textit{disentangled factors of variation}. Note that there is yet no commonly accepted definition of a disentangled representation \cite{SpatialBDecoder}.

TODO: elaborate on distributed representations. See [55] and [15].
TODO elaborate on invariant representation

An important prior for representations of real world data is that there are \textit{multiple explanatory factors} or \textit{factors of variation} that underlie the data generating distribution. An additional prior is that there exists a \textit{hierarchical organization of explanatory factors} in which more abstract concepts to describe the real world are defined in terms of less abstract ones lower in the hierarchy. Therein also lies an advantage of using deep architectures as they can potentially learn more abstract factors at higher layers. More abstract has the advantage that it is more \textit{invariant} (i.e. robust) to local changes in the input (e.g. a cat is still a cat no matter the surrounding scene or the change in lighting). Therefore, a representation that comprises highly abstract concepts becomes a highly non-linear function of the raw input.

To identify and disentangle the explanatory factors is an important objective of representation learning and is further detailed in Section~\ref{subsec:unsupDFoV}. Achieving this, as \cite{ReprLearning} proposes, should result in a good representation significantly more robust to the complex variations in natural data. 

Essentially, the learned representation is usually associated with a latent vector such as in an autoencoder. Autoencoders provide a suitable framework for representation learning. In our work we use mixing autoencoders (cf. \ref{subsec:mixAE}) to drive the learning of object representations.

Note that representation learning can be carried out in supervised and unsupervised form. In unsupervised learning only unlabelled data is used to learn a representation. Our method is fully unsupervised.
% that exposes important semantic features as distinctly decodable factors


% TODO: Read slides from Paolo again: http://www.cvg.unibe.ch/media/teaching/course/slides/Advanced%20Topics%20in%20Machine%20Learning/Representation_Learning.pdf
% TODO: also check video/slides from winterschool:
% https://drive.google.com/file/d/1MWY93qIDP9Y6sTNuJONT002NUXU_Tpz-/view
% http://www.winterschool.inf.unibe.ch/
% TODO: add some examples of representation learning models are ... vgl [29]


%=========================================================================
% Manifold learning and latent space interpolation
%=========================================================================
\subsection{Manifold learning and latent space interpolation}
Manifold is an important concept in machine learning based on which concepts such as latent space, latent vector, representation and latent space interpolation can be better understood. 

A \textit{manifold} can be considered a connected region in the latent space of a manifold learning algorithm. It has a much smaller dimensionality than the original data space. This is exploited in, for instance, autoencoders as is the case with our model. A manifold represents the input data in a condensed space with an intrinsic coordinate system that can be traversed and whose coordinates (i.e. points) can be projected back onto input space. As a loose definition, a manifold consists of a connected set of points where each point is surrounded by a neighborhood of other points~\cite{DeepLearningBook}. The existence of a neighborhood should allow for transformations to move on the manifold from one point to a neighboring one. The dimensionality of the manifold can vary from one point to another (e.g. manifold intersections). However, often the manifold, embedded in a higher-dimensional space, uses only a small number of the data space dimensionality. Each dimension of the manifold corresponds to a local direction of variation. This implies when a dimension (or multiple) of a manifold latent vector is changed arithmetically (i.e. linearly) the right amount, a transformed latent vector will result that also lies on the manifold and that may yield non-linear changes in the output image~\cite{StyleGAN}. This is where latent space interpolation comes into play. 

\textit{Manifold learning} is based on the assumption that data probability mass is highly concentrated along a low-dimensional manifold (or a collection thereof) where all the valid inputs are projected to. This implies that most inputs from the data space yield latent space coordinates which lie outside a manifold and are irrelevant, respectively. Interesting latent space interpolations and variations in the output, respectively, will occur only if we move along a direction that lies on the manifold or move from one manifold to another~\cite{DeepLearningBook}.

The \textit{manifold hypothesis} states that the probability distribution over images, text strings, and audio that occur in real life is highly concentrated as well~\cite{ReprLearning}. It is straightforward that an image created with pixel values uniformly distributed will unlikely result in a natural image. Also, it is likely that images resembling each other strongly in image space such as pairs of images with slightly different rotated objects will also be reachable in latent space by traversing the corresponding manifold. The concept of latent space interpolation is about traversing a manifold.

\textit{Latent space interpolation} is not a well defined operation. It relies on the vague notion of a "semantically meaningful combination" of latent vectors and also depends on the dataset. Essentially, a latent space interpolation is commonly implemented as a convex combination~\footnote{a linear combination of points where all coefficients are non-negative and sum to 1} of two latent vectors $z_1, z_2$ as follows: $\hat{x}_{\alpha} = g_{\theta}(\alpha z_1+(1-\alpha)z_2)$ where $\alpha \in [0,1]$, $g_{\theta}$ is a decoder and $\hat{x}_{\alpha}$ the decoded output~\cite{AE_Interp}. An ideal interpolation scenario with synthetic data is depicted in Figure~\ref{fig:interpolation_ideal}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{images/interpolation.png}
\caption{An ideal interpolation scenario \cite{InkscapeInterpolation}. As the latent vector of the original image on the left is interpolated and traversed on the manifold, respectively, the output image morphs into the original image on the right and vice versa.}
\label{fig:interpolation_ideal}
\end{figure}

Sample results of interpolations between the natural image features are shown in Figure~\ref{fig:interpolation_real}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{images/latent_space_interpolations_paper_42.png}
\caption{Interpolations with features of real images \cite{InterpolExamples}. Top row shows input images.}
\label{fig:interpolation_real}
\end{figure}


%=========================================================================
%=========================================================================
% UNSUPERVISED DISENTANGLING FACTORS OF VARIATION
\subsection{Disentangling factors of variation}\label{subsec:unsupDFoV}
Apart from being distributed and invariant, good representations should also capture factors of variation in the input space and disentangle (i.e. separate) them into different independent features as part of the representation~\cite{ReprLearning}. A \textit{factor of variation} (or \textit{explanatory factor}) refers to a distinct and hidden source of influence with respect to the input data. A factor of variation explains part of the data. For instance, when analyzing an image of a car, the factors of variation include the position of the car, its color, and the angle and brightness of the sun~\cite{DeepLearningBook}.

One of the difficulties of a representation learning algorithm to disentangle factors of variation is when many of the factors influence a lot of the data observed. For instance, many natural images are composed of multiple objects with several light sources and shadows overlapping in complex ways. Here the question arises how the objects can be disentangled from their shadows. An important measure to take, as~\cite{ReprLearning} suggests, is to use as much data as possible to support learning representations that separate the various explanatory factors. In addition to using a large quantity of examples, deep learning is one of the foremost approaches to process complex natural data due to its ability to learn high-level abstract features from raw data by building complex concepts out of simpler concepts. 

Further, it is important to distinguish between learning invariant representations and learning representations with disentangled explanatory factors. Invariant representations have reduced sensitivity to variation in the data that are irrelevant to the task at hand. But these representations may be intended for multiple tasks which makes it difficult to learn them beforehand. Thus~\cite{ReprLearning} concludes that the most robust approach to representation learning is to \textit{disentangle as many factors as possible, discarding as little information about the data as is practical}.

A \textit{disentangled representation} can be described as one which separates the factors of variation and explicitly represents the important attributes of the data \cite{Framework4QuantEvalDisRepr}. In a disentangled representation, each section capturing a factor of variation is invariant to changes in sections representing other explanatory factors.

Some applications for disentangled representations are \cite{ChallengeAssLofDisRepr}:
\begin{itemize}
  \item directly build a predictive model instead of using the high dimensional input data
  \item use for learning of downstream tasks, transfer learning and few shot learning
  \item use for tasks involving latent space interpolations (as in our work)
  \item use when good interpretability is required
\end{itemize}
Ultimately, the goal is to learn representations that are invariant to irrelevant changes in the data~\cite{FwkQuantEvalDisRep}.

\par \textbf{Related work.} Many works exist that have disentangled factors of variation in raw data. We present a number of significant previous contributions towards unsupervised disentangling next and relate them to our work. Note that many prior works on disentangling factors of variation are fully supervised. They use labels for all factors of variation to be disentangled. In contrast, our method is fully unsupervised with unlabelled data only. Furthermore, many previous methods work on synthetic datasets or datasets with aligned data, that is one single centered object per image. Our method, however, faces the challenge of disentangling factors of variation in natural, unaligned data.

% [12] Previous approaches to disentangling factors of variation in data include content vs. style, form vs. motion or facial expression vs. identity.

%-- [12] Discovering Hidden FoV in DN -- start -------------------
\par Cheung \textit{et al.}~\cite{DiscHiddenFoViDN} augments autoencoders with regularization terms during training. They use an unsupervised cross-covariance penalty (XCov) as a method to disentangle class-relevant signals (observed variables) from other factors in the latent variables along with a standard supervised cross-entropy loss. In case of MNIST they consider the class label as a high-level representation of its corresponding input. Their model learns a class invariant smooth continuous latent representation $z$ that encodes digit style (slant) whereas the observed variable $\hat{y}$ represents the digit itself. On the TFD dataset, the observed variable $\hat{y}$ encodes the facial expression while the autoencoder is able to retain the identity of the faces through latent variable $z$. Here the XCov penalty prevents expression label variation from â€˜leakingâ€™ into the latent representation.
%-- [12] end -------------------------------

%-- [40] start
\par Eastwood and Williams~\cite{FwkQuantEvalDisRep} propose a framework for the quantitative evaluation of disentangled representations when the ground-truth factor of variation is available. The framework is limited to synthetic datasets. The authors state that reliable disentanglement is far from solved even in the restricted setting of the framework. Further, they state that there is at that time no quantitative benchmark for disentangled representation learning available. A more recent work \cite{SpatialBDecoder} uses the Mutual Information Gap (MIG) metric to quantitatively evaluate disentanglement in representations. MIG requires the underlying factors of variation to be known and is therefore not applicable to evaluate an unsupervised method.
%-- [40] end

%-- [54] start -------------------------------LZ
\par Watters \textit{et al.}~\cite{SpatialBDecoder} present an architecture called "Spatial Broadcast Decoder" leveraging a variational autoencoder. It aims at improving disentangling representations, reconstruction accuracy and generalization. Their key idea is to use a different architecture for the decoder other than the common deconvolutional network. Instead, they broadcast (replicate) the latent space vector across the spatial dimensions and concatenate two X- and Y-"coordinate" channels to the third dimension. Then a fully convolutional decoder network with 1x1 stride is applied to reconstruct the input. With this architectural novelty they address the problem for a deconvolutional network to render an object at a particular position and argue that with the spatial broadcast decoder this becomes a simple task. Experiments on synthetic datasets show encouraging results, however they do not demonstrate their method on more complex, natural data. The conclusion states that this decoder improves latent representations, most dramatically for datasets with objects varying in position. The latter is an interesting feature as our method has to cope with non-aligned objects.
%-- [54] end -------------------------------

%-- [55] start -------------------------------LZ
\par Greff \textit{et al.} \cite{BindingRC} describe the binding problem, a problem in the field of representation learning. It refers to ambiguities and interferences occuring in distributed representations of multiple objects from the same input. It can arise when multiple objects stemming from the same input are to be represented and disentangled at the same time. A proper disentanglement of objects might not be achieved. Where images contain only one object at a time the binding problem is avoided. They argue that the use of convolutions also mitigates the problem. Further, they propose an unsupervised method that explicitly models inputs as a composition of multiple objects. It dynamically binds features of different objects together and recovers these using the notion of mutual (pixel) predictability. Based on the latter, their framework uses clustering and a denoising autoencoder to iteratively reconstruct the input. Even though they show promising results on binary images from artificial datasets (e.g. dSprites), experiments on natural image data are left as future work.
%-- [55] end -------------------------------

TODO summarize a few significant previous unsupervised methods here... (skim through paper list )
% TODO make sure to sum "UNIT: Unsupervised Image-to-Image Translation Networks, Jan Kautz https://arxiv.org/abs/1703.00848"
% TODO 27.02: add INFOGAN, VAE, beta-VAUE, (DIP-VAE?)
% TODO 27.02: paper [37] â€Learning to Segment via Cut-and-Pasteâ€: auch erwÃ¤hnen da Ã¤hnliches Ziel: we're trying to "move objects around independently of their background"
% TODO summarize [22] "Disentangling factors of variation in deep representations using adversarial training"

% TODO: evtl. tabelle machen mit Ã¼bersicht und einordnung der verschiedenen methoden ???
% TODO: Some popular models for unsupervised disentangling factors of variation are INFOGAN, $\beta$-VAE. Supervised variants are DNAGAN, ... (vgl. [29]) 
% Consider "Towards a Definition of Disentangled Representations" [56]

%=========================================================================
\subsubsection{Disentangling Factors of Variation by Mixing Them}
Our method is significantly based on the work of Hu \textit{et al.}~\cite{DisentFacOfVarByMixTh} and pushes its idea further. First we summarize the results of \cite{ChallengInDisentIFoF} and \cite{UnderstDegenAndAmbInAT} as a theoretical background for \cite{DisentFacOfVarByMixTh} and then present the method itself. 

%-- [28] start -------------------------------
Szabo~\textit{et al.}~\cite{ChallengInDisentIFoF} identify two main challenges in disentangling factors of variation: the \textit{shortcut problem} and the \textit{reference ambiguity}. In the shortcut problem the model learns degenerate encodings in which all information is encoded only in one part of the feature, i.e. either in the component $c$ for common attributes shared by both images or the component $v$ for varying attributes. For an image pair there are two $v$ and one $c$ in latent space. The encoder maps a complete description of its input into vector $N_v$ and the decoder completely ignores vector $N_c$.
The second challenge, reference ambiguity, occurs when the attribute representation for an image is not guaranteed to follow the same interpretation on another image. In other words, the reference in which a factor is interpreted may depend on other factors which makes the attribute transfer~\footnote{i.e. the replacement of a feature chunk, which exposes an image attribute, by the same chunk from another image} ambiguous. For example, the viewpoint angle of a vessel gets interpreted in two different ways depending on the boat type.
%-- [28] end -------------------------------
%-- [16] start -------------------------------
\par The two challenges identified in~\cite{ChallengInDisentIFoF} are then addressed by Szabo~\textit{et al.}~ \cite{UnderstDegenAndAmbInAT}. They introduce an adversarial weakly supervised training method that uses image triplets and fully tackles the shortcut problem by means of a composite loss function consisting of an autoencoder loss and an adversarial (i.e. GAN) loss. Used in conjunction this function provably averts the shortcut problem. Concerning our work, this implies that the shortcut problem cannot occur in theory as our method is based also on a composite loss function. Furthermore, they analyze the reference ambiguity and prove that it is unavoidable when disentangling with only weak labels. The problem occurs when a decoder reproduces the data without satisfying the disentangling properties for the varying attribute $v$. Practically, this means not all the factors of variation can provably be disentangled from weakly labeled data (i.e. when only $c$ is known for image pairs).    
%-- [16] end -------------------------------

\par \textbf{Disentangling Factors of Variation by Mixing Them.} Hu~\textit{et al.}~\cite{DisentFacOfVarByMixTh} present a novel unsupervised method to disentangle factors of variation without any data knowledge. The factors of variation correspond to image attributes such as the pose or color of objects. The disentangled representation consists of a number of feature chunks where each chunk represents a factor of variation. The disentanglement is attained by a neural architecture including a mixing autoencoder, a classifier and a GAN. They use a composite loss function including a reconstruction loss, an adversarial loss and a classification loss. The mixing autoencoder is used to enforce invariance which means that each image attribute is encoded into a distinct feature chunk such that it is invariant to changes in other chunks and likewise for the decoding part. The GAN establishes the adversarial training to ensure that the decoded images from the mixed features follow approximately the data generating distribution. To avoid the shortcut problem a classification constraint ensures that each feature chunk corresponds to a discernible image attribute such that degenerate encodings cannot occur.

The differences between \cite{DisentFacOfVarByMixTh} and our method are outlined in Table~\ref{tab:diffHuVsLorbms}.
\begin{table} [H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Concern & Theirs & Ours\\
\hline
Feature space & attribute transfer & object transfer\\
\hline
Input data  & 2 images & 5 images\\
\hline
Dataset & single object, aligned & multiple object, unaligned\\
\hline
Classifier & per feature decide source image & per image decide quadrant \\
\hline
\end{tabular}
\caption{Differences between \cite{DisentFacOfVarByMixTh} and our method.} \label{tab:diffHuVsLorbms}
\end{table}



% TBD...
% Notice that their feature space is designed for "attribute transfer" while in our work the feature space enables "object transfer".
% TODO In this work, we share a similar goal as Hu \textit{et al.}, but we use a natural dataset and we mix objects rather than object attributes. ...
% 
% \par They disentangle and swap attributes, we disentangle and swap objects
% 
% \par image attribute - factor of variation - feature vector - feature chunk
% an object representation is learned as a concatenation of feature chunks
% uses high-dimensional feature chunks
% feature space is only designed for attribute transfer but not for sampling
% how is disentanglement achieved? 
% -> invariance objective i.e. an architecture with mixing autoencoders encourages disentangled encoding of attributes and disentangled decoding of feature chunks, respectively
% -> classification objective such that each feature chunk corresponds to a discernible attribute in the original image 
% encoding of an attribute, decoding of a feature chunk
% 



%=========================================================================
% Problem statement and Method:
% LEARNING OBJECT REPRESENTATIONS BY MIXING SCENES
%=========================================================================
% \chapter{Learning object representations by mixing scenes}
% \section{Conception}
\section{Learning object representations by mixing scenes}
\subsection{Motivation}
% TODO: make simple system overview diagram using images as in [25] Figure 2
% TODO: use images in D:\learning-object-representations-by-mixing-scenes\src\results\dump_mixes\20190221_133431_dump_mixes5cols or gen. new
Inspired by the approach of Hu \textit{et al.}~\cite{DisentFacOfVarByMixTh}, our idea is the following:     The basic motivation, the intent of the system, is that we would like to be able to move objects around by editing the feature space. The purpose in the end is to build a feature space representation where I take a feature from one and plug it there and then the rendering adapts the content - that's the ultimate goal.
\par image attribute - factor of variation - feature vector - feature chunk
\par an object representation is learned as a concatenation of feature chunks

\par [62] the spatial configuration of objects

\par the ULTIMATE OBJECTIVE of this is: we want to encode an image and get a feature representation that has such well disentangled features (2D) and attributes (3D) that it makes it possible to take this representation and move features or attributes around within in and therefore move or change objects in the feature space (manifold?)

\par DLB: We would like learning algorithms to be able to discover and disentangle such manifold coordinates

\par WICHTIG: an important assumption of the model is that each image consists of 4 quadrants where each contains at most one object.

\par What architectural priors do I have in LORBMS? "Network architectures certainly reflect strong prior beliefs about the nature of the problem they will be applied to."

\par TODO: explain that the model has to tackle a very difficult problem because of the natural image data set: "A major source of difficulty in many real-world artificial intelligence applications
is that many of the factors of variation influence every single piece of data we are
able to observe. The individual pixels in an image of a red car might be very close
to black at night. The shape of the carâ€™s silhouette depends on the viewing angle.
Most applications require us to disentangle the factors of variation and discard the
ones that we do not care about."

\par Can we discover structure in the data? Can we learn the latent space of the data and then interpolate through it?
% https://www.youtube.com/watch?time_continue=718&v=Tu3FqCD7-BY

%%%%%%
% TODO: Put good effort in realizing all explicit and implicit assumptions that you make, and clearly state them.
%%%%%%

\par our solution is an unconditional generative model...

\par Our mixed representation can also be called a "compositional representation" cf [54] Our architecture encourages compositional representations. " A compositional representation consists
of components that can be recombined, and such recombination underlies generalization"

\par Show what is used during training and what not during testing (e.g. discriminator)

\par and a GAN is used to improve the realism of the results

\par LEARN A MODEL THAT IDENTIFIES OBJECTS IN THE SCENE AND LEARNS TO COMPOSE THEM SO YOU CAN MERGE SCENES.

\par https://arxiv.org/pdf/1803.06414.pdf "Learning to Segment via Cut-and-Paste": We propose and formalize a new cut-and-paste adversarial training scheme for box-supervised instance segmentation, which captures an intuitive prior, that objects can move independently of their background.
-> perhaps use latter sentence as idea for the intuition that we're trying to "move objects around independently of their background"

\par what is the theoretical groundwork for the proposed method?

\par We use a GAN to check for realism of the generated images.

\par This subsection explicates the reason ...

\par A baseline could also be pure chance s.a. 50 percent chance..

\par an encoder for computing a low-dimensional latent representation, and a decoder for synthesizing the output image.

how do I enforce invariance between object representations and object attribute representations, respectively?
x3 should be a valid image according to the input data distribution.

The classifier consists of $3x3x8$ multi-class classifiers, one for each chunk, that decide for each chunk the original image ($x_1$, $x_2$) and the specific tile, respectively, that was used to generate a tile of the composite image.

(Hinweis: [12] has nice formulation of cost function, perhaps use as inspiration)

\par To generate the unbalanced mask $m$ with a bias towards the first of the two contexts, we used a Bernoulli distribution with $p=0.6$ and $q=1-p=0.4$, respectively, such that $Pr(X=1)=0.6$ and $Pr(X=0)=1-P(X=1)=0.4$ where $1$ represents the first context (i.e. image $x_1$) and $0$ the second context (i.e. image $x_2$).

\par One big drawback of GANs is the notorious instability of the discriminator during training. During training of modified DCGAN encountered lack of GAN training, i.e. the generator loss was immediately 0, the discriminator loss 27.6 ($dsc_loss_real$ was 0 and $dsc_loss_fake$ was 27.6 (meaning the discriminator always output 1 no matter the input). To counteract this problem we implemented and used spectral normalization for GANs (also SNGAN, \cite{SNGAN}) to regularize the Lipschitz constant. With the latter in place, the GAN training set in. We first added to the discrminator, and later according to TTUR paper also in the autoencoder. Note that spectral normalization does not use extra weights, therefore comes at no memory cost.
Important: also mention link to Lipschitz continuity: "They applied spectral normalization to the weights in both generator and discriminator, unlike the previous paper which only normalizes the discriminator weights. They set the spectral norm to 1 to constrain the Lipschitz constant of the weights. Itâ€™s just used for controlling the gradients. This spectral normalization idea was first introduced by Miyato et. al." https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b

\par In high dimensional spaces, embeddings?

\par Some methods are based on new mathematical models and others based on intuition back up by experiments.

\par In \cite{1611.03383} Vielleicht als Inspiration fÃ¼r 'Qualitativ evaluation': "we performed nearest neighbor retrieval in the learned embedding spaces... We computed the corresponding representations for all samples (for the unspecified component we used the mean of the approximate posterior distribution) and then retrieved the nearest neighbors for a given query image"


\subsection{Model architecture}
In this section, we formally present our method. A fundamental assumption is that a real world image $I_{ref}$ contains four objects, ideally one in each quadrant. The method considers $I_{ref}$ as made up of four quadrants $I^{q1}_{ref}, I^{q2}_{ref}, I^{q3}_{ref}, I^{q4}_{ref}$ where each quadrant contains either one object or none. In addition to $I_{ref}$, the method receives four other "quadrant replacement" images $I_{q1},I_{q2},I_{q3},I_{q4}$ where each represents a candidate for the replacement of the respective quadrant in $I_{ref}$. See section \ref{subsec_img_clustering} for more details on "quadrant replacement" images. A replacement of one or more quadrants in an image is considered a mixing of two scenes with the result of a new scene. Note that the replacement of quadrants and mixing of scenes, respectively, exclusively takes place in the latent space of the model as is described later. These per-quadrant replacement images have been preselected in the algorithm outlined in section \ref{subsec_img_clustering} and underlie the same assumptions as $I_{ref}$. In total the model receives five input images simultaneously.

The model architecture is depicted in Figure~\ref{fig:model_arch}. Its main components are:

\begin{itemize}
   \item a GAN comprised of a generator $G$ and a discriminator $D$ for adversarial training
   \item an autoencoder including encoder $Enc$ and decoder $Dec$ for image generation
   \item a classifier $Cls$ to encourage disentanglement within the feature vector of an image
   \item a latent space scene mixing algorithm (see section \ref{subsec_img_mixing_algo})
\end{itemize}
 
The GAN and the adversarial training, respectively, is used to generate realistic images representing the mixed scenes. Note that the generator $G$ is synonymous for the entire autoencoder. The autoencoder functions as the generator in the GAN framework. That means in training when $G$ is updated during backpropagation, what actually gets updated is the encoder and the decoder. The GAN generator therefore is used twofold: Firstly, using $Enc$ it encodes the input image into a disentangled feature vector that is then mixed with other feature vectors according to the scene mixing algorithm in \ref{subsec_img_mixing_algo}. Secondly, given the mixed feature vector, the generator, using $Dec$, generates a new image $I_{mix}$ which is then evaluated by the discriminator as real or fake. Additionally, the classifier receives $I_{mix}$ to decide for each quadrant whether its content originates from a replacement image or not and thereby provide a feedback to the autoencoder to what degree it's quadrant disentanglement encoding and decoding is coherent in the output. Only if the encoder well disentangles the quadrants in the feature space can the decoder generate an image that represents a semantically correct scene where the respective quadrants can be rediscovered (by the classifier) at the same location as in the original position.

The generator is composed of the encoder and the decoder. The encoder $Enc$ maps an image $A$ into a latent space representation
\begin{equation} \label{eq:2}
    Enc(A) = [a_1, a_2, a_3, a_4] = f_A
\end{equation}
where $f_A$ represents the feature vector of image $A$ which is a concatenation of a fixed number of feature chunks $a_i$. See Figure~\ref{fig:encoder} for a schematic representation. Each chunk represents the respective image quadrant $q_i$ in the feature space. $a_i$ is meant to exclusively contain a lower dimensional representation of the object in $q_i$ such that this chunk is invariant to modifications in the neighboring chunks. Hence $a_i$ represents a disentangled factor of variation of the input image. At the same time it can be viewed as the DNA part of one quadrant of the output image.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/model_encoder.png}
\caption{The encoder maps image $A$ to feature vector $f_A$. Chunk $a_i$ represents the quadrant $q_i$ in latent space.}
\label{fig:encoder}
\end{figure}

By swapping some of the feature vector chunks we obtain a new, mixed representation $f_{I_{mix}} = [a_1, b_2, c_3, d_4]$ where $a_1,b_2,c_3,d_4$ represent quadrants from possibly different input images. Using decoder $Dec$, said representation is projected back into image space as
\begin{equation} \label{eq:3}
    Dec(f_{I_{mix}}) = I_{mix}
\end{equation}
where $I_{mix}$ represents a new, realistic scene containing a mixture of objects from one up to four images. See section \ref{subsec_img_mixing_algo} for more details on how the mixing takes place.

The decoder is not only used to generate new images $I_{mix}$. It is also used to reconstruct images $\hat{A} = Dec(Enc(A))$ to facilitate a reconstruction constraint on the autoencoder to the help guide the learning process of the generative model.

The reconstruction constraint on the autoencoder is twofold. The first constraint is based on $\hat{A}$ just introduced. For the second, $I_{mix}$ is encoded again as $Enc(I_{mix}) = \hat{f}_{I_{mix}}$ to obtain a reconstruction of its original feature vector, $\hat{f}_{I_{mix}}$. The original feature vector $f_A$ is then reconstructed as $\hat{f}_A$ using as many chunks from $\hat{f}_{I_{mix}}$ as possible. In fact, as many as were used from $f_A$ in the mixing algorithm to create $f_{I_{mix}}$. The remaining chunks are taken directly from $f_A$. Finally, using the decoder, $\hat{f}_A$ is decoded into a second reconstructed image $A' = Dec(\hat{f}_A)$ on which the second reconstruction constraint is imposed. Both constraints contribute an equally weighted training signal to the autoencoder.

To establish the adversarial training, $I_{mix}$ and the original image $A$ are passed to the discriminator $D$. $D$ decides on the realism of the fake image $I_{mix}$ thereby issuing a training signal for improvement to the autoencoder (generator loss). Additionally, $D$ decides on the realism of the real image $A$ and the fakeness of the fake image $I_{mix}$ together thereby issuing a training signal to itself (discriminator loss).

\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/lorbms_model.png}
\caption{Schematic of our model architecture.}
\label{fig:model_arch}
\end{figure}

Figure~\ref{fig:model_arch} shows the sequence of two mixing autoencoders ($Enc-Dec-Enc-Dec$) that helps to enforce disentanglement of image objects in latent space and data space alike. In latent space when the model encodes each image quadrant into a feature chunk, each chunk should be invariant to modifications in the other chunks. And in data space when the model decodes the feature vector into an image, each quadrant should represent the object its corresponding feature chunk carried. Intuitively, if we decode and re-encode the mixed feature vector $f_{I_{mix}}$, the resulting vector $\hat{f}_{I_{mix}}$ should preserve the original chunks copied into it and keep the same order of chunks as in $f_{I_{mix}}$.

To further foster the disentanglement of factors of variation, a classification challenge is posed to a classifier $Cls$ that receives a pair of images including the mix image $I_{mix}$ along with one of the original "quadrant replacement" images $I_{qi}$ with $i \in \{1,2,3,4\}$. For each $I_{qi}$, the classifier has to decide if that image occurs in $I_{mix}$ and if so in which quadrant. Therefore, if the autoencoder does a thorough job in encoding (and disentangling, respectively) the objects from the various input images and equally in decoding the mixture of the chunks to a new image, the classifier should be able to recognize the original objects in the generated, mixed scene. The classifier is depicted in more detail in Figure~\ref{fig:model_cls}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{images/lorbms_model_cls.png}
\caption{The classifier $Cls$ decides if an image $I_{q_i}$ occurs in $I_{mix}$ and in which quadrant.}
\label{fig:model_cls}
\end{figure}
Note that the challenge for the classifier is twofold. It not only has to decide if an image occurs in $I_{mix}$ at all but also in which quadrant. The classifier output are four binary variables $[v_1,v_2,v_3,v_4]$, one for each quadrant of $I_{mix}$, that hold the classifiers decision whether image $I_{q_i}$ occurs in quadrant $q_i$ ($i \in \{1,2,3,4\}$) or not. The output along with the ground truth $[x_1,x_2,x_3,x_4]$, the actual occurrence of the "quadrant replacement" images in $I_{mix}$, is then used to define the classifier loss.


\subsection{Loss functions}
The encoder and decoder together as the generator $G$ receive a composite loss that is a weighted sum of three losses: the reconstruction loss, the adversarial loss and the classifier loss. 

\textbf{Reconstruction loss} The reconstruction loss imposes a constraint on the autoencoder that steers it to produce images that closely match the originals $\hat{I}_{ref} \approx I_{ref}$ and is given by
\begin{equation} \label{eq:4}
    L_{rec} = \mathbb{E}_{I_{ref}\sim p_{data} (I_{ref})}\big[ \Vert I_{ref} - \hat{I}_{ref} \Vert_p + \Vert I_{ref} - I'_{ref} \Vert_p \big]
\end{equation}
where $p = 1$ or $p = 2$. However, the well known deficiency of $l_1$ or $l_2$ (i.e. mean squared error) as a loss function is blurry predictions that $D$ can easily discriminate. 

\textbf{Adversarial loss} As in \cite{DisentFacOfVarByMixTh} and \cite{1511.05440}, to counteract this flaw we use an adversarial loss to guide $G$ towards sharp predictions as follows:
\begin{equation} \label{eq:5}
    L^G_{adv} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[ L_{bce}(D(G(I_{ref},I_{q1},I_{q2},I_{q3},I_{q4})), 1)\big]
\end{equation}
where $j \in \{ref,q1,q2,q3,q4\}$ and $L_{bce}$ is the binary cross entropy loss:
\begin{equation} \label{eq:6}
    L_{bce}(p, y) = -(ylog(p) + (1 - y)log(1-p))
\end{equation}
where $y$ is the ground truth and $p$ the prediction. Equation (\ref{eq:5}) is equivalent to the non-saturating GAN loss in equation (\ref{eq:g_loss}). Note that the adversarial loss is computed using the discriminator.

\textbf{Classifier loss} Thirdly, to encourage disentanglement in the latent space representation of the images, we impose the classifier constraint defined as
\begin{equation} \label{eq:7}
    L_{Cls} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[\sum_{j} \lambda_j \sum_{i} L_{bce}(\hat{y}^j_i, y^j_i)\big]
\end{equation}
where $j \in \{q1,q2,q3,q4\}$, $i=1,2,\cdots,4$ and the classifier prediction $\hat{y}^j = Cls(I_{mix}, I_j) = [\hat{y}^j_1, \hat{y}^j_2, \hat{y}^j_3, \hat{y}^j_4]$. $Cls$ thus consists of four binary classifiers, one for each quadrant of $I_{mix}$, the generated image. Each classifier decides if a quadrant in $I_{mix}$ was generated using the quadrant from the replacement image $I_j$ or from $I_{ref}$. The main challenge for $Cls$ is it does not know which image $I_j$ it is given besides $I_{mix}$ and therefore cannot simply remember what to predict. This loss can be minimized only if the disentanglement of the four quadrants as well as the decoding thereof works well.

\textbf{Combined final loss} To combine the three losses we introduce coefficient parameters $\lambda_{rec}$, $\lambda_{adv}$ and $\lambda_{Cls}$. By means of these we can optimize the tradeoff between original image similarity (\ref{eq:4}), sharp predictions (\ref{eq:5}) and disentanglement of the four quadrants (\ref{eq:6}). Thus we get the final loss on $G$ as follows:
\begin{equation} \label{eq:g_loss_comp}
    L_{G} = \lambda_{rec} L_{rec} + \lambda_{adv} L^G_{adv} + \lambda_{Cls} L_{Cls}
\end{equation}

\subsection{Finding visually similar images} \label{subsec_img_clustering}
A "quadrant replacement" image is loosely defined an image that has a quadrant which is very similar visually, as perceived by the human eye, to the same quadrant of another image. For instance, this could be a blue sky in the top right quadrant of two images, or two baseball players in the bottom left quadrant. Note that for our model the two quadrants can appear very similar semantically but should not be the same pixel-wise. Intuitively, we want to replace a quadrant with a different but (semantically) similar looking quadrant to give the model a challenge in generating a new image with different content, yet at the same time this task should not be easy thus the quadrants must not be too close visually (pixel-wise).

In unsupervised learning there is no human intervention. There are no human annotated labels. Likewise we cannot rely on human labor to find and group similar images. The problem arises how we can provide the model with meaningful "quadrant replacement" images since annotations in the dataset are inexistent and random selection will work poorly. We therefore have to rely on another mechanism to help us detect visually similar images that satisfy the above requirements of a "quadrant replacement" image. To this end, we devise a novel visual similarity detection algorithm that leverages the clustering method \textit{DeepCluster} described in \ref{subsec:deepcluster}.

\textbf{Visual similarity detection algorithm} As noted in \ref{subsec:deepcluster}, we exploit the fact that images whose features are close in latent space will also look similar in image space. The closer two features are in latent space the more visually similar two images will be in data space. Visually similar means both images contain an object of the same type such as a person, plane, dog, etc., yet the objects themselves may look substantially different on the pixel level.

TODO evtl hievon noch etwas zehren: "similarity relationships in high-dimensional data is to start by using an autoencoder to compress your data into a low-dimensional space (e.g. 32 dimensional), then use t-SNE for mapping the compressed data to a 2D plane"

Given a dataset $X = \{x_1,x_2,\cdots,x_N\}$ of $N$ images. The algorithm first slices every image from $X$ into four quadrants and creates four extra datasets $X_{q1}, X_{q2}, X_{q3}, X_{q4}$. Subsequently each dataset $X_{qi}$ is processed separately. To produce the features for all images from $X_{qi}$, the algorithm uses a convnet pretrained with \textit{DeepCluster}. The features are then clustered using \textit{k}-means to obtain groups of features. For each feature of a cluster the ten nearest features are calculated using the \textit{k}-nn algorithm with the L2 distance metric as provided by Johnson \textit{et al.}~\cite{FaissKnn}. Finally, each image in $X$ is annotated with four lists containing the ten nearest image ids, one per quadrant. The algorithm is sketched in Algorithm~\ref{alg:vis_sim_det_algo}.

\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwIn{a dataset $X$}
\KwOut{dataset $X'$ with nearest neighbor annotations}
\Begin{
\ForEach{image $x \in X$} {
    slice $x$ into four quadrants $x_{q1}, x_{q2}, x_{q3}, x_{q4}$\;
    create datasets $x_{q1} \in X_{q1}, x_{q2} \in X_{q2}, x_{q3} \in X_{q3}, x_{q4} \in X_{q4}$
}
\ForEach{$X_{qi}$} {
    compute features for all images in $X_{qi}$\;
    cluster features with \textit{k}-means\;
    \ForEach{cluster $c$} {
        \ForEach{feature $f \in c$} {
            compute ten nearest features $tnf$ with \textit{k}-nn and L2 distance\;
            create list $tni$ of ten nearest neighbor images using $tnf$\;
            annotate image $x_{qi}$ belonging to $f$ with $tni$\;
        }
    }    
}
\ForEach{image $x$ in $X$} {
    \ForEach{$X_{qi}$} {
        annotate $x$ with ten nearest neighbor list $tni$ from $x_{qi}$
    }
}
}
\caption{Visual similarity detection algorithm}\label{alg:vis_sim_det_algo}
\end{algorithm}

Note that the visual similarity detection algorithm is used only to preprocess and annotate the dataset, respectively. It is not employed during training or test of the model. The list of ten nearest image ids per quadrant is later utilized during training by the \textit{latent space scene mixing algorithm} as described next.

TODO: add some examples of visually similar looking images as determined by the algorithm??


\subsection{Latent space scene mixing algorithm} \label{subsec_img_mixing_algo}
As a result of the clustering algorithm in section \ref{subsec_img_clustering}, each $I_{ref}$ from the dataset has four ordered lists of ten nearest neighbors (10NN-list), one for each quadrant $I^{qi}_{ref}$ with $i \in \{1,2,3,4\}$. The 10NN-list contains the ten images whose corresponding quadrant is closest in feature space to $I^{qi}_{ref}$. The scene mixing algorithm happens in the latent space of the autoencoder where the feature encodings of the input images are available. This process is summarized in Algorithm~\ref{alg:img_mixing_algo}. 

\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwIn{a reference image $I_{ref}$, its feature vector $f_{I_{ref}}$, a threshold $\tau$}
\KwOut{A mixed feature vector $f_{I_{mix}}$ from 2 to 4 images}
\Begin{
\ForEach{quadrant $I^{qi}_{ref}$ of $I_{ref}$} {
    $I_{qi}$ $\leftarrow$ choose a replacement candidate uniformly from 10NN-list of $I^{qi}_{ref}$\;
}
\ForEach{quadrant $I^{qi}_{ref}$ of $I_{ref}$} {
    \If{$I_{qi}$ has the greatest L2 distance among all replacement candidates}{
        select feature of quadrant $I^{qi}_{ref}$ and ignore $I_{qi}$
    }
    \If{$I_{qi}$ has the lowest L2 distance among all replacement candidates}{
        replace feature of quadrant $I^{qi}_{ref}$ with feature of candidate $I_{qi}$
    }
    \uIf{L2 distance of $I_{qi}$ is greater than $\tau$}{
        select feature of quadrant $I^{qi}_{ref}$ and ignore $I_{qi}$
    }
    \Else{
        replace feature of quadrant $I^{qi}_{ref}$ with feature of candidate $I_{qi}$
    }
} 
}
\caption{Latent space scene mixing algorithm}\label{alg:img_mixing_algo}
\end{algorithm}

The algorithm ensures the resulting mixed feature vector has the following properties:
\begin{itemize}
   \item at least 1 quadrant remains from $I_{ref}$ (line 5)
   \item at least 1 quadrant of $I_{ref}$ is replaced (line 8)
   \item only replacements occur which are "sufficiently similar" to the original (line 11, 14)
\end{itemize}
Therefore, the number of replaced quadrants and features, respectively, in $f_{I_{mix}}$ is always between 1 and 3. In other words, the mixed scene will always be made up of between 2 and 4 images. See Figure~\ref{fig:quad_repl_ex} for visual depictions of mixed feature vectors.

\begin{figure}[ht!]
     \centering
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/1/9_6-images_I_ref_I_M_mix_0100_0100_0011_0001_0010_0010_1000_0100.jpg}
         \caption{1000}
         \label{fig:quad_repl_ex_1_1}
     \end{subfigure}  
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/1/1_5-images_I_ref_I_M_mix_1100_0001_0001_0111_0010_0100_0011_0010.jpg}
         \caption{0100}
         \label{fig:quad_repl_ex_1_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/1/8_2-images_I_ref_I_M_mix_0100_0010_0010_0001_1001_1101_1011_0100.jpg}
         \caption{0010}
         \label{fig:quad_repl_ex_1_3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/1/11_7-images_I_ref_I_M_mix_1110_0001_0010_0001_0100_0010_0010_0001.jpg}
         \caption{0001}
         \label{fig:quad_repl_ex_1_4}
     \end{subfigure}
     \hfill
     % new row
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/11/23_1-images_I_ref_I_M_mix_0111_1100_1110_0011_0010_0010_0010_0111.jpg}
         \caption{1100}
         \label{fig:quad_repl_ex_2_1}
     \end{subfigure}
     \hfill     
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/11/6_6-images_I_ref_I_M_mix_0100_0100_0100_0010_1001_1010_0011_1110.jpg}
         \caption{0011}
         \label{fig:quad_repl_ex_2_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/11/42_1-images_I_ref_I_M_mix_0010_1010_1100_1110_1100_0001_0001_0011.jpg}
         \caption{1010}
         \label{fig:quad_repl_ex_2_3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/11/25_5-images_I_ref_I_M_mix_0100_0011_0100_0010_0010_0110_1000_0001.jpg}
         \caption{0110}
         \label{fig:quad_repl_ex_2_4}
     \end{subfigure}
     % new row
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/111/10_5-images_I_ref_I_M_mix_0010_0100_0010_1100_1011_1101_0100_0110.jpg}
         \caption{1101}
         \label{fig:quad_repl_ex_3_1}
     \end{subfigure}
     \hfill     
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/111/33_3-images_I_ref_I_M_mix_0100_0011_0001_1011_1001_0010_1010_0010.jpg}
         \caption{1011}
         \label{fig:quad_repl_ex_3_2}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/111/1_3-images_I_ref_I_M_mix_1110_0110_1000_1110_1011_1100_1101_0001.jpg}
         \caption{1110}
         \label{fig:quad_repl_ex_3_3}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{images/img_mixes/111/3_1-images_I_ref_I_M_mix_1101_1101_0101_0100_1100_0011_0111_1000.jpg}
         \caption{1101}
         \label{fig:quad_repl_ex_3_4}
     \end{subfigure}     
    \caption{Examples of mixed feature vectors in data space. In each example on the left is $I_{ref}$, on the right is the mixed scene $I_{mix}$. The binary mask denotes clockwise (0) original quadrant and (1) replacement quadrant, starting from the top left corner of the image. First row: 1 replacement quadrant, second row: 2 replacement quadrants, third row: 3 replacement quadrants.}
    \label{fig:quad_repl_ex}
\end{figure}


%=========================================================================
%=========================================================================
\subsection{Implementation}
Like INFOGAN, we use DCGAN for our implementation.

% tentative_____tentative_____tentative_____tentative_____tentative_____tentative_____
\par A search on Google (21.11.18) for standard autoencoder architectures trained on MS COCO dataset did not yield any useful results. The idea was to draw inspiration from previous work and to ensure as much as possible that the autoencoder architecture would be a strong fit to synthesize images following the distribution of the MS COCO dataset.

\par The rationale behind the use of dense blocks is that multiple stacked CONV layers can develop more complex features of the input volume (before the destructive pooling operation). The core idea of dense blocks is that for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. A small growth rate $k$ is sufficient to achieve good results. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the networkâ€™s â€œcollective knowledgeâ€. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Cf. http://cs231n.github.io/convolutional-networks/ and DenseNet paper.

\par INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
Here we see two CONV layers stacked before every POOL layer. This is generally a good
idea for larger and deeper networks, because multiple stacked CONV layers can develop
more complex features of the input volume before the destructive pooling operation.

\par In the decoder we used resize convolution as described in \cite{odena_dumoulin_olah_2016}.

\par The receptive field of the DenseNet encoder (using DenseBlocks) after the calibration efforts amounted to 660x660. Clearly a size detrimental to the 64x64 pixel architecture aiming at disentangling four objects split evenly in four quadrants. We adjust the receptive field to a size of 46x46, a region somewhat bigger than one quadrant.

%=========================================================================
% \chapter{Experiments}
\section{Experimental Results}
In this section, we first perform different kinds of experiments on a real-world dataset to validate the
effectiveness of our method. We then perform ablation studies on our proposed
method. 
We experimentally show...

From [34]: It is in general difficult to evaluate how â€˜goodâ€™ the generative model is. Indeed, however, either subjective or objective, some definite measures of â€˜goodnessâ€™ exists, and essential two of them are â€˜diversityâ€™ and the sheer visual quality of the images.
    
\subsection{Datasets}
\subsubsection{Places}
A 10 million Image Database for Scene Recognition. The Places dataset contains significantly more complex factors of variation than MNIST.

\subsubsection{Microsoft COCO}
On average the Microsoft COCO dataset \cite{1405.0312} contains 3.5 categories and 7.7 instances per image. Another interesting observation is only 10\% of the images in MS COCO have only one category per image.
validation set: total: 5000, greaterThan300: 4916, moreThan4: 2430
training set: total: 118287, greaterThan300: 116510, moreThan4: 57180
For data augmentation we use random crops four times per image, resulting in a dataset size of 228720.
14.02.19: see examples in 
% C:\Users\lz826\git\learning-object-representations-by-mixing-scenes\doc\images\data_augmentation. 
% create_jpeg_tfrecords_l2mix_flip.py
From 1 image we make 6 images: orig, flipped orig, scaled 0.6 and random crop orig, scaled 0.7 and random crop flipped, max size random crop orig, max size random crop flipped
    
\subsection{Evaluation metrics}
For quantitative assessment of generated examples, we used inception score (Salimans \textit{et al.}, 2016)
and Frechet inception distance (FID) (Heusel \textit{et al.}, 2017). See paper [31]. The inception score measures the â€œobjectnessâ€ of a generated image (cf. [30]). This metric allows us to avoid relying on human evaluations. Higher Inception score indicates better image quality. We include the Inception score because it is widely used and thus makes it
possible to compare our results to previous work. FID measures the sample quality and diversity. FID calculates the Wasserstein-2 distance between the generated images and the real images in the feature space of an Inception-v3 network (cf [32], [33]). Lower FID values mean closer distances between synthetic and real data distributions. The FID is a distance, while the Inception Score is a score. In contrast to FID, inception score is
measured on the set of generated images independently from the original images.
FID implementation taken from https://github.com/bioinf-jku/TTUR.

Idee: Diagram mit IS vs Training iteration erstellen -> siehe paper [31] Figure 15b

Aus https://arxiv.org/pdf/1802.03446.pdf: It has been shown that FID is consistent with human judgments and is more robust to noise than IS [37] (e.g. negative correlation between the FID and visual quality of generated samples)

Aus https://arxiv.org/pdf/1802.03446.pdf: According to this paper, FID and IS do not specifically evalute disentanglement in latent space.

From paper [33]: A well-performing approach to measure the performance of GANs is the
â€œInception Scoreâ€ which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples.
Siehe: For formula of IS see (8) in [33].
    
Some sample FID and IS values here: https://nealjean.com/ml/frechet-inception-distance/    
    
\par \textbf{PSNR} The proposal is that the higher the PSNR, the better degraded image has been reconstructed to match the original image and the better the reconstructive algorithm.
http://www.ni.com/white-paper/13306/en/
    
\par From [54]: when evaluating disentangled representations we both employ standard metrics [Kim and Mnih, 2017, Chen \textit{et al.}, 2018, Locatello \textit{et al.}, 2018] and (in view of their limitations) emphasize visualization analysis    
    
\subsection{Experiments}
We now experimentally evaluate the method. An online serarch regarding self-attention layer (from SAGAN paper) and autoencoder does not yield any results.

Idea: do a "nearest neighbor classification" as in paper [1] table 2.

%-- [12] Experiment Idee -------------------------------
\par\cite{DiscHiddenFoViDN} ALS INSPIRATION: In [12]: "To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with zni fixed to 0 and y fixed to one-hot vectors corresponding to each class label. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation". Auch fuer meinen Autoencoder machen um Beispiele aufzuzeigen?
%-- [12] -----------------------------------------------

\subsubsection{Experiments using LORBMS model on downstream tasks}
See paper [63] for some ideas... "We also studied the effect of improved interpolation on downstream tasks, and showed that ACAI led to improved performance for feature learning and unsupervised clustering"

\subsubsection{Traversing image manifolds}
Idea see paper [25]: Once you have a trained model, you can do traversing of image manifolds.

\subsection{Findings in experiments}
\paragraph{Exp10 vs Exp11} 21.11.18: Considering exp10 and exp11, it seems that the results of exp11 (i.e. without instance normalization) suggest that the use of instance norm yields better results (exp10) in that the output is not polluted with recurring artifacts/textures.

\paragraph{Exp60 vs Exp62} 20.02.19: exp62 with 2-img-CLS achieves better FID in first 50 epochs! Exp60 deteriorates after e=30
% cf. https://docs.google.com/document/d/1wWYBJTLVzWeR9h27UrUB7qrNzHgkUHUzoJjyZyYL-gY/edit

\paragraph{Exp62 vs Exp67} 20.02.19: Analyze impact of CoordConv layer in ENC + DSC: TBD

\par For so and so We use the publicly available code of Kraehenbuehl cf [43]

\par Oftentimes changes in hyperparameters only have a marginal effect on the results.

\subsection{Hyperparameters and training details}
As noted in \cite{GANLandscape}, the search space for optimal GAN architectures is humongous: exploring all combinations of all losses, different normalization and regularization schemes, and the plethora of possible architectures is outside of the feasible realm. We therefore take a more pragmatic approach where we conduct a number of experiments with intuitive reasoning for hyperparameter values and combinations thereof (e.g. according to other papers, experiences from fellow lab researchers).

\textbf{Hyperparameter search} As \cite{SpatialBDecoder} and \cite{ChallCmonAssInUnLearOfDR} confirm, finding good parameter sets for hyperparameter-sensitive networks is still a very challenging and time consuming problem. Similar to \cite{AreGANsEqual}, we perform hyperparameter optimization and, for each experiment, look for the best FID across the training run (simulating early stopping). To choose the best model, every epoch we compute the FID between the 10k samples generated by the model and the 10k samples from the test set.

We choose $\beta_1 = 0.5$ and $\beta_2 = 0.999$ as recommended in \cite{GANLandscape}.

% ---------------- nur Notizen START
We use the two learning rate rule for GAN \cite{1706.08500} with $0.0005$ for the generator and $0.0002$ for the discriminator.
Beispiel: We use the same feature map counts in our convolution layers as Karras \textit{et al.} [26]. We use batch normalization [25], spectral normalization [38] and attention mechanism [55].
We initialize all weights of the convolutional, fully-connected, and affine transform layers using $N(0, 1)$. The biases are initialized to zero?.
We do not use dropout.
Batch normalization: [51] It normalizes the pre-activations of nodes in a layer to mean $\beta$ and standard deviation $\gamma$, where both $\beta$ and $\gamma$ are parameters learned for each node in the layer.
% ---------------- nur Notizen END

\subsection{Training}
We train the model to maximize the data log-likelihood using SGD.
Training strategies for disentangling:...
The training with ... did not yield well-disentangled features as the generated images were blurry and not representing objects and sceneries well. 

\par Training Procedure: Our models are implemented in TensorFlow \cite{1605.08695} and are trained using a batch size of 4 instances for the generator and 8 instances for the...
Cf. paper https://arxiv.org/pdf/1803.06414.pdf

\par How do we find a good set of hyperparameters? sequential optimization (hand tuning) by running a series of experiments that build on top of each other. https://vimeo.com/250399261

\subsection{Training convergence}
TODO / IDEA: See [50] appendix D fÃ¼r ein Beispiel dh verwende FID zum Aufzeigen der Training convergence.

\subsection{Results on MS COCO}
(siehe [31] fÃ¼r eine coole Grafik Figure 1 fÃ¼r FID score und verschiedenen Hyperparametern..., auch Table 2)
Samples from both models are provided in Figure 13.

\subsection{Model Performance}
We evaluated the model using the MS Coco dataset first.
We hypothesize...
We find empirically that...
Thus, both quantitative and qualitative results demonstrate...
We found that the model is not sensitive to this hyperparameter
The model achieves strong performance in ...

\par Paper https://arxiv.org/pdf/1806.05575.pdf says "The evaluation metric used for the hyperparameter search was the Frechet Inception Distance (FID)"

\subsection{Qualitative and quantitative evaluation}
See paper [a framework for the quantitative evaluation of disentangled representations]: "visual inspection remains the standard evaluation metric"; "current research generally lacks a clear metric for quantitatively evaluating and comparing disentangled representations."
Evtl. "Disentanglement metric score" verwenden aus Paper 41 fÃ¼r Comparison mit anderen Models?

\subsubsection{Qualitative Evaluation}
We show examples of images generated by our method in Figs. 3 and 4.

\par Perhaps to a system diagram again with sample images as in [55] figure 2 (b)

interpolation in latent/feature space
"latent space interpolations"
"Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects." Cf. 3dgan.csail.mit.edu/papers/3dgan\_nips.pdf

\subsubsection{Quantitative Evaluation}
Beispiel aus [50]: Figure 9 shows how the FID and perceptual path length metrics evolve during the training of our configurations B and F with the FFHQ dataset. With R1 regularization active in both configurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images.

Beispiel aus [31]: We computed the FrÂ´echet inception distance between the true distribution and the generated
distribution empirically over 10000 and 5000 samples.

Here we calculate for the generated test images Inception Scores (IS) and FID. The FID is computed as in 


\subsection{Ablation Study}
cf. e.g. [44] oder appendix D in [51].

%=========================================================================
% \chapter{Conclusions}
\section{Conclusions}
None of our image preprocessing attempts
We show ...
The first is
to elaborate on the impacts of using your approach. The
second is to state limitations or disadvantages of your solution,
thus enabling you to provide directions for future
research in the field.

\section{Future work}
In general, run the experiments.... Do random search for a good set of hyperparameters. A lot of time was spent running experiments with varying sets of hyperparameters using sequential optimization.

\par One could automate the time consuming hyperparameter search by using Bayesian optimization or genetic algorithms. Note that these methods are also inherently sequential by nature. https://vimeo.com/250399261
Another promising approach to consider is random search where the hyperparamters are drawn from a distribution and the training happens in parallel where at the end of the run the best performing set of hyperparameters is picked. However, this approach also requires a substantial amount of work to put in place around a highly non-standard model architecture such as LORBMS. An even more interesting approach to consider is the recently proposed method called 'population based training' (TODO ref to paper) which is based on random search but where sets of hyperparameters and their performances are compared to the population repeatedly during training and then exploited and built upon as training continues. However, for an efficient execution of this method it requires many GPUs. 

\printbibliography

%=========================================================================
\begin{appendices}
\section{Appendix}
\subsection{Network Structure}
Here we give the network structures..

\subsection{Experiments with non-significant results}
Here we list the conducted experiments that did not produce significant results. TODO:
\begin{enumerate}
  \item add self-attention layer (cf. SAGAN) in decoder before last DenseBlock. Rationale: ...
  \item use SqueezeNet instead of AlexNet for classifier because of lack of memory/increase batch size. 12.02: 
\end{enumerate}

\subsubsection{Squeezenet}
We take SqueezeNet instead of AlexNet as the classifier network implementation with the aim to save memory consumption and in favor of a bigger batch size for faster training. In the training process however we noticed that Squeezenet would permanently make an assignment prediction of 0.5 even after the first iteration no matter the input whereas AlexNet would take up the training process as expected. A look into Squeezenet's penultimate layer before the final average pooling layer revealed that its activation was all 0 after the first backpropagation already. We conjecture that with a cross entropy loss function where most ground truth values are 0 (i.e. actual assignments), SqueezeNet suffers from a lack of numerical stability around 0 and therefore collapses to 0 from which is does not recover. This behavior can be reproduced consistently with the test program \texttt{test\_squeezenet\_0\_collapse.py}. We dismissed the idea of using SqueezeNet altogether.

\subsection{Sheets of samples}
...
Figure 9: WGAN algorithm: generator and critic are DCGANs
...
Figure 10: Standard GAN procedure: generator and discriminator are DCGANs
\end{appendices}


\end{document}
