\documentclass[11pt,a4paper]{article}

\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{appendix}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii,
sorting=nty,
%style=alphabetic
%style=reading
style=ieee,
citestyle=ieee
]{biblatex}
\addbibresource{bibliography.bib}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{  \tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{Learning object representations by mixing scenes}

\author{Lukas Zbinden\\
Computer Vision Group\\
Computer Science Department, University of Bern\\ 
2018\\
{\tt\small lukas.zbinden@unifr.ch}
}

\maketitle
%\thispagestyle{empty}

%=========================================================================
%%%%%%%%% ABSTRACT
\begin{abstract}
   Our project 
   abstract: in 4 sentences: state the problem; say why it's an interesting problem; say what your solution achieves; say what follows from your solution
   Do not use custom terminology in the abstract;
use terms and concepts that are widely accepted. Make it easy to see the main point!
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction and Motivation}
The dawn of deep learning in 2012 with the introduction of the groundbreaking AlexNet neural network reignited the idea of an intelligent computer that is on a par with the human brain. After much disappointment in the late 21st century, the decade old research field of artificial intelligence and machine learning in particular was revived around the globe. While today research is still far away from the lofty aim of a truly intelligent machine, impressive achievements have been published continuously (cite?). One research area in machine learning where progress has stood out in recent years is computer vision. The field of computer vision aims at equipping a machine with artificial eyesight to process and interpret visual signals such as images and video. For instance, the computer has learned to classify objects in images with an accuracy that exceeds human performance or detects skin cancer with a higher precision than doctors. Other significant applications such as autonomous driving or medical image analysis have emerged from the ability to artificially understand visual data and presumably many more are yet to be discovered. Within computer vision, one area of research focuses on the representation of visual data such as objects or attributes thereof in lower dimensional space, the latent or feature space. Considering that an image resides in a high dimensional space, the aim of representation learning in the context of computer vision is to learn a model that successfully transforms a given image into a lower dimensional representation that is meaningful and captures the significant image features in condensed form. This type of dimensionality reduction opens up an array of new possibilities for downstream applications that make use of latent space representations. Furthermore, some research imposes additional constraints on the low dimensional representation to lay the groundwork for more fine-grained methods. This research is called disentangling factors of variation. A factor of variation represents an image object or image attribute in latent space. This research aims to learn a latent space representation that is not only low dimensional but has an inherent structure that includes disentangled, independent factors of variation. With such a structure at hand, latent space interpolations become possible where objects or attributes are swapped, altered or used to perform basic arithmetic operations. This work centers around the challenging problem of learning disentangled representations and disentangling factors of variation, respectively, given image data.

TODO: argue about unsupervised learning.
TODO: argue why DL and not explicit math for this problem (-> no rigidity, no formalism, no direct math applicable)

We propose a novel machine learning model capable of learning object representations by mixing scenes. Given two real images containing scenes, the model is expected to generate a new realistic scene that mixes the content of the two inputs. For instance, the two scenes could be pictures taken in a park with several persons and dogs in each. The generated image will in turn also represent a scene in a similar looking park with people and dogs mixed together from both input scenes. The model extracts objects from both scenes and incorporates them in a new made-up scene. The mixing of the objects is done in the latent space based on the extracted object representations.

The proposed solution is expected to generate based on the input images a new scene containing a mixture of the input content including objects and background. The model is able to perform this operation because it has learned to extract objects from images into well structured representations. An object representation consist of multiple independent factors of variation, each representing an image attribute in latent space. This allows the model to compose a crossbreed representation with selectively chosen factors of variation from multiple extracted object representations. This mixed representation is then decoded into a new scene image. Thus the model encodes a given image into an object representation consisting of disentangled factors of variation which makes it possible to swap or move objects around within it or mix it with other representations into a crossbreed and thereby manipulating and controlling the content of the subsequent generated image. 
Intuitively, the key idea is that...

The learned object representations open up a variety of possible downstream applications such as image search, latent space interpolations or visualization of high-dimensional data. The proposed model could be used for fine-grained image search using nearest neighbor search. The algorithm would receive an image showing some objects and using the model would extract the objects in the latent space as object representations and features, respectively. Continuing, the algorithm would run the search in the feature space for images having a similar feature. This search would work without requiring any labels and find images containing similar objects as the query image. Further, latent space interpolation AT WORK WED 02.01...

The model is trained in an unsupervised fashion without any human interference. Our work aims to produce a representation that is made up of well disentangled factors of variation representing image objects and object attributes, respectively. Experiments showcase the potential of the model through qualitative and quantitative evaluation of the generated samples.

\subsection{Challenges}
TBD list expected challenges for the model
\begin{enumerate}
  \item unsupervised disentanglement
  \item a dataset with aligned data is a desirable property for a machine learning model because TODO: why?. For instance, CelebA is a face dataset where an image contains exactly one face which is often center aligned. In contrast, the proposed model is applied to a natural, heterogenous dataset in which the data (e.g. objects) is inherently not aligned. By design however, the model makes the assumption that a given image contains up to four objects where each object is approximately aligned in one of the quadrants. The underlying dataset will therefore pose a substantial challenge to the model in the learning process
\end{enumerate}

\subsection{Contributions}
The contributions of our work are summarized as follows:
\begin{enumerate}
  \item neural architecture engineering, hyperparamter tuning
  \item An implementation of the proposed dataset based on MS COCO
  \item An implementation of the proposed method
  \item A study of the capabilities and limitations of the proposed model
\end{enumerate}

% ...........................................................................................
% The introduction serves a twofold purpose. Firstly, it
% gives the background on and motivation for your research,
% establishing its importance. Secondly, it gives a summary
% and outline of your paper. When you write the background review, you should
% consider including technological trends of the area, open
% problems and recent promising developments.
% A proper flow is to first set the context,
% then present your proposal, then provide the verification,
% and lastly wrap up with conclusions
% 
% - says "I am going to look at the following things"
% - introduce area of study - representation learning, disentangling factors of variation
% - Describe the problem, state your contributions and the implications
% - say what is really new in your work
% - tell what your work is about, what problem it solves, why the problem is interesting, what is really new in your work, why it's so neat
% - set the proper context for understanding the rest of your paper!
% - introduce your idea using EXAMPLES and only then present the general case, explain it as if you were speaking to someone using a whiteboard;
% ...........................................................................................

learned representations, data space

disentangling factors of variation in image data. 

feature representation vs object representation
feature = numeric representation of raw data
feature space is a vector space
feature = high dimensional vector
a collection of data points then becomes a point cloud in the feature space 
model = mathematical "summary" of features (summary -> a gemoetric shape)
model = geometric summary of point cloud
point cloud = approximate geometric shape
disentangling image features into semantically meaningful properties

"a separate set of dimensions" = a feature chunk

Devise a manifold learning algorithm that can discover and capture independent factors of variation.

\par Concretely, we used a ... to model the distribution over image features and the latent factors of variation.

% - from [12]
Representation learning: Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsupervised learning algorithms such as autoencoders seek efficient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables. 

% -- from [26]
While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. For example, for a dataset of faces, a useful disentangled representation may allocate a separate set of dimensions for each of the following attributes: facial expression, eye color, hairstyle, presence or absence of eyeglasses, and the identity of the corresponding person. A disentangled representation can be useful for natural tasks that require knowledge of the salient attributes of the data, which include tasks like face recognition and object recognition.

%=========================================================================
\section{Related work}
% Secondly, it provides a critique of the
% approaches in the literature—necessary to establish the
% contribution and importance of your paper. 
% Critiquing the major approaches of the background
% work will enable you to identify the limitations of the
% other works and show that your research picks up where
% the others left off.


\subsection{Autoencoders}
TBD...

\subsection{Generative Adversarial Network (GAN)}
%-- GAN start -------------------------------
This type of neural network, the so-called Vanilla GAN, was introduced in 2014 by \cite{1406.2661} and has inspired a significant amount of research and progress. It is called ``the coolest idea in deep learning in the last 20 years'' by AI pioneer Yann LeCun and ``a significant and fundamental advance'' by AI luminary Andrew Ng. The GAN network defines a learning framework for synthetic image generation in which two neural networks, usually convolutional, namely the generator $G$ (a generative model) and the discriminator $D$ (a discriminative model), compete against each other in a so called minimax game. The framework includes a dataset from which two things result. Firstly, the dataset's images are used by the discriminator to learn to discriminate real from fake images. Real images come from the dataset, fake images come from the generator. Secondly, the dataset is defined by an implicit probability data distribution $p_d$ which all its samples follow. The generator's objective is to learn and approximate, respectively, the data distribution $p_d$ and by that generate new, realistic images that look similar to samples from the dataset. Thus the generator implicitly defines a probability model distribution $p_g$ as the distribution of the samples obtained from the generator given some input $z$ (e.g. a Gaussian noise). In other words, the discriminator learns to determine whether a sample is from $p_d$ or $p_g$ and the generator learns to invent samples from $p_g$ that closely ressemble samples from $p_d$ and ultimately confuse $D$ in its decision.
\par \textbf{Minimax game} The minimax game comes into play in the learning phase. The two models are simultaneously trained so that $G$ learns to generate samples that are hard to classify by $D$, while $D$ learns to discriminate the samples generated by $G$. As training evolves, $D$ forces $G$ to produce even better samples. This setting can be viewed as a two-player game where both players (models) try to minimize their own loss and the final state of the game is the Nash equilibrium where both players cannot improve their loss anymore. Ideally, when $G$ is fully trained, it should not be possible for $D$ to perform any better than randomly guessing. The two-player minimax game is formally defined as
\begin{equation} \label{eq:1}
\min\limits_{G} \max\limits_{D} V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}\big[log\, D(x)\big] + \mathbb{E}_{z\sim p_{z} (z)}\big[log(1 - D(G(z)))\big]
\end{equation}
where $z$ is a random noise. $G$ tries to minimize $log(1 - D(G(z))$ (or maximize $D(G(z))$) such that $D$ gives a high probability for input $G(z)$, while $D$ tries to maximize $log(D(x)) + log(1 - D(G(z))$ with a high probability for $x$ and a low probability for $G(z)$. The two networks are trained in an alternating manner: for every training batch, one network is fixed and the other is updated so that backpropagation happens in one network at a time exclusively. In an ideal convergence scenario, $G$ has learned to produce samples so realistic that $D$ cannot discern them from real images and assigns probability 0.5 to every image (Nash equilibrium). In this state, $D$ cannot teach $G$ anymore how to improve, convergence has set in. Note that the expectation value is practically the average over the training images x.

\par \textbf{GAN loss functions} 
Multiple formulations of the GAN loss functions exist (\cite{1807.04720}). In this work we focus on the non-saturating GAN loss introduced in \cite{1406.2661} as the empirical evaluation in \cite{1807.04720} suggests to favor this version when applying GANs to a new dataset. The hinge loss version has also shown good performance in \cite{1802.05957} but \cite{1807.04720} concludes that it performs very similar to the non-saturating loss.

The discriminator loss function is
\begin{equation} \label{eq:d_loss}
    L_{D} = -\mathbb{E}_{x\sim p_{data} (x)}\big[log(D(x)\big] - \mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(D(1 - \hat{x})\big]
\end{equation}
where $p_{data}$ denotes the (true) data distribution and $p_{model}$ the model distribution. It is equivalent to the binary cross entropy between the data and the model distributions of real and generated images ($D$ has to discriminate between two classes).

The generator loss function is
\begin{equation} \label{eq:g_loss}
    L_{G} = -\mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(D(\hat{x})\big]
\end{equation}
where the generator creates samples $\hat{x} = G(z)$ which the discriminator believes to be real $D(\hat{x}) \approx 1$ and thereby trying to approach $log(1) = 0$.
%-- GAN end -------------------------------


%-- [24] DCGAN start -------------------------------
\par \textbf{DCGAN} \cite{1511.06434} introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate for the first time that they are a strong candidate for purely unsupervised learning. The authors propose a more stable set of architectures for training generative adversarial networks and give evidence that adversarial networks learn good representations of images. According to Ian Goodfellow, DCGAN defines a quasi-standard generator network architecture. 
%-- [24] DCGAN end -------------------------------

%-- [26] INFOGAN start -------------------------------
\par\textbf{INFOGAN} \cite{1606.03657} describes InfoGAN, a type of GAN that learns disentangled representations in an unsupervised manner (no supervision of any kind) with the addition of maximizing the mutual information between a small subset of the latent variables and the observation. InfoGAN can disentangle both discrete and continuous latent factors, scale to complicated datasets, and typically requires no more training time than regular GANs.  The effective results suggest that generative modelling augmented with a mutual information cost be a fruitful approach for learning disentangled representations. They used DCGAN for implementation and stable GAN training, respectively. Gist: concat noise vector $z$ with latent vector $c$ to form input for generator G. Additionally, extend discriminator D with Q-network on top to generate $\hat{c}$ to then calculate the reconstruction loss with $c$ and train entire network accordingly using backprob (aside the standard GAN training). (In [29] gibts auch eine kurze Zusammenfassung)
%-- [26] INFOGAN end -------------------------------

%-- [32] SAGAN start -------------------------------
\par \textbf{SAGAN} \cite{1805.08318} introduces self-attention to the GAN framework, enabling both the generator and the discriminator to efficiently model relationships between widely separated spatial regions. Convolution processes the information in a local neighborhood only, thus using convolutional layers alone is computationally inefficient for modeling long-range dependencies in images.
%-- [32] SAGAN end -------------------------------

%-- [29] DNA-GAN start -------------------------------
\par \textbf{DNAGAN} In \cite{1711.05415}, Xiao et al. propose the supervised DNA-GAN by using an analogy with the DNA double helix structure, where different kinds of traits are encoded in different DNA pieces. In the same way, they argue that different visual attributes in an image are controlled by different pieces in the latent representation of that image. Similar to our model, for two parallel input images, they use an encoder to retrieve their representations and a decoder to generate new images whose realism is judged by a discriminator (GAN). Additionally, they use labels to support the disentanglement in the latent space where image pairs are required to have different labels for a certain attribute (e.g. smiling or not). The respective part of a label in the latent representations of the image pair is then swapped to obtain two crossbreed representations and images, respectively.
%-- [29] DNA-GAN end -------------------------------

\par \textbf{Joint loss function}
Similar to our work, a number of methods (\cite{1511.05440},\cite{1711.07410},\cite{1604.07379}) use a joint loss function for the generator which is composed of two or more losses and oftentimes includes a reconstruction loss and an adversarial loss $L_G = \lambda_{rec}L_{rec} + \lambda_{adv}L^G_{adv} + \lambda_{i}L_{i}$ where i is a method specific additional loss. To look up the weightings in other works proves useful to find reasonable values more efficiently. For instance, \cite{1604.07379} uses $\lambda_{rec} = 0.999$ and $\lambda_{adv} = 0.001$ on ImageNet whereas \cite{1711.07410} deploys $\lambda_{rec} = 30$ and $\lambda_{adv} = 1$ on CelebA.

\subsection{CNNs}
%-- CNN start -------------------------------
\par CNNs: As most of parameters exist in fully connected (FC) layers
%-- CNN end -------------------------------


\subsection{Manifold learning and latent space interpolation}
As noted by Laine [32], interpolation of latent-space vectors may yield surisingly non-linear changes in the image (vgl. [50] 4.1).


\subsection{Representation learning}
%-- [15] start -------------------------------
\par\cite{1206.5538} elaborates on representation learning whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors. In 3.5 they distinguish between learning invariant features and learning to disentangle explanatory factors. They conclude that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical. Doing so they propose should give rise to a good representation significantly more robust to the complex and richly structured variations extant in natural images for AI-related tasks. The manifold hypothesis is introduced which makes the assumption that the data lies along a low-dimensional manifold where the probability mass is highly concentrated. They argue that a representation being learned can be associated with an intrinsic coordinate system (on the embedded lower-dimensional manifold). For instance, this can be demonstrated well by a variational autoencoder.
%-- [15] end -------------------------------

Some representation learning models are ... vgl [29]

\subsection{Unsupervised disentangling factors of variation}
\par the ULTIMATE OBJECTIVE of this is: we want to encode an image and get a feature representation that has such well disentangled features (2D) and attributes (3D) that it makes it possible to take this representation and move features or attributes around within in and therefore move or change objects in the feature space (manifold?)

\par Many prior works on DFoV are fully supervised, i.e. they use labels for all factors of variation to be disentangled.

\par image attribute - factor of variation - feature vector - feature chunk
an object representation is learned as a concatenation of feature chunks
uses high-dimensional feature chunks
feature space is only designed for attribute transfer but not for sampling
how is disentanglement achieved? 
-> invariance objective i.e. an architecture with mixing autoencoders encourages disentangled encoding of attributes and disentangled decoding of feature chunks, respectively
-> classification objective such that each feature chunk corresponds to a discernible attribute in the original image 
encoding of an attribute, decoding of a feature chunk

\par\cite{1412.6583} states in 2014 that there is a lack of standard benchmark tasks for evaluating disentangling performance. Their evaluation is based on examining qualitatively what factors of variation are discovered for different datasets. ALS INSPIRATION: In [12]: "To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with zni fixed to 0 and y fixed to one-hot vectors corresponding to each class label. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation". Auch für meinen Autoencoder machen um Beispiele aufzuzeigen?

\par "A disentangled representation is generally described as one which separates the factors of variation, explicitly representing the important attributes of the data" [a framework for the quantitative evaluation of disentangled representations]

\par image space vs feature space paper [42]

This work builds on the results of \cite{1711.07410} and \cite{1711.02245}.
\cite{1711.02245} identifies two main challenges in disentangling factors of variation: the shortcut problem and the reference ambiguity. In the shortcut problem the model learns degenerate encodings in which all information is encoded only in one part of the feature, i.e. either in the component $c$ for common attributes shared by both images or the component $v$ for varying attributes. For an image pair there are two $v$ and one $c$. The encoder maps a complete description of its input into vector $N_v$ and the decoder completely ignores vector $N_c$.
\newline The second challenge is named reference ambiguity where the attribute representation for an image is not guaranteed to follow the same interpretation on another image. In other words, the reference in which a factor is interpreted may depend on other factors which makes the attribute transfer ambiguous. For example, the viewpoint angle of a vessel gets interpreted in two different ways depending on the boat type.
%-- [16] start -------------------------------
\par[Understanding Degeneracies and Ambiguities in Attribute Transfer] addresses these challenges. They introduce an adversarial weakly supervised training method that uses image triplets and fully tackles the shortcut problem by means of a composite loss function consisting of an autoencoder loss and an adversarial (i.e. GAN) loss. Used in conjunction they provably avoid that challenge. 
Furthermore, they analyze the reference ambiguity and prove that it is unavoidable when disentangling with only weak labels. The problem occurs when a decoder reproduces the data without satisfying the disentangling properties for the varying attribute $v$. Practically, this means not all the factors of variation can provably be disentangled from weakly labeled data (i.e. when only $c$ is known for image pairs).    
%-- [16] end -------------------------------

%-- from [12]
In previous approaches, content vs style, form vs motion, facial expression vs identity were explored.

%-- [40] start
[40] propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. The authors state that reliable disentanglement is far from solved even in their restricted setting. Further, [40] implies that there is currently no quantitative benchmark available for disentangled factor learning.
%-- [40] end

%-- [27] start (nicht sicher ob inkludieren soll) ------------------------
[Semi-supervised learning with deep generative models] utilizes a variational autoencoder in a semi-supervised learning paradigm which is capable of separating content and style in data.
%-- [27] end -------------------------------

%-- [12] Discovering Hidden FoV in DN -- start -------------------
\par\cite{1412.6583} augments autoencoders with regularization terms during training. They use an unsupervised cross-covariance penalty (XCov) as a method to disentangle class-relevant signals (observed variables) from other factors in the latent variables along with a standard supervised cross-entropy loss. In case of MNIST they consider the class label as a high-level representation of its corresponding input. Their model learns a class invariant smooth continuous latent representation $z$ that encodes digit style (slant) whereas the observed variable $\hat{y}$ represents the digit itself. On the TFD dataset, the observed variable $\hat{y}$ encodes the facial expression while the autoencoder is able to retain the identity of the faces through latent variable $z$. Here the XCov penalty prevents expression label variation from ‘leaking’ into the latent representation.
%-- [12] end -------------------------------

\par Some popular models for unsupervised disentangling factors of variation are INFOGAN, $\beta$-VAE. Supervised variants are DNAGAN, ... (vgl. [29]) 

\par This work pushes the idea of \cite{1711.07410} further.

%=========================================================================
%=========================================================================
%=========================================================================
%=========================================================================
% Problem statement and Method:
% Learning object representations by mixing scenes
\section{Learning object representations by mixing scenes}
In this section, we first review the formulation of GANs briefly. Next, we present the LSGANs along with their benefits in Section 3.2. Finally, two model architectures of LSGANs are introduced in 3.3. (Taken from chap3 https://arxiv.org/pdf/1611.04076.pdf)

the intent of the system, the basic motivation for all of this, is that we would like to be able to move objects around by editing the feature space. the purpose in the end is to build a representation where I take a feature from one and plug it there and then the rendering adapts the content - that's the ultimate goal.
\par image attribute - factor of variation - feature vector - feature chunk
\par an object representation is learned as a concatenation of feature chunks

\par WICHTIG: an important assumption of the model is that each image consists of 4 quadrants where each contains at most one object.

\par ***
Put good effort in realizing
all explicit and implicit assumptions that you make, and
clearly state them.
***

\par our solution is an unconditional generative model...

\par Show what is used during training and what not during testing (e.g. discriminator)

\par and a GAN is used to improve the realism of the results

\par LEARN A MODEL THAT IDENTIFIES OBJECTS IN THE SCENE AND LEARNS TO COMPOSE THEM SO YOU CAN MERGE SCENES.

\par https://arxiv.org/pdf/1803.06414.pdf "Learning to Segment via Cut-and-Paste": We propose and formalize a new cut-and-paste adversarial training scheme for box-supervised instance segmentation, which captures an intuitive prior, that objects can move independently of their background.
-> perhaps use latter sentence as idea for the intuition that we're trying to "move objects around independently of their background"

\par what is the theoretical groundwork for the proposed method?

\par We use a GAN to check for realism of the generated images.

\par This subsection explicates the reason ...

\par A baseline could also be pure chance s.a. 50 percent chance..

\par an encoder for computing a low-dimensional latent representation, and a decoder for synthesizing the output image.

how do I enforce invariance between object representations and object attribute representations, respectively?
x3 should be a valid image according to the input data distribution.

The classifier consists of $3x3x8$ multi-class classifiers, one for each chunk, that decide for each chunk the original image ($x_1$, $x_2$) and the specific tile, respectively, that was used to generate a tile of the composite image.

(Hinweis: [12] has nice formulation of cost function, perhaps use as inspiration)

\par To generate the unbalanced mask $m$ with a bias towards the first of the two contexts, we used a Bernoulli distribution with $p=0.6$ and $q=1-p=0.4$, respectively, such that $Pr(X=1)=0.6$ and $Pr(X=0)=1-P(X=1)=0.4$ where $1$ represents the first context (i.e. image $x_1$) and $0$ the second context (i.e. image $x_2$).

\par One big drawback of GANs is the notorious instability of the discriminator during training. During training of modified DCGAN encountered lack of GAN training, i.e. the generator loss was immediately 0, the discriminator loss 27.6 ($dsc_loss_real$ was 0 and $dsc_loss_fake$ was 27.6 (meaning the discriminator always output 1 no matter the input). To counteract this problem we implemented and used spectral normalization for GANs (also SNGAN, \cite{1802.05957}) to regularize the Lipschitz constant. With the latter in place, the GAN training set in. We first added to the discrminator, and later according to TTUR paper also in the autoencoder. Note that spectral normalization does not use extra weights, therefore comes at no memory cost.
Important: also mention link to Lipschitz continuity: "They applied spectral normalization to the weights in both generator and discriminator, unlike the previous paper which only normalizes the discriminator weights. They set the spectral norm to 1 to constrain the Lipschitz constant of the weights. It’s just used for controlling the gradients. This spectral normalization idea was first introduced by Miyato et. al." https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b

\par In high dimensional spaces, embeddings?

\par Some methods are based on new mathematical models and others based on intuition back up by experiments.

\par In \cite{1611.03383} Vielleicht als Inspiration für 'Qualitativ evaluation': "we performed nearest neighbor retrieval in the learned embedding spaces... We computed the corresponding representations for all samples (for the unspecified component we used the mean of the approximate posterior distribution) and then retrieved the nearest neighbors for a given query image"

\par We used the pretrained AlexNet model from the work of DeepCluster (\cite{1807.05520}) to cluster the dataset into groups of similar images.

\subsection{Model architecture}
In this section, we formally present our method. The model assumes that a real world image $I_{ref}$ contains four objects, ideally in each quadrant one object. The model thus considers $I_{ref}$ as made up of four quadrants $I^1_{ref}, I^2_{ref}, I^3_{ref}, I^4_{ref}$.

The encoder $E$ maps $A$ into a latent space representation
\begin{equation} \label{eq:2}
    Enc(A) = [a_1, a_2, a_3, a_4]
\end{equation}
where $a_i$ represents the feature vector of the respective image quadrant $A_i$. $a_i$ is supposed to represent the object in $A_i$ exclusively such that it is invariant to modifications in the remaining feature vectors, hence it represents a disentangled factor of variation of the input image.

By swapping some of the feature vectors we obtain a new, mixed representation. Using decoder $D$, that representation is decoded into a newly generated image $I_{mix}$
\begin{equation} \label{eq:3}
    Dec([a_i, b_j, c_l, d_n]) = I_{mix}
\end{equation}
on which the discriminator is trained along with the original image $I_{ref}$ in an adversarial fashion.

% tentative_____tentative_____tentative_____tentative_____tentative_____tentative_____
\par Training Procedure: Our models are implemented in TensorFlow \cite{1605.08695} and are trained using a batch size of 4 instances for the generator and 8 instances for the...
Cf. paper https://arxiv.org/pdf/1803.06414.pdf

\par A search on Google (21.11.18) for standard autoencoder architectures trained on MS COCO dataset did not yield any useful results. The idea was to draw inspiration from previous work and to ensure as much as possible that the autoencoder architecture would be a strong fit to synthesize images following the distribution of the MS COCO dataset.

\par The rationale behind the use of dense blocks is that multiple stacked CONV layers can develop more complex features of the input volume (before the destructive pooling operation). The core idea of dense blocks is that for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. A small growth rate $k$ is sufficient to achieve good results. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth
rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Cf. http://cs231n.github.io/convolutional-networks/ and DenseNet paper.

\par INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
Here we see two CONV layers stacked before every POOL layer. This is generally a good
idea for larger and deeper networks, because multiple stacked CONV layers can develop
more complex features of the input volume before the destructive pooling operation.

\subsection{Loss functions}
The encoder and decoder together as the generator $G$ receive a composite loss that is a weighted sum of three losses: the reconstruction loss, the standard GAN loss and the classifier loss. The reconstruction loss imposes a constraint on the autoencoder that steers it to produce images that closely match the originals $\hat{I}_{ref} \approx I_{ref}$ and is given by
\begin{equation} \label{eq:4}
    L_{rec} = \mathbb{E}_{I_{ref}\sim p_{data} (I_{ref})}\big[ \Vert I_{ref} - \hat{I}_{ref} \Vert_p + \Vert I_{ref} - I'_{ref} \Vert_p \big]
\end{equation}
where $p = 1$ or $p = 2$.

However, the deficiency of $l_1$ or $l_2$ (i.e. mean squared error) as a loss function is blurry predictions that $D$ can easily discriminate. As in \cite{1711.07410} and \cite{1511.05440}, to counteract this flaw we use an adversarial loss to guide $G$ towards sharp predictions as follows:
\begin{equation} \label{eq:5}
    L^G_{adv} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[ L_{bce}(D(G(I_{ref},I_{q1},I_{q2},I_{q3},I_{q4})), 1)\big]
\end{equation}
where $j \in \{ref,q1,q2,q3,q4\}$ and $L_{bce}$ is the binary cross entropy loss:
\begin{equation} \label{eq:6}
    L_{bce}(p, y) = -(ylog(p) + (1 - y)log(1-p))
\end{equation}
where $y$ is the ground truth and $p$ the prediction. Equation (\ref{eq:5}) is equivalent to the non-saturating GAN loss in equation (\ref{eq:g_loss}). Note that the adversarial loss is computed using the discriminator.

Thirdly, to encourage disentanglement in the latent space representation of the images, we impose the classifier constraint defined as
\begin{equation} \label{eq:7}
    L_{Cls} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[\sum_{j} \lambda_j \sum_{i} L_{bce}(\hat{y}^j_i, y^j_i)\big]
\end{equation}
where $j \in \{q1,q2,q3,q4\}$ and the classifier prediction $\hat{y}^j = Cls(I_{mix}, I_j) = [\hat{y}^j_1, \hat{y}^j_2, \hat{y}^j_3, \hat{y}^j_4]$. $Cls$ thus consists of four binary classifiers, one for each quadrant of $I_{mix}$, the generated image. Each classifier decides if a quadrant in $I_{mix}$ was generated using the respective quadrant from $I_j$ or not. The main challenge for $Cls$ is it does not know which image $I_j$ it is given and therefore cannot take a shortcut as to which quadrant is expected to match.

TODO at work 23.01 answer the question: when can this loss be minimized?


To combine the three losses we introduce coefficient parameters $\lambda_{rec}$, $\lambda_{adv}$ and $\lambda_{Cls}$. By means of these we can optimize the tradeoff between original image similarity (\ref{eq:4}), sharp predictions (\ref{eq:5}) and disentanglement of the four quadrants (\ref{eq:6}). Thus we get the final loss on $G$ as follows:
\begin{equation} \label{eq:g_loss_comp}
    L_{G} = \lambda_{rec} L_{rec} + \lambda_{adv} L^G_{adv} + \lambda_{Cls} L_{Cls}
\end{equation}

\subsection{Implementation}
Like INFOGAN, we use DCGAN for our implementation.

%=========================================================================
\section{Experimental Results}
In this section, we first perform different kinds of experiments on a real-world dataset to validate the
effectiveness of our method. We then perform ablation studies on our proposed
method. 
We experimentally show...

From [34]: It is in general difficult to evaluate how ‘good’ the generative model is. Indeed, however, either subjective or objective, some definite measures of ‘goodness’ exists, and essential two of them are ‘diversity’ and the sheer visual quality of the images.
    
\subsection{Datasets}
\subsubsection{Places}
A 10 million Image Database for Scene Recognition. The Places dataset contains significantly more complex factors of variation than MNIST.

\subsubsection{Microsoft COCO}
On average the Microsoft COCO dataset \cite{1405.0312} contains 3.5 categories and 7.7 instances per image. Another interesting observation is only 10\% of the images in MS COCO have only one category per image.
validation set: total: 5000, greaterThan300: 4916, moreThan4: 2430
training set: total: 118287, greaterThan300: 116510, moreThan4: 57180
For data augmentation we use random crops four times per image, resulting in a dataset size of 228720.
    
\subsection{Evaluation metrics}
For quantitative assessment of generated examples, we used inception score (Salimans et al., 2016)
and Frechet inception distance (FID) (Heusel et al., 2017). See paper [31]. The inception score measures the “objectness” of a generated image (cf. [30]). This metric allows us to avoid relying on human evaluations. Higher Inception score indicates better image quality. We include the Inception score because it is widely used and thus makes it
possible to compare our results to previous work. FID measures the sample quality and diversity. FID calculates the Wasserstein-2 distance between the generated images and the real images in the feature space of an Inception-v3 network (cf [32], [33]). Lower FID values mean closer distances between synthetic and real data distributions. The FID is a distance, while the Inception Score is a score. In contrast to FID, inception score is
measured on the set of generated images independently from the original images.
FID implementation taken from https://github.com/bioinf-jku/TTUR.

Idee: Diagram mit IS vs Training iteration erstellen -> siehe paper [31] Figure 15b

Aus https://arxiv.org/pdf/1802.03446.pdf: It has been shown that FID is consistent with human judgments and is more robust to noise than IS [37] (e.g. negative correlation between the FID and visual quality of generated samples)

Aus https://arxiv.org/pdf/1802.03446.pdf: According to this paper, FID and IS do not specifically evalute disentanglement in latent space.

From paper [33]: A well-performing approach to measure the performance of GANs is the
“Inception Score” which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples.
Siehe: For formula of IS see (8) in [33].
    
Some sample FID and IS values here: https://nealjean.com/ml/frechet-inception-distance/    
    
\par \textbf{PSNR} The proposal is that the higher the PSNR, the better degraded image has been reconstructed to match the original image and the better the reconstructive algorithm.
http://www.ni.com/white-paper/13306/en/
    
\subsection{Experiments}
We now experimentally evaluate the method. An online serarch regarding self-attention layer (from SAGAN paper) and autoencoder does not yield any results.

\subsection{Findings in experiments}
21.11.18: Considering exp10 and exp11, it seems that the results of exp11 (i.e. without instance normalization) suggest that the use of instance norm yields better results (exp10) in that the output is not polluted with recurring artifacts/textures.

\par For so and so We use the publicly available code of Kraehenbuehl cf [43]

\par Oftentimes changes in hyperparameters only have a marginal effect on the results.

\subsection{Hyperparameters and training details}
As noted in \cite{1807.04720}, the search space for GANs is humongous: exploring all combinations of all losses, different normalization and regularization schemes, and the plethora of possible architectures is outside of the feasible realm. We therefore take a more pragmatic approach where we conduct a number of experiments with intuitive reasoning for hyperparameter values and combinations thereof (e.g. according to other papers, experiences from fellow lab researchers).

\textbf{Hyperparameter search} Similar to \cite{1711.10337}, we perform hyperparameter optimization and, for each experiment, look for the best FID across the training run (simulating early stopping). To choose the best model, every
epoch we compute the FID between the 10k samples generated by the model and the 10k samples from the test set.

We choose $\beta_1 = 0.5$ and $\beta_2 = 0.999$ as recommended in \cite{1807.04720}.

% ---------------- nur Notizen START
We use the two learning rate rule for GAN \cite{1706.08500} with $0.0005$ for the generator and $0.0002$ for the discriminator.
Beispiel: We use the same feature map counts in our convolution layers as Karras et al. [26]. We use batch normalization [25], spectral normalization [38] and attention mechanism [55].
We initialize all weights of the convolutional, fully-connected, and affine transform layers using $N(0, 1)$. The biases are initialized to zero?.
We do not use dropout.
Batch normalization: [51] It normalizes the pre-activations of nodes in a layer to mean $\beta$ and standard deviation $\gamma$, where both $\beta$ and $\gamma$ are parameters learned for each node in the layer.
% ---------------- nur Notizen END

\subsection{Training}
We train the model to maximize the data log-likelihood using SGD.
Training strategies for disentangling:...
The training with ... did not yield well-disentangled features as the generated images were blurry and not representing objects and sceneries well. 

\par How do we find a good set of hyperparameters? sequential optimization (hand tuning) by running a series of experiments that build on top of each other. https://vimeo.com/250399261

\subsection{Training convergence}
TODO / IDEA: See [50] appendix D für ein Beispiel dh verwende FID zum Aufzeigen der Training convergence.

\subsection{Results on MS COCO}
(siehe [31] für eine coole Grafik Figure 1 für FID score und verschiedenen Hyperparametern..., auch Table 2)

\subsection{Model Performance}
We evaluated the model using the MS Coco dataset first.
We hypothesize...
We find empirically that...
Thus, both quantitative and qualitative results demonstrate...
We found that the model is not sensitive to this hyperparameter
The model achieves strong performance in ...

\par Paper https://arxiv.org/pdf/1806.05575.pdf says "The evaluation metric used for the hyperparameter search was the Frechet Inception Distance (FID)"

\subsection{Qualitative and quantitative evaluation}
See paper [a framework for the quantitative evaluation of disentangled representations]: "visual inspection remains the standard evaluation metric"; "current research generally lacks a clear metric for quantitatively evaluating and comparing disentangled representations."
Evtl. "Disentanglement metric score" verwenden aus Paper 41 für Comparison mit anderen Models?

\subsubsection{Qualitative Evaluation}
We show examples of images generated by our method in Figs. 3 and 4.

interpolation in latent/feature space
"latent space interpolations"
"Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects." Cf. 3dgan.csail.mit.edu/papers/3dgan\_nips.pdf

\subsubsection{Quantitative Evaluation}
Beispiel aus [50]: Figure 9 shows how the FID and perceptual path length metrics evolve during the training of our configurations B and F with the FFHQ dataset. With R1 regularization active in both configurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images.

Beispiel aus [31]: We computed the Fr´echet inception distance between the true distribution and the generated
distribution empirically over 10000 and 5000 samples.

Here we calculate for the generated test images Inception Scores (IS) and FID. The FID is computed as in 


\subsection{Ablation Study}
cf. e.g. [44] oder appendix D in [51].

%=========================================================================
\section{Conclusion}
None of our image preprocessing attempts
We show ...
The first is
to elaborate on the impacts of using your approach. The
second is to state limitations or disadvantages of your solution,
thus enabling you to provide directions for future
research in the field.

\section{Future work}
In general, run the experiments.... Do random search for a good set of hyperparameters. A lot of time was spent running experiments with varying sets of hyperparameters using sequential optimization.

\par One could automate the time consuming hyperparameter search by using Bayesian optimization or genetic algorithms. Note that these methods are also inherently sequential by nature. https://vimeo.com/250399261
Another promising approach to consider is random search where the hyperparamters are drawn from a distribution and the training happens in parallel where at the end of the run the best performing set of hyperparameters is picked. However, this approach also requires a substantial amount of work to put in place around a highly non-standard model architecture such as LORBMS. An even more interesting approach to consider is the recently proposed method called 'population based training' (TODO ref to paper) which is based on random search but where sets of hyperparameters and their performances are compared to the population repeatedly during training and then exploited and built upon as training continues. However, for an efficient execution of this method it requires many GPUs. 

\printbibliography

%=========================================================================
\begin{appendices}
\section{Appendix}
\subsection{Network Structure}
Here we give the network structures..

\subsection{Experiments with non-significant results}
Here we list the conducted experiments that did not produce significant results. TODO:

\subsection{Sheets of samples}
...
Figure 9: WGAN algorithm: generator and critic are DCGANs
...
Figure 10: Standard GAN procedure: generator and discriminator are DCGANs
\end{appendices}


\end{document}
