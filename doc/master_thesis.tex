\documentclass[12pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{appendix}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage[boxed]{algorithm2e}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii,
sorting=nty,
%style=alphabetic
%style=reading
style=ieee,
backref,
citestyle=ieee
]{biblatex}
\DefineBibliographyStrings{english}{%
  backrefpage = {page}, % originally "cited on page"
  backrefpages = {pages}, % originally "cited on pages"
}
\addbibresource{bibliography.bib}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{  \tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\rulesep}{\unskip\ \vrule\ }

\begin{document}

%%%%%%%%% TITLE
\title{Learning Object Representations by Mixing Scenes}

\author{Lukas Zbinden\\
Computer Vision Group\\
Institute of Computer Science\\ 
University of Bern, Switzerland\\ 
2019\\
{\tt\small lukas.zbinden@unifr.ch}
}

\maketitle
%\thispagestyle{empty}

%=========================================================================
%%%%%%%%% ABSTRACT
\begin{abstract}
   Our project 
   abstract: in 4 sentences: state the problem; say why it's an interesting problem; say what your solution achieves; say what follows from your solution
   Do not use custom terminology in the abstract;
use terms and concepts that are widely accepted. Make it easy to see the main point!

% The entire story of your paper should be comprehensible from this abstract and the figures and tables
\end{abstract}

% TODO \newpage
% TODO \section*{Acknowledgements}
% TODO Thank Paolo, Qyiang, Simon, Adrian, Givi

\newpage
\tableofcontents
\newpage
\listoffigures
\newpage
\listoftables
\newpage

%%%%%%%%% BODY TEXT
% \chapter{Introduction}
\section{Motivation}
The dawn of deep learning in 2012 with the invention of the groundbreaking AlexNet neural network reignited the idea of an intelligent computer that is on a par with the human brain. After much disappointment in the late 21st century, the decade old research field of artificial intelligence and deep learning in particular was revived around the globe. While today research is still far away from the lofty aim of a truly intelligent machine, impressive achievements have been published continuously ever since the breakthrough. One research area in deep learning where progress has stood out in recent years is \textit{computer vision}. The field of computer vision aims at equipping a machine with artificial eyesight to process and interpret visual signals such as images and video. For instance, the computer has learnt to classify objects in images with an accuracy that exceeds human performance or identifies skin cancer with a precision higher than that of medical experts. Other significant applications such as autonomous driving or medical image analysis have emerged from the ability to artificially understand visual data and presumably many more are yet to follow. 

Within computer vision, one area of research focuses on the representation of visual data such as objects or attributes thereof in lower dimensional space, the so-called latent, feature or embedding space. Whereas an image resides in high dimensional space, the aim of \textit{representation learning} in the context of computer vision is to learn a model that successfully transforms a given image to a representation residing in a lower dimensional space that is meaningful and captures the significant image features. This type of dimensionality reduction opens up a multitude of new possibilities for downstream applications that make use of latent space representations. Going a step further, some research imposes additional constraints on the representation to lay the groundwork for more fine-grained applications. This research area is called \textit{disentangling factors of variation}. A factor of variation represents an image object or image attribute in latent space. The aim is to learn a representation that is not only low dimensional and meaningful but exposes a structure consisting of disentangled, independent factors of variation. With such a structure at hand, \textit{latent space interpolations} become possible where objects or attributes can be swapped, mixed or modified using arithmetic operations some of which often resulting in non-linear changes in the output image. Latent space interpolations thus present a powerful technique for image and video processing. Previous works have been successful at representation learning and disentangling factors of variation on a variety of datasets mostly either preprocessed and aligned or even synthetic. Many of these methods are supervised or semi-supervised and some are unsupervised.

In our work, we pose the bold question if it is possible to devise an unsupervised learning method effective at learning object representations directly from a natural, unprocessed dataset with several objects per image. Previous methods have been remarkable at achieving this task yet using simpler data mostly with one center-aligned object per image only. Our aim, however, is to find a method that performs as well as previous works but learns solely from a collection of random images with everyday scenes in their natural context. These images come with a varying number of objects per sample and add an extra layer of difficulty to the learning task. More specifically, we focus on learning representations and disentangling factors of variation in an unsupervised manner using natural image data. In a world of ever increasing data it has become infeasible to process the vast amounts manually and make sense of them using the generally higher achieving \textit{supervised learning} approach where the machine model is told the expected outcome for a given data sample. Here, \textit{unsupervised learning} comes to the rescue as a technique that learns directly from unlabeled data without the limitation of unsupervised learning requiring human annotation and can therefore better exploit the vast potential that lies therein. To that end, we propose a novel machine learning model aimed at learning object representations by mixing scenes. Given 2 real images showing different scenes, the model generates a new realistic scene that mixes the content of the 2 inputs. For instance, the 2 scenes could be pictures taken in a park with several persons and dogs in each. The generated image will in turn also represent a scene in a similar looking park with people and dogs mixed together from both input scenes. The model extracts objects from both scenes and joins them in a new made-up scene. The mixing of the objects is done in the latent space based on the extracted object representations. We name the resulting architecture of our model \textit{LORBMS} (Learning Object Representations By Mixing Scenes).

By successfully mixing and generating realistic scenes, the proposed method learns to extract objects from an image into well structured representations. It is such a representation that can be of most value to many downstream applications such as image search, classification, object detection, semantic segmentation, latent space interpolations and transfer learning tasks. This motivates our work.

\vspace{5mm}
% last paragraph: an abbreviated road map of the question you address and the means by which you answer it
Below we propose LORBMS, an unsupervised method to extract object representations and generate mixed scenes using a natural dataset. We investigate the question if the proposed model is able to learn object representations from a complex unstructured dataset where samples come with a varying number of objects. To the best of our knowledge, this has not been successfully achieved and published before. Our work builds on a previous method that successfully disentangles factors of variation in the MNIST, Sprites and CelebA datasets. We present this groundwork in Section~\ref{sec:related_work} along with other relevant prior works. In Section~\ref{sec:lorbms_method} we describe our method in detail. By both quantitatively and qualitatively evaluating the generated scenes along with other experimental results, we showcase the potential of our approach in Section~\ref{sec:experiments}.  

\subsection{Contributions}
Our contributions can be summarized as follows:
\begin{itemize}
  \item a novel method called LORBMS to learn object representations in a completely unsupervised manner
  \item a novel visual similarity detection algorithm
  \item an implementation of the proposed method
  \item engineering of neural architectures for the components of the method
  \item an implementation of the proposed detection algorithm based on MS COCO dataset
  \item extensive hyperparameter tuning
  \item a qualitative and quantitative study of the capabilities and limitations of the proposed model
\end{itemize}

\subsection{Challenges}
Here we list major challenges we expect for the proposed method as it faces the task of learning from natural image data.
\begin{itemize}
  \item \textbf{Unsupervised learning} Without any human annotation, the model has to find a way to learn from unlabeled complex image data solely and to generalize well to unseen data.
  \item \textbf{Learning object representations} The model should learn to produce low dimensional object representations that correlate well with the image input and capture the most relevant semantic content thereof to leverage downstream applications. The training images contain a varying number of objects at different locations which will significantly increase the learning difficulty.
  \item \textbf{Disentangling factors of variation} Ideally, the learnt object representations should expose a structure that consists of disentangled factors of variation. Each factor would represent an object or an object attribute from image space.
  \item \textbf{Unaligned natural dataset} A dataset with aligned data is a desirable property for a machine learning model because the amount of image data variability the model has to cope with is considerably lower than in a natural dataset where objects occur almost randomly in any variation. It can be observed in many representation learning papers that methods most often use relatively simple even synthetic datasets for experiments. For instance, CelebA is a face dataset where an image contains exactly one face which is often center aligned. In contrast, our model is applied to a natural dataset in which the data (e.g. objects) is inherently unaligned and comes with a high variability. Adding to the difficulty, the model makes the assumption that a given image contains up to 4 objects where each object is approximately aligned in one of the quadrants. The underlying dataset will therefore pose a substantial challenge to the model in the learning process.
  % TODO add more items here?
\end{itemize}


%=========================================================================
% \chapter{Background}
\newpage
\section{Related work}\label{sec:related_work}
% Secondly, it provides a critique of the
% approaches in the literature—necessary to establish the
% contribution and importance of your paper. 
% Critiquing the major approaches of the background
% work will enable you to identify the limitations of the
% other works and show that your research picks up where
% the others left off.
In this section, we first give a brief overview of neural networks. We start with MLPs first and then focus on autoencoders and CNNs as 2 relevant types of neural networks. Secondly, we give an introduction to GANs and a number of variations thereof. Thirdly, we review the 2 research areas this work is rooted in, namely \textit{representation learning} in general and \textit{unsupervised disentangling factors of variation} in particular. For the latter, we give an overview of previous major approaches along with their characteristics to establish the contribution and relevance of our work. Finally, we present a previous method as the groundwork on which our method is built.


\subsection{Multi-layer perceptron (MLP)}\label{subsec:mlp}
The multi-layer perceptron (MLP) is a generic and probably the simplest type of feedforward neural network. The MLP consists of an input and an output layer and one or more hidden layers in between. Every neuron of each layer is connected with all neurons of the previous layer. Each layer of an MLP is what is called a \textit{fully connected layer} and can be written as $y = \sigma (Wx + b)$ where $W$ is a weight matrix, $x$ the input, $b$ the bias and $\sigma$ an activation function, also called non-linearity. Each layer and each neuron, respectively, thus performs a dot product with the input and its weights and adds the bias. The result is passed to an activation function which introduces non-linearity to the layer. In fact, the activation function is what makes a stack of fully connected layers exploitable in the first place. Without non-linearity, all the fully connected layers in an MLP, with each layer being a linear function, simply represent a linear combination which is still a linear function in itself and could be collapsed into one single layer again. Therefore, the non-linearity is what makes the multi-layer perceptron inherently non-linear and enables it's expressive power. A single fully connected layer can also be used as a \textit{single layer classifier}, a type of \textit{linear classifier}. For instance, this is useful in transfer learning experiments (see Section~\ref{subsec:linearclassifier}). An important characteristic of an MLP is stated by the universal approximation theorem~\cite{mlpUnivApprox}. It essentially says a feedforward neural network with at least one hidden layer, sufficient neurons and a suitable activation function (e.g. ReLU) can approximate any possible continuous convex function. So an MLP can be thought of as a \textit{universal function approximator}. We can thus exploit the expressive power of neural networks to tackle problems and tasks yet unsolved and unachieved. The MLP is a typical example of a deep learning model and is usually trained using the stochastic gradient descent (SGD) optimization method.

% TODO add a few sentences about NN and complexity theory - Turing complete? NP vs P?

A reason why we do not always go to an MLP to accomplish a task is their exponential demand in computational and memory resources as the size of the network grows (i.e. number of neurons). For instance, each layer, which is always a fully connected layer, has a fixed number of neurons each of which is connected to all neurons of the previous layer which results in a considerable amount of weights per neuron and equally many computations in the forward as well as the backward pass while training. While the architecture is simple in its structure it is inflexible in some respects. Every neurons' receptive field spans the entire input and it is not possible to limit the receptive field of a neuron (cf. Section~\ref{sec:rf}). MLPs are great for approximating many non-linear functions but not necessarily a good choice for computationally and memory intensive tasks such as image processing. For the latter case, a convolutional neural network (CNN) is a more suitable choice. We describe more specific types of neural networks such as autoencoders, CNNs and GANs in the following sections.
% COULD 19.03: mention Lipschitz continuity here? context:
% theory: a NN with dense blocks cannot guarantee the Lipschitz continuity / is not Lipschitz % continuous anymore -> but you need that in the discriminator (?)
% theory: why do we need Lipschitz continuity in the discriminator? -> ask Qyiang
% see C:\Users\lz826\Dropbox\UniFR\Courses\ATML\Lectures\ALL\week 04-06 - Deep Feedforward Networks - Intro.pdf

\subsection{Autoencoder}
\subsubsection{Overview}
An autoencoder is a type of neural network suitable for unsupervised learning. It can be used for instance as a generative model, for representation learning or dimensionality reduction. An autoencoder consists of an encoder network $f_{\theta}$ and a decoder network  $g_{\theta}$ as depicted in Figure~\ref{fig:autoencoder}. The encoder takes a high dimensional input instance $x$ and encodes it into a lower dimensional latent space representation $z$. Its task is to encode as much of the relevant information from the input into the compressed $z$ (while leaving the irrelevant information out) in order to allow the decoder to reconstruct the input $x$ as $\hat{x}$ as faithfully as possible.
\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/autoencoder_schema.png}
\caption{Overview of an autoencoder \cite{chollet_autoencoders}.}
\label{fig:autoencoder}
\end{figure}
A common objective function for an autoencoder is to minimize the pixel-wise reconstruction loss $\Vert x - \hat{x} \Vert^2$, also called squared error. Both networks are trained jointly using SGD. Note that this architecture does not enforce any kind of structure on the latent representation. Both the encoder and decoder networks can be implemented as MLPs or CNNs depending on the type of input data. For image data, CNNs are normally used.

Autoencoders are often used in unsupervised learning. More specifically, an autoencoder learns in a self-supervised manner in which the labels (or ground truth) are generated from the input data. In a standard autoencoder the label is exactly the input because the autoencoder tries to reproduce the input as closely as possible. 

Autoencoders are applied in many contexts. The encoder is often used as a \textit{feature extractor} where the representation $z$ being the "feature extract" is used for downstream tasks other than reproducing the input. The decoder can be used as a generative model whose input $z'$ is different than $z$ obtained from the encoder. Autoencoders also play an important role not only in our work but in the work by Hu \textit{et al.}~\cite{DisentFacOfVarByMixTh}. In particular, they make use of a \textit{mixing autoencoder} which we describe next.


\subsubsection{Mixing Autoencoder}\label{subsec:mixAE}
A mixing autoencoder, a term introduced by Reed \textit{et al.}~\cite{DeepVisAnaMak} and coined in \cite{DisentFacOfVarByMixTh}, refers to a neural network architecture with an autoencoder where multiple instances of the encoder are used in parallel to project a set of data points to latent space. An example is shown in Figure~\ref{fig:mixing_autoencoder}. All encoder instances share one set of weights. The latent space representations are then "mixed" in some way and finally projected back onto data space using the decoder. As with the encoder, multiple instances of the decoder with shared weights can exist. As a result of the latent space mixing, the output $x_3$ can mean any kind of mixture of the inputs $x_1$, $x_2$.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.7]{images/mixing_autoencoder.png}
\caption{Schematic of a mixing autoencoder \cite{DisentFacOfVarByMixTh}.}
\label{fig:mixing_autoencoder}
\end{figure}

Mixing autoencoders provide a structural prior for disentangling latent representations and is used as part of the respective method in \cite{DeepVisAnaMak}, \cite{DisentFacOfVarByMixTh} as well as in our work.


%=========================================================================
%=========================================================================
% CONVOLUTIONAL NEURAL NETWORK (CNN)
\subsection{Convolutional Neural Network (CNN)}
\subsubsection{Evolution}
%-- CNN start -------------------------------
Deep convolutional neural networks (CNNs), also called \textit{convnets}, are among the most common type of neural networks. With respect to image processing and in comparison to MLP networks, CNNs have achieved greater success and are more time and space efficient as described later. As a result, CNNs are ubiquitous in computer vision applications. In 1979, Fukushima~\cite{FukushimaCnn79} and in 1989, Le Cun \textit{et al.}~\cite{CunGrounworkCNNs} laid significant groundwork for CNNs~\footnote{this is not an account of the history of CNNs}. But it was not until 2012, when Krizhevsky \textit{et al.}~\cite{AlexNet} released a CNN model which went on to win the annual \textit{ImageNet} large-scale visual object recognition challenge (ILSVRC)~\cite{Imagenet} with an unparalleled accuracy and by a significant margin, that CNNs gained widespread research attention in fields such as object detection~\cite{FasterRCNN}, image recognition~\cite{AlexNet,DenseNet}, semantic segmentation~\cite{DeepLabv3+}, instance segmentation~\cite{MaskScoringRCNN} and image captioning~\cite{ImageCaptioningSOTA}. CNNs as a key ingredient in the respective architectures have helped to achieve unprecedented state-of-the-art performances. And CNNs continue to do so today. A reason for the success of CNNs on image data could lie in their ability to act as automatic feature extractors where a feature is a condensed piece of information that represents an essential semantic content of an image. A feature is used as a core input for many downstream tasks. In contrast, explicit math approaches to understanding image data have not been as successful. Probably because the rigidity of math does not match up well with the inherent complexity and unstructuredness of natural image data. CNNs as universal function approximators and feature extractors, respectively, have adapted much better to this kind of data.

\subsubsection{Architecture and design}
The design of a CNN architecture is not a rigorously defined procedure but rather an art depending on the task to solve. For instance, given an image recognition task, a CNN is followed by a classifier often in the form of an MLP. In contrast, to obtain a generative model, a CNN would be followed by a decoder. A CNN architecture therefore is the result of an assembly of a number of individual components and layers, respectively. Those are (re-)used across a wide range of different types of CNNs. Considering Figure~\ref{fig:simple_cnn_arch}, we describe relevant CNN components along with their roles within the architecture  next. 
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/simple_cnn_architecture.png}
\caption{A simple CNN architecture for image recognition \cite{cnn_arch_article}.}
\label{fig:simple_cnn_arch}
\end{figure}

\paragraph{Convolutional (conv) layer} Performs the convolution operation between its input, which is either the input image itself or the output of the previous layer, and its kernels. The kernels are the set of weights that belong to a layer. The result of the convolution is called the feature map or activation map and stores the information if and where a certain feature occured in the input. That is the intuition, the convolutional layer looks for a specific local structure and does so across the entire input. The orchestration of a number of convolutional layers will result in the ability of the CNN to detect higher level features such as objects. See Section~\ref{subsec:innerworkconvl} for more details. 

\paragraph{Kernel/filter            } Holds the weights of a convolutional layer. The kernel is shared between all neurons of the convolutional layer unlike in a fully connected layer where each neuron has its own sets of weights. The number of kernels a layer has (a hyperparameter) depends on the chosen depth of the output volume such that the number of kernels equals the number of output channels. The kernel is applied to the input by using a dot product. One dot product results in a scalar which represents exactly one value in the resulting feature map. The size of the kernel is a hyperparameter and is often 1, 3 or 5. Intuitively, each kernel is used to learn to detect a certain feature across the input volume.

\paragraph{Stride                   } Defines the step size of the kernel as it slides across the input volume. It is often 1 or 2. With a stride greater than 1 the spatial output dimension will always be smaller than the input. Thus a convolutional layer can also be used to reduce the spatial dimension of an input as an alternative to the pooling layer.

\paragraph{Padding					} The padding controls the output size of the feature map of a convolutional layer. If the padding is zero (in ML frameworks often denoted by \textit{VALID}), the kernel stays inside the input dimension and the output size will be smaller. A padding greater than zero (in ML frameworks often denoted by \textit{SAME}) causes the input volume to be padded with zeros such that the output size of the convolution is the same as the input size.

\paragraph{Normalization layer      } Oftentimes a convolutional layer is followed by a normalization layer before the activation function. Normalization of the activations of a convolutional layer can help make the learning process of the network more effective. Common normalization types are \textit{batch norm}, \textit{instance norm} and \textit{spectral norm}. Which type to use depends on the problem task at hand. Not always is the use of a normalization layer effective. For instance, in an encoder aiming at disentangling factors of variation, batch norm would be counterproductive as it normalizes activations across the mini-batch smoothing out too many details whereas instance norm works on one training example only and thus could be a much better choice. Batch norm, however, works great with CNNs for image classification.
% COULD: add more details re normalization

\paragraph{Non-linearity layer (activation function)} A non-linearity layer consist of an activation function which takes the output of a convolutional layer and processes it element-wise to produce the activation map~\footnote{activation map and feature map are often used interchangeably in the literature}. The activation function essentially enables the CNN to learn a non-linear function. Without it, a sequence of convolutional layers would collapse into one linear function and building deeper models would not be worthwhile. Common activation functions are Sigmoid, Tanh, Softmax and the rectified linear unit (ReLU), presumably the most common. 
% COULD: add more details re activation function

\paragraph{Pooling layer            } The pooling layer's sole responsibility is to reduce the spacial size of the activation maps. The idea is to drop irrelevant information so as to keep the most important activations only, thereby to focus on the "essence" of a feature and getting rid of the rest to ultimately perform tasks based on very compact but highly decisive information. Common types are max pooling and average pooling. The output of a pooling layer will be the same independently of where a specific feature is located inside its pooling region as the maximum or the average will be the same. This contributes to the translation-invariant property of convnets. Note that the pooling operation also helps to build the abstraction hierarchy in the convnet as it drops irrelevant information and thus makes the retained information more "abstract". 

\paragraph{Fully connected layer    } As illustrated in Figure~\ref{fig:simple_cnn_arch} the feature maps resulting from the convolutional layers are passed to a fully connected layer. In fact, the last 3 layers represent an MLP as described in Section~\ref{subsec:mlp} except that the input is not a vector but the activation maps (a 3D tensor) from the previous layer. The fully connected layers act as a classifier that receive the high level features from the convolutional layers as input and outputs a class probability distribution representing the class predictions for the input image. Note that a fully connected layer with $n$ neurons is equivalent to a convolutional layer with $n$ 1x1 kernels.
% https://wiki.tum.de/display/lfdv/Layers+of+a+Convolutional+Neural+Network#LayersofaConvolutionalNeuralNetwork-ConvolutionalLayer

\paragraph{Output layer} 
The output layer depends on the task performed by the CNN. For a regression task, it could be the output of one single neuron. For an image recognition task as in Figure~\ref{fig:simple_cnn_arch}, it would imply a layer with $n$ neurons where $n$ is the number of classes. In addition, for single-class images the output layer would use a softmax function to normalize all inputs into a probability distribution with all predictions for the $n$ classes summing up to 1 (along with the binary cross-entropy loss).  For multi-class images, the output layer would use a sigmoid function to assign an independent probability to each of the available classes (along with the categorical cross-entropy loss).


\subsubsection{Inner workings of a convolutional layer}\label{subsec:innerworkconvl}
The convolutional layer is the main building block of a CNN. For simplicity we will only consider image data as input to a CNN. This implies that any input is 3 dimensional $h \times w \times c$ where $h$ is the height of the image, $w$ the width and $c$ the number of channels, which for colored images is 3 and for grey images 1.  In subsequent layers however $c$ can take on any number and represents the number of feature and activation maps, respectively, that a layer outputs. The convolutional layer receives as input a 3 dimensional tensor and produces as output a 3 dimensional tensor. Each dimension in the output can be different from the respective dimension in the input, but usually the spatial dimension is quadratic, i.e. $h = w$. When a convolution is applied, the spatial dimension of the tensor is often reduced depending on the kernel size and the stride. Simultaneously, the depth dimension $c$ is often increased to compensate for the reduced spatial dimension. Therefore, the spatial input information lost in the convolution operation "lives on" in the output, namely in the activation maps of the depth dimension. A kernel of a layer looks for a distinct feature in the input and is activated by it when detected and produces the activation map. Each activation map stores the occurrence of a distinct feature detected in the input tensor (e.g. an edge, a texture, an object) and carries that information to the subsequent layer. Consequently, the depth dimension should be large enough to account for a sufficient amount of distinct features to detect in the input. To showcase the inner workings of a convolutional layer $i$ consider Figure~\ref{fig:conv_layer}. 
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/convolutional_layer.png}
\caption{A convolutional layer $i$ with its input and output volume. By convolving each of the 15 kernels $k_i$ across the input volume the 15 activation maps of the output volume result. Note that as depicted one convolution operation $f * k$ produces exactly one value in one activation map. $f$ is the receptive field of $s$ and equals the size of $k$.}
\label{fig:conv_layer}
\end{figure}
Layer~$i$ receives as input the output tensor of the previous layer~$i-1$ and produces itself an output which becomes the input to the subsequent layer $i+1$. $c$ in the output volume refers to the number of output channels and activation maps, respectively. Each activation map represents the result of convolving (or sliding) each of the kernels (also filter) across each input channel of dimension $h \times w$ and captures the feature the kernel has learnt to detect. If a kernel detects edges, the activation map will only be non-zero if the input volume contains an edge that activates the kernel. Because a convolutional layer is \textit{translation-invariant}, that is, the same kernel is convolved over the entire spatial dimension of the input volume, it will detect any edge irrelevant of its spatial location. Note that the depth of the kernel (e.g. 6 in Figure~\ref{fig:conv_layer}) always equals the depth of the input volume because the convolution operation, that is the dot product of the receptive field of the kernel in the input volume with the kernel, always spans the entire depth dimension. 

With each subsequent layer, the convnet can detect more elaborate features. In the first few layers of the network the features detected are low level such as edges or curves. As these features are combined and processed together in subsequent layers, the features learnt become more high level such as objects and even later represent abstract concepts such as faces, arms, legs \cite{DeepVis}.

A sequence of convolutional layers as in Figure~\ref{fig:conv_layer} represents a series of linear functions which is again a linear function. In order to allow it to model non-linear functions, each convolutional layer is followed by an activation function (e.g. ReLU). The activation function is what introduces the non-linearity to the CNN.

\textbf{Time and space complexity} In case the kernel size is greater than $1 \times 1$, the convolutional layer is much more time and space efficient in comparison with a fully connected layer. In the convolutional layer each neuron is only connected to a few neurons of the previous layer and it shares its weights with neighboring neurons whereas in a fully connected layer each neuron is connected with all the neurons in the previous layer with no weight sharing between neighboring neurons. This impacts not only the number of computations (time) to perform in a forward or backward pass but also the number of weights to keep in memory (space). In case the kernel size is equal to $1 \times 1$ and the stride is 1, the convolutional layer is equivalent to a fully connected layer.


\subsubsection{AlexNet: case study of a seminal CNN}\label{subsubsec:alexnet}
As a real world example, consider the convnet named \textit{AlexNet}~\cite{AlexNet} in Figure~\ref{fig:AlexNet}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.9\textwidth]{images/alexnet-cnn-architecture-layers_ann.png}
\caption{The AlexNet convnet \cite{AlexNet}.}
\label{fig:AlexNet}
\end{figure}
AlexNet consists of 8 layers with weights where the first 5 are conv layers (\textit{conv1 - conv5}) followed by 3 fully connected layers (\textit{fc6 - fc8}). The latter is essentially an MLP classifier. AlexNet has 62.3 million trainable parameters of which only 3.7 million (6\%) come from the convolutional layers. The \textit{input} is an image of size $224 \times 224 \times 3$ (in fact, 227 pixels were used in \cite{AlexNet} unlike stated in Figure~\ref{fig:AlexNet}). Across the 5 conv layers, the main part of the feature extraction with respect to the input takes place. In this process the spatial dimension of the input is reduced from $227 \times 227$ to $6 \times 6$ using kernels of decreasing size before the resulting $6 \times 6 \times 256$ activation map is passed to the classifier. To introduce non-linearity to the network, all the 8 layers are followed by the ReLU activation function except fc8 which is followed by the softmax function. Moreover, conv1 and conv2 are followed by normalization layers to help in generalization (however, they are not common anymore). To help reduce spatial dimension and as a measure against overfitting, max-pooling layers are employed after conv1, conv2 and conv5. 

AlexNet as presented in Figure~\ref{fig:AlexNet} is a non-linear image classifier. Under the hood the network can also be considered a concatenation of a feature extractor (conv1 - conv5) and a non-linear classifier (fc6 - fc8). The feature extractor part, for instance, can be used for transfer learning. AlexNet is still widely used today as a well established and de facto standard convnet.
% see http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf
% https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf

\subsubsection{Receptive field and its implications}\label{sec:rf}
In fully connected networks, the output value of any neuron depends on the entire input to the network, that is all the pixels in case of an image input. In contrast, the output value of a neuron~\footnote{a CNN neuron is equivalent to the dot product of a filter of a convolutional layer with the input tensor} in a CNN only depends on a region of the input image, the so called \textit{receptive field} (RF), or \textit{field of view}, for that neuron. Only the region defined by the receptive field of a neuron impacts the output value of that neuron. Anywhere outside that region does not affect the outcome. Note that in case of an MLP the receptive field of any neuron always spans the entire input image. We assume the RF is square shaped and the size of each side is given in pixels. The receptive field of a neuron can be considered with respect to the previous layer or with respect to the input image (or any of the previous layers). Considering only the previous layer, the receptive field of the current layer is equal to its filter size. It follows that only if the filter size of a layer is greater than 1x1 will the receptive field increase. Assuming that is the case for all layers, the network starts off with a very small receptive field that allows it to learn only low level features such as edges or curves. The next layer then looks at a combination of receptive fields from the previous layer which means a larger receptive field. As the network goes deeper, the receptive field continues to increase and so each neuron of the next layer looks at yet a larger region of the input image and is therefore capable of learning more abstract features such that the network can eventually learn high level features such as objects or regions. In conclusion, with respect to the input image, the receptive field size of any layer in the CNN is a function of the number of previous layers along with the filter size and stride of each layer \cite{li_recept_field_2017}.

What needs to be ensured wit respect to the receptive field when designing a convolutional network is that it covers the entire relevant region of the input image. What the relevant region is depends on the application.
In case of a classification, object detection or a segmentation problem the receptive field of the CNN should span the entire input image.  The size therefore can be too small when important information to the problem is left out, for instance smaller than the size of the object to detect. Likewise, the field can be as big as the input image or larger such that each output value in the last layer is affected by the whole input image. With such a large field it might be difficult for a CNN to disentangle multiple objects in the input image if every pixel of the input image maps to every element of the feature vector. 

In short, it is crucial to consider the receptive field and its size, respectively, with respect to the problem at hand when designing a convolution network.


\subsubsection{Unsupervised training of CNNs}\label{subsec:deepcluster}
CNNs are often trained in a supervised setting where human annotated labels are available to drive the learning process. A major challenge in the realm of unsupervised learning is the lack of labels to direct the learning process. In supervised learning the availability of labels as ground truth (usually by human annotation) allows us to define loss functions that measure the accuracy of the prediction with respect to the label and thereby providing an error signal to perform backpropagation with and realize the learning process, respectively. Using labels we can easily train a CNN as a regression or classification task. In unsupervised learning however this is not as straightforward since ground truth is nonexistent. The task is still to train a CNN but in an unsupervised manner. A widespread method in unsupervised learning is clustering. A recent approach using clustering is presented in the work by Caron \textit{et al.} \cite{DeepCluster}. Their novel \textit{DeepCluster} technique is used to perform unsupervised training of CNNs. In a first step, a (randomly initialized) convnet without the classifier is taken to produce the features for all images from a large dataset by simple forward passing. Those features are then grouped using the clustering algorithm \textit{k}-means into \textit{k} clusters. Notice that when comparing the images of the features, the features belonging to the same cluster will also appear similar in the image space on a semantic level but not necessarily on a pixel level. Each of those clusters then becomes the pseudo-label for all the features belonging to that cluster. In a second step, with the classifier appended to the convnet, the convnet is trained in a supervised manner using the pseudo-labels, cross-entropy loss and backpropagation. The convnet has to predict the correct cluster for a given image. After each epoch, this 2 step process is repeated for a number of times. 
\par What results is a trained convnet that is capable of extracting visual features from images and achieves notable performance in many standard transfer tasks such as classification or object detection.
%-- CNN end -------------------------------


%=========================================================================
% GAN
%=========================================================================
\subsection{Generative Adversarial Network (GAN)}
\subsubsection{Vanilla GAN}
%-- GAN start -------------------------------
This type of neural network, the so-called Vanilla GAN, was introduced in 2014 by Goodfellow \textit{et al.}~\cite{1406.2661} and has inspired a significant amount of research and progress ever since. It is called ``the coolest idea in deep learning in the last 20 years'' by AI pioneer Yann LeCun and ``a significant and fundamental advance'' by AI luminary Andrew Ng. The GAN network defines a learning framework for synthetic image generation in which 2 neural networks, usually convolutional, namely the generator $G$ (a generative model) and the discriminator $D$ (a discriminative model), compete against each other in a so called minimax game. The framework includes a dataset from which 2 things result. Firstly, the dataset's images are used by the discriminator to learn to discriminate real from fake images. Real images come from the dataset, fake images come from the generator. Secondly, the dataset is defined by an implicit probability data distribution $p_d$ which all its samples follow. The generator's objective is to learn and approximate, respectively, the data distribution $p_d$ and by that generate new, realistic images that look similar to samples from the dataset. Thus the generator implicitly defines a probability model distribution $p_g$ as the distribution of the samples obtained from the generator given some input $z$, often a Gaussian noise. In other words, the discriminator learns to determine whether a sample is from $p_d$ or $p_g$ and the generator learns to invent samples from $p_g$ that closely resemble samples from $p_d$ and ultimately confuse $D$ in its decision. Even though so far GANs have received the most research on image data, the idea of learning a data distribution is more general and should lend itself to other types of input. Generally, at inference time only the generator is used to create new images. However, the discriminator can also serve other purposes such as a classifier (\cite{ImprTechn4TrainGANs}) or for transfer learning (e.g. \cite{SpotArtifacts}).
\par \textbf{Minimax game} The minimax game comes into play in the learning phase. The 2 models are simultaneously trained so that $G$ learns to generate samples that are hard to classify by $D$, while $D$ learns to discriminate the samples generated by $G$. As training evolves, $D$ forces $G$ to produce even better samples. This setting can be viewed as a two-player game where both players (models) try to minimize their own loss and the final state of the game is the Nash equilibrium where both players cannot improve their loss anymore. Ideally, when $G$ is fully trained, it should not be possible for $D$ to perform any better than randomly guessing. The two-player minimax game is formally defined as
\begin{equation} \label{eq:1}
\min\limits_{G} \max\limits_{D} V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}\big[log\, D(x)\big] + \mathbb{E}_{z\sim p_{z} (z)}\big[log(1 - D(G(z)))\big]
\end{equation}
where $z$ is a random noise. $G$ tries to minimize $log(1 - D(G(z))$ (or maximize $D(G(z))$) such that $D$ gives a high probability for input $G(z)$, while $D$ tries to maximize $log(D(x)) + log(1 - D(G(z))$ with a high probability for $x$ and a low probability for $G(z)$. The 2 networks are trained in an alternating manner: for every training batch, one network is fixed and the other is updated so that backpropagation happens in one network at a time exclusively. In an ideal convergence scenario, $G$ has learnt to produce samples so realistic that $D$ cannot discern them from real images and assigns probability 0.5 to every image (Nash equilibrium). In this state, $D$ cannot teach $G$ anymore how to improve, convergence has set in. Note that the expectation value is practically the average over the training images x.

\par \textbf{GAN loss functions} 
Multiple formulations of the GAN loss functions exist (\cite{GANLandscape}). In this work we focus on the non-saturating GAN loss introduced in \cite{1406.2661} as the empirical evaluation in \cite{GANLandscape} suggests to favor this version when applying GANs to a new dataset. The hinge loss version has also shown good performance in \cite{SNGAN} but \cite{GANLandscape} concludes that it performs very similar to the non-saturating loss.

The discriminator loss function is
\begin{equation} \label{eq:d_loss}
    \mathcal{L}_{D} = -\mathbb{E}_{x\sim p_{data} (x)}\big[log(D(x))\big] - \mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(1 - D(\hat{x}))\big]
\end{equation}
where $p_{data}$ denotes the (true) data distribution and $p_{model}$ the model distribution. It is equivalent to the binary cross entropy between the data and the model distributions of real and generated images ($D$ has to discriminate between 2 classes).

The generator loss function is
\begin{equation} \label{eq:g_loss}
    \mathcal{L}_{G} = -\mathbb{E}_{\hat{x}\sim p_{model} (x)}\big[log(D(\hat{x}))\big]
\end{equation}
where the generator creates samples $\hat{x} = G(z)$ which the discriminator believes to be real $D(\hat{x}) \approx 1$ and thereby trying to approach $log(1) = 0$.
%-- GAN end -------------------------------

\par \textbf{Joint loss function}
Similar to our work, a number of other methods (\cite{1511.05440},\cite{DisentFacOfVarByMixTh},\cite{1604.07379}) use a joint loss function for the generator which is composed of 2 or more losses and oftentimes includes a reconstruction loss and an adversarial loss $\mathcal{L}_G = \lambda_{rec}\mathcal{L}_{rec} + \lambda_{adv}\mathcal{L}^G_{adv} + \lambda_{i}\mathcal{L}_{i}$ where $i$ is a method specific additional loss. To look up the weightings in other works proves useful to find reasonable values more efficiently. For instance, \cite{1604.07379} uses $\lambda_{rec} = 0.999$ and $\lambda_{adv} = 0.001$ on ImageNet whereas \cite{DisentFacOfVarByMixTh} deploys $\lambda_{rec} = 30$ and $\lambda_{adv} = 1$ on CelebA.

\subsubsection{Significant GAN works}
Here we summarize significant types of GANs that have evolved from the original GAN formulation and which have influenced our work. See \cite{theGanZoo} for a comprehensive list of other GAN works.

%-- [24] DCGAN start -------------------------------
\par \textbf{DCGAN} \cite{DCGAN} introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrates for the first time that they are a strong candidate for purely unsupervised learning. The authors propose a more stable set of architectures for training generative adversarial networks based on convolutional layers and give evidence that adversarial networks learn good representations of images. Moreover, while the vanilla GAN could not yet produce visually realistic CIFAR-10 like images, DCGAN for the first time showed that it was possible to generate authentic images using a GAN. According to Ian Goodfellow, DCGAN defines a quasi-standard generator network architecture. Accordingly, DCGAN has become the de facto backbone architecture for many other subsequent types of GANs.
%-- [24] DCGAN end -------------------------------

%-- [31] SNGAN start -------------------------------
\par \textbf{SNGAN} \cite{SNGAN} presents a novel weight normalization technique for stabilizing GAN training, the \textit{spectral normalization} (SN). They propose to add SN to the GAN discriminator. This normalization technique also helps GANs to prevent collapse at the beginning of training and to produce more diverse images, which is significant because the lack of diversity in image generation is a problem for GANs. GANs with SN achieve better or comparative inception scores. We use it in both our GAN discriminator and generator as discussed later.
%-- [31] SNGAN end -------------------------------

%-- [32] SAGAN start -------------------------------
\par \textbf{SAGAN} \cite{SAGAN} introduces the self-attention mechanism to convolutional GANs. It helps both the generator and the discriminator to model relationships between widely separated spatial regions. The self-attention module is thus a non-local operation. It complements the nature of convolutions in that they look at only a small local region of the input (see Section~\ref{sec:rf}). Their propopsed SAGAN achieves best FID and IS results on the ImageNet dataset. Another finding is that while \cite{SNGAN} proposes to add spectral normalization to the GAN discriminator, adding it to the GAN generator improves the training further. We adopt this finding in our method where we add spectral normalization to the generator (i.e. decoder) as well. 
%-- [32] SAGAN end -------------------------------

%-- [33] TTUR start -------------------------------
\par \textbf{TTUR} Heusel \textit{et al.}~\cite{TTUR} propose the 2 time-scale
update rule (TTUR) to help stabilize the training of GANs. In TTUR, the generator and discriminator have separate learning rates unlike original GAN training. With TTUR, they show, the GAN training essentially converges to a local Nash equilibrium meaning it is beneficial for a good learning dynamic. We make use of TTUR in our method to help stabilize our DCGAN training. Moreover, they introduce the \textit{Fréchet Inception Distance} (FID), a metric to measure the quality of generated images. FID basically measures the similarity of the generated images with a set of real images. The FID metric is consistent with human judgment and unlike the Inception Score (IS), another popular metric, it has the advantage that it takes into account real images.
%-- [33] TTUR end -------------------------------

%-- [29] DNA-GAN start -------------------------------
\par \textbf{DNA-GAN} In \cite{DnaGan}, Xiao \textit{et al.} propose the supervised DNA-GAN by using an analogy with the DNA double helix structure, where different kinds of traits are encoded in different DNA pieces. In the same way, they argue that different visual attributes in an image are controlled by different attribute-relevant parts in the latent representation of that image. Similar to our model, for 2 parallel input images, they use an encoder to retrieve their representations and a decoder to generate new images whose realism is judged by a discriminator (GAN). Additionally, they use labels to support the disentanglement in the latent space where image pairs are required to have different labels for a certain attribute (e.g. smiling or not). The respective part of a label in the latent representations of the image pair is then swapped to obtain 2 crossbreed representations and images, respectively. DNA-GAN is able to disentangle latent representations and allow for interpolations on datasets such as celebA.
%-- [29] DNA-GAN end -------------------------------

%-- [66] Conditional GAN, PatchGAN start -------------------------------
\par \textbf{Conditional GAN, PatchGAN} Isola \textit{et al.}~\cite{CondGAN_PatchGAN} use a conditional GAN to achieve impressive image-to-image translation. A conditional GAN (cGAN) learns a mapping from an observed image $x$ and a random noise vector $z$ to an output image $y$, $G : \{x,z\} \rightarrow y$. Moreover, they introduce the PatchGAN discriminator which penalizes at the scale of image patches rather than the entire image. They argue that using L1 encourages less blurring than L2 and that since L1 already enforces correctness at the low frequency level, it suffices to have a discriminator that pays attention only to high-frequencies and to the structure of local image patches, respectively. The PatchGAN classifier decides for each patch in an image if it is real or fake. The PatchGAN can be understood as a form of texture/style loss.
%-- [66] Conditional GAN, PatchGAN end -------------------------------

%-- [26] INFOGAN start -------------------------------
\par\textbf{INFOGAN} Chen \textit{et al.}~\cite{InfoGAN} describe InfoGAN, one of the first GANs to learn disentangled representations in an unsupervised manner. On top of the GAN framework, InfoGAN maximizes the mutual information (from information theory) between a subset of the latent factors and the generated images. InfoGAN can disentangle both discrete and continuous latent factors and be applied to more complicated datasets such as celebA. The results suggest that generative modelling enhanced with a mutual information cost is a fruitful approach for learning disentangled representations. INFOGAN disentangles latent factors such as digit shape and rotation on MNIST, pose and lighting on 3D faces or hair style and emotion on celebA. They use DCGAN as implementation for stable GAN training. 
%-- [26] INFOGAN end -------------------------------

% COULD 26.02 evtl add style gan

%=========================================================================
%=========================================================================
% Representation learning
\subsection{Representation learning}
Representation learning~\cite{ReprLearning} or feature learning~\footnote{\textit{representation}, \textit{feature} and \textit{feature vector} are often used interchangeably} is a field within machine learning that aims at automatically discovering and extracting representations from raw data~\footnote{i.e. real world data such as images, video and sensor data} which can be used as input for many downstream tasks. While technically a representation can simply be a vector of numbers, semantically a representation is of good quality when it contains the discriminative information from the input data in an organized way and in a much lower dimension. A good representation is \textit{expressive}, meaning that it can express many concepts from the data space. Such a representation can leverage predictors or classifiers as they can extract decisive information more easily. In that sense, a good representation is one which makes the downstream task easier to solve. Yet the downstream task might be unknown at the time of learning the representation. Some key properties of a good representation are \textit{distributed}, \textit{invariant} and \textit{disentangled factors of variation}.

\textbf{Distributed representation}
What we aim at is a distributed representation that has the capacity to represent an exponential number of concepts as opposed to a local (non-distributed) representation in which only linearly many concepts fit. In a local representation, one neuron represents one thing (one-to-one relationship). In contrast, many neurons represent one concept in a distributed representation and a neuron participates in the representation of many concepts (many-to-many relationship, see \cite{DistrRepHinton}). It follows that a distributed representation also lends itself better to the notion of generalization. When a neural network encounters unseen data at inference time, it has a better chance of representing that input with a distributed representation as parts can be reused to represent something new. Neural networks such as autoencoders mostly use distributed representations by design.

\textbf{Invariant representation}
A representation is invariant if changes in the input data that are irrelevant to the concept being represented do not affect the representation itself. In other words, the representation remains unchanged as long as the input data is transformed in a way that is unconnected to the represented concept. While intuitively this property makes sense, it is difficult to achieve in practice as it is a challenge already to even learn a proper representation of a concept. The invariance property is also related to the disentangling property described next.

\textbf{Disentangled factors of variation} An important assumption for representations of real world data is that there are multiple \textit{explanatory factors} or \textit{factors of variation} that underlie the data generating distribution. An additional assumption is that there exists a \textit{hierarchical organization of explanatory factors} in which more abstract concepts to describe the real world are defined in terms of less abstract ones lower in the hierarchy. Therein also lies an advantage of using deep architectures as they can potentially learn more abstract factors at higher layers. More abstract has the advantage that it is more \textit{invariant} (i.e. robust) to local changes in the input (e.g. a cat is still a cat no matter the surrounding scene or the change in lighting). Therefore, a representation that comprises highly abstract concepts becomes a highly non-linear function of the raw input. To identify and disentangle the explanatory factors is an important objective of representation learning and is further detailed in Section~\ref{subsec:unsupDFoV}. Achieving this, as \cite{ReprLearning} proposes, should result in a good representation that is significantly more robust to the complex variations in real world data. 

With respect to real world data, attempts to extract representations with rigorous explicit algorithms (e.g. pure math) have not been as promising as approximative approaches with neural networks. Significant in using deep learning is that the system can automatically perform feature engineering, otherwise a labor-intensive hand-crafted process including feature extraction, feature importance and feature selection. This allows to construct new applications faster and allows for more progress towards an AI system that can understand the world around it.

The notion of a representation is usually associated with a latent vector such as in an autoencoder. Autoencoders provide a suitable framework for representation learning. In our work we use mixing autoencoders (see Section~\ref{subsec:mixAE}) to drive the learning of object representations. Representation learning can be carried out in supervised or unsupervised form. In unsupervised learning only unlabeled data is used. Our method is fully unsupervised.

Some examples of representation learning models are variational autoencoder (VAE) \cite{VAE}, $\beta$-VAE \cite{betaVAE}, UNIT \cite{UNIT}, InfoGAN \cite{InfoGAN}, DNA-GAN \cite{DnaGan}.

% COULD: also check video/slides from winterschool Paolo on Rep Learning:
% https://drive.google.com/file/d/1MWY93qIDP9Y6sTNuJONT002NUXU_Tpz-/view


%=========================================================================
% Manifold learning and latent space interpolation
%=========================================================================
\subsection{Manifold learning and latent space interpolation}
Manifold is an important concept in machine learning based on which concepts such as latent space, latent vector, representation and latent space interpolation can be better understood. 

A \textit{manifold} can be considered a connected region in the latent space of a manifold learning algorithm. It has a much smaller dimensionality than the original data space. This is exploited in, for instance, autoencoders as is the case with our model. A manifold represents the input data in a condensed space with an intrinsic coordinate system that can be traversed and whose coordinates (i.e. points) can be projected back onto input space. As a loose definition, a manifold consists of a connected set of points where each point is surrounded by a neighborhood of other points~\cite{DeepLearningBook}. The existence of a neighborhood should allow for transformations to move on the manifold from one point to a neighboring one. The dimensionality of the manifold can vary from one point to another (e.g. manifold intersections). However, often the manifold, embedded in a higher-dimensional space, uses only a small number of the data space dimensionality. Each dimension of the manifold corresponds to a local direction of variation. This implies when a dimension (or multiple) of a manifold latent vector is changed arithmetically (i.e. linearly) the right amount, a transformed latent vector will result that also lies on the manifold and that may yield non-linear changes in the output image~\cite{StyleGAN}. This is where latent space interpolation comes into play. 

\textit{Manifold learning} is based on the assumption that data probability mass is highly concentrated along a low-dimensional manifold (or a collection thereof) where all the valid inputs are projected to. This implies that most inputs from the data space yield latent space coordinates which lie outside a manifold and are irrelevant, respectively. Interesting latent space interpolations and variations in the output, respectively, will occur only if we move along a direction that lies on the manifold or move from one manifold to another~\cite{DeepLearningBook}.

The \textit{manifold hypothesis} states that the probability distribution over images, text strings, and audio that occur in real life is highly concentrated as well~\cite{ReprLearning}. It is straightforward that an image created with pixel values uniformly distributed will unlikely result in a natural image. Also, it is likely that images resembling each other strongly in image space such as pairs of images with slightly different rotated objects will also be reachable in latent space by traversing the corresponding manifold. The concept of latent space interpolation is about traversing a manifold.

\textit{Latent space interpolation} is not a well defined operation. It relies on the vague notion of a "semantically meaningful combination" of latent vectors and also depends on the dataset. Essentially, a latent space interpolation is commonly implemented as a convex combination~\footnote{a linear combination of points where all coefficients are non-negative and sum to 1} of 2 latent vectors $z_1, z_2$ as follows: $\hat{x}_{\alpha} = g_{\theta}(\alpha z_1+(1-\alpha)z_2)$ where $\alpha \in [0,1]$, $g_{\theta}$ is a decoder and $\hat{x}_{\alpha}$ the decoded output~\cite{AE_Interp}. An ideal interpolation scenario with synthetic data is depicted in Figure~\ref{fig:interpolation_ideal}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.4\textwidth]{images/interpolation.png}
\caption{An ideal interpolation scenario \cite{InkscapeInterpolation}. As the latent vector of the original image on the left is interpolated and traversed on the manifold, respectively, the output image morphs into the original image on the right and vice versa.}
\label{fig:interpolation_ideal}
\end{figure}

Sample results of interpolations between the natural image features are shown in Figure~\ref{fig:interpolation_real}.
\begin{figure}[ht]
\centering
\includegraphics[scale=0.5]{images/latent_space_interpolations_paper_42.png}
\caption{Interpolations with features of real images \cite{InterpolExamples}. Top row shows input images.}
\label{fig:interpolation_real}
\end{figure}


%=========================================================================
%=========================================================================
% UNSUPERVISED DISENTANGLING FACTORS OF VARIATION
\subsection{Disentangling factors of variation}\label{subsec:unsupDFoV}
Apart from being distributed and invariant, good representations should also capture factors of variation in the input space and disentangle (i.e. separate) them into different independent features as part of the representation~\cite{ReprLearning}. A \textit{factor of variation} (or \textit{explanatory factor}) refers to a distinct and hidden source of influence with respect to the input data. A factor of variation explains part of the data. For instance, when analyzing an image of a car, the factors of variation include the position of the car, its color, and the angle and brightness of the sun~\cite{DeepLearningBook}.
% TODO MENTION FROM [12] "However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables."

One of the difficulties of a representation learning algorithm to disentangle factors of variation is when many of the factors influence a lot of the data observed. For instance, many natural images are composed of multiple objects with several light sources and shadows overlapping in complex ways. Here the question arises how the objects can be disentangled from their shadows. An important measure to take, as~\cite{ReprLearning} suggests, is to use as much data as possible to support learning representations that separate the various explanatory factors. In addition to using a large quantity of examples, deep learning is one of the foremost approaches to process complex natural data due to its ability to learn high-level abstract features from raw data by building complex concepts out of simpler concepts. 

Further, it is important to distinguish between learning invariant representations and learning representations with disentangled explanatory factors. Invariant representations have reduced sensitivity to variation in the data that are irrelevant to the task at hand. But these representations may be intended for multiple tasks which makes it difficult to learn them beforehand. Thus~\cite{ReprLearning} concludes that the most robust approach to representation learning is to \textit{disentangle as many factors as possible, discarding as little information about the data as is practical}.

A \textit{disentangled representation} can be described as one which separates the factors of variation and explicitly represents the important attributes of the data \cite{Framework4QuantEvalDisRepr}. In a disentangled representation, each section capturing a factor of variation is invariant to changes in sections representing other explanatory factors.

Some applications for disentangled representations are \cite{ChallengeAssLofDisRepr}:
\begin{itemize}
  \item directly build a predictive model instead of using the high dimensional input data
  \item use for learning of downstream tasks, transfer learning and few shot learning
  \item use for tasks involving latent space interpolations (as in our work)
  \item use when good interpretability is required
\end{itemize}
Ultimately, the goal is to learn representations that are invariant to irrelevant changes in the data~\cite{FwkQuantEvalDisRep}.

\par \textbf{Related work.} Many works exist that have disentangled factors of variation in raw data. We present a number of significant previous contributions towards unsupervised disentangling next and relate them to our work. Note that many prior works on disentangling factors of variation are fully supervised. They use labels for all factors of variation to be disentangled. In contrast, our method is fully unsupervised with unlabeled data only. Furthermore, many previous methods work on synthetic datasets or datasets with aligned data, that is one single centered object per image. Our method, however, faces the challenge of disentangling factors of variation in natural, unaligned data.

% [12] Previous approaches to disentangling factors of variation in data include content vs. style, form vs. motion or facial expression vs. identity.

%-- [12] Discovering Hidden FoV in DN -- start -------------------
\par Cheung \textit{et al.}~\cite{DiscHiddenFoViDN} augments autoencoders (AE) with regularization terms during training. They use an unsupervised cross-covariance penalty (XCov) as a method to disentangle class-relevant signals (observed variables) from other factors in the latent variables along with a standard supervised cross-entropy loss. In case of MNIST they consider the class label as a high-level representation of its corresponding input. Their model learns a class invariant smooth continuous latent representation $z$ that encodes digit style (slant) whereas the observed variable $\hat{y}$ represents the digit itself. On the TFD dataset, the observed variable $\hat{y}$ encodes the facial expression while the autoencoder is able to retain the identity of the faces through latent variable $z$. Here the XCov penalty prevents expression label variation from ‘leaking’ into the latent representation.
%-- [12] end -------------------------------

%-- [75] BETA-VAE -- start -------------------
\par Higgins \textit{et al.}~\cite{betaVAE} introduce the \textbf{$\beta$-VAE} which is based on the variational autoencoder (VAE). $\beta$ is an extra hyperparameter added to the standard VAE objective to encourage more disentangling in the latent representation. $\beta$-VAE is a method for unsupervised learning of disentangled representations. It is able to disentangle more latent factors than the basic VAE and in a similar capacity as the unsupervised INFOGAN. $\beta$-VAE achieves considerable disentanglement of multiple latent factors in various datasets including celebA, chairs and faces.
%-- [75] end -------------------------------

%-- [18] DIP-VAE -- start -------------------
Similar to $\beta$-VAE, Kumar\textit{et al.}~\cite{DIPVAE} propose \textbf{DIP-VAE}, another extension of the variational autoencoder for unsupervised learning of disentangled representations. Based on variational inference, they impose a regularizer on the VAE objective to encourage disentanglement. DIP-VAE outperforms $\beta$-VAE in disentangling latent factors as shown by quantitative metrics and visual latent traversals on the 2D shapes, celebA and chairs datasets.
%-- [18] end -------------------------------

%-- [3] UNIT -- start -------------------
Liu \textit{et al.}~\cite{UNIT} propose the unsupervised image-to-image translation framework \textbf{UNIT} based on VAEs and GANs (namely Coupled GANs). They try to map an image from one domain to a corresponding image in another domain (e.g. from day to night). UNIT assumes the existence of a shared latent space where corresponding images from different domains have a same latent representation. Based on that they build the UNIT framework to generate domain translated images. They show convincing results on tasks such as sunny to rainy, day to night, summery to snowy on a street images dataset and others. Unlike our method, the primary aim of this work is not disentangling latent representations but shares a similar goal in being a generative model which, given 2 source images, produces a new image that is a mixture of the inputs.
%-- [3] end -------------------------------

%-- [22] start -------------------------------
Mathieu \textit{et al.}~\cite{1611.03383} use a conditional generative model and labeled observations (images, audio) to extract latent representations that consist of specified and unspecified factors of variation. Similiar to UNIT, they use VAEs and GANs but with the aim of disentangling latent factors, like in our work. They show convincing interpolation results on MNIST, Sprites and NORB dataset.
%-- [22] end -------------------------------

%-- [37] ”Learning to Segment via Cut-and-Paste” start ------------------------------- 
Remez \textit{et al.}~\cite{CutAndPaste} propose an instance segmentation architecture based on a Mask R-CNN and GAN discriminator that can \textbf{learn via cut-and-paste of image objects}. They use adversarial training to learn to generate segmentation masks for image objects. The realism of the resulting image with the pasted object is judged by a discriminator. Their approach is weakly supervised in that they use bounding boxes only for training (no supervision on the masks). The results on COCO and Cityscapes datasets show some accurate segmentation masks. This work presents a somewhat similar approach to ours in that both try to move objects around independently of the background. However, the 2 methods are clearly distinct approaches in that one works with segmentation and the other with mixing latent representations.
%-- [37] end -------------------------------

%-- [40] start
\par Eastwood and Williams~\cite{FwkQuantEvalDisRep} propose a \textbf{framework for the quantitative evaluation of disentangled representations} when the ground-truth factor of variation is available. The framework is limited to synthetic datasets. The authors state that reliable disentanglement is far from solved even in the restricted setting of the framework. Further, they state that there is at that time no quantitative benchmark for disentangled representation learning available. A more recent work \cite{SpatialBDecoder} uses the Mutual Information Gap (MIG) metric to quantitatively evaluate disentanglement in representations. MIG requires the underlying factors of variation to be known and is therefore not applicable to evaluate an unsupervised method.
%-- [40] end

%-- [54] start -------------------------------LZ
\par Watters \textit{et al.}~\cite{SpatialBDecoder} present an architecture called \textbf{Spatial Broadcast Decoder} where they leverage a variational autoencoder. It aims at improving disentangling representations, reconstruction accuracy and generalization. Their key idea is to use a different architecture for the decoder other than the common deconvolutional network. In fact, the decoder is designed of convolutional layers only. First, they broadcast (replicate) the latent space vector across the spatial dimensions and concatenate 2 X- and Y-"coordinate" channels to the third dimension. Then a fully convolutional network with 1x1 stride is applied to reconstruct the input. With this architectural novelty they address the problem for a deconvolutional network to render an object at a particular position and argue that with the spatial broadcast decoder this becomes a simple task. Intuitively, the concatenated coordinate system should support the decoder place an object at the correct image location. Likewise, the coordinate system encourages the encoder to encode and disentangle the location information of an object into the latent vector $z$ such that the decoder can use this information together with the fixed coordinate system to determine the exact location in the output image. Put simply, if the encoder adds the location information as a $(x,y)$ coordinate to the latent $z$, then the decoder can easily compute the location by matching that coordinate in the coordinate system. Experiments on synthetic datasets show encouraging results, however they do not demonstrate their method on more complex, natural data. The conclusion states that this decoder improves latent representations, most significantly for datasets with objects varying in position. The latter is an interesting feature as our method has to cope with non-aligned objects and the notion of object position is important, respectively. Say if the encoder in our system would explicitly encode the location information of an object into the latent representation, that could prove very beneficial for the task of mixing objects and scenes, respectively.
%-- [54] end -------------------------------

%-- [55] start -------------------------------LZ
\par Greff \textit{et al.} \cite{BindingRC} describe the \textbf{binding problem}, a problem in the field of representation learning. It refers to ambiguities and interferences occuring in distributed representations of multiple objects from the same input. It can arise when multiple objects stemming from the same input are to be represented and disentangled at the same time. A proper disentanglement of objects might not be achieved. Where images contain only one object at a time the binding problem is avoided. They argue that the use of convolutions also mitigates the problem. Further, they propose an unsupervised method that explicitly models inputs as a composition of multiple objects. It dynamically binds features of different objects together and recovers these using the notion of mutual (pixel) predictability. Based on the latter, their framework uses clustering and a denoising autoencoder (DAE) to iteratively reconstruct the input and learn object representations, respectively. Even though they show promising results on binary images from artificial datasets (e.g. dSprites), experiments on natural image data are left as future work. As Greff noted in a presentation, he would expect that the model would cluster by color but not by objects when exposed to a natural dataset. 
%-- [55] end -------------------------------

%-- [69] start -------------------------------LZ
% COULD: Summarize IODINE: IODINE can segment complex scenes and learn disentangled object features without supervision on datasets like CLEVR, Objects Room, and Tetris.
%-- [69] end -------------------------------LZ

% TODO: perhaps summerize [67] MONet: Unsupervised Scene Decomposition and Representation as it is similar aim
% TODO summarize a few significant previous unsupervised methods here... (skim through paper list )
% TODO: mention "disentanglement metrics" % cf. https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html
\begin{table} [H]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Method} & \textbf{Paper} & \textbf{Architecture} & \textbf{LT} & \textbf{Datasets}\\
\hline
Reconstruction Clustering & \cite{BindingRC} & DAE, Clustering & us & Shapes, Bars, Corners, MNIST \\
\hline
$\beta$-VAE & \cite{betaVAE} & VAE & us & celebA, 3D chairs, 3D faces \\
\hline
DIP-VAE & \cite{DIPVAE} & VAE &  us & celebA, 3D chairs, 2D shapes \\
\hline
UNIT & \cite{UNIT} & VAE, CoGAN & us & streets (own), celebA, SYNTHIA \\
\hline
Cheung \textit{et al.} & \cite{DiscHiddenFoViDN} & AE & ss & MNIST, TFD, Multi-PIE \\
\hline
DNA-GAN & \cite{DnaGan} & GAN & sv & celebA, Multi-PIE \\
\hline
Mathieu \textit{et al.} & \cite{1611.03383} & VAE, GAN & sv, us & Sprites, MNIST, NORB \\
\hline
\end{tabular}
\caption{Overview and characteristics of prior works on disentangled representation learning. (LT) learning type, (us) unsupervised, (ss) semi-supervised, (sv) supervised.} \label{tab:priorWorks}
\end{table}

% TODO 27.02: paper [37] ”Learning to Segment via Cut-and-Paste”: auch erwähnen da ähnliches Ziel: we're trying to "move objects around independently of their background"

% Consider "Towards a Definition of Disentangled Representations" [56]

%=========================================================================
\subsubsection{Disentangling Factors of Variation by Mixing Them}
Our method is significantly based on the work of Hu \textit{et al.}~\cite{DisentFacOfVarByMixTh} and pushes its idea further. First we summarize the results of \cite{ChallengInDisentIFoF} and \cite{UnderstDegenAndAmbInAT} as a theoretical background for \cite{DisentFacOfVarByMixTh} and then present the method itself. 

%-- [28] start -------------------------------
Szabo~\textit{et al.}~\cite{ChallengInDisentIFoF} identify 2 main challenges in disentangling factors of variation: the \textit{shortcut problem} and the \textit{reference ambiguity}. In the shortcut problem the model learns degenerate encodings in which all information is encoded only in one part of the feature, i.e. either in the component $c$ for common attributes shared by both images or the component $v$ for varying attributes. For an image pair there are 2 $v$ and one $c$ in latent space. The encoder maps a complete description of its input into vector $N_v$ and the decoder completely ignores vector $N_c$.
The second challenge, reference ambiguity, occurs when the attribute representation for an image is not guaranteed to follow the same interpretation on another image. In other words, the reference in which a factor is interpreted may depend on other factors which makes the attribute transfer~\footnote{i.e. the replacement of a feature chunk, which exposes an image attribute, by the same chunk from another image} ambiguous. For example, the viewpoint angle of a vessel gets interpreted in 2 different ways depending on the boat type.
%-- [28] end -------------------------------
%-- [16] start -------------------------------
\par The 2 challenges identified in~\cite{ChallengInDisentIFoF} are then addressed by Szabo~\textit{et al.}~ \cite{UnderstDegenAndAmbInAT}. They introduce an adversarial weakly supervised training method that uses image triplets and fully tackles the shortcut problem by means of a composite loss function consisting of an autoencoder loss and an adversarial (i.e. GAN) loss. Used in conjunction this function provably averts the shortcut problem. Concerning our work, this implies that the shortcut problem cannot occur in theory as our method is based also on a composite loss function. Furthermore, they analyze the reference ambiguity and prove that it is unavoidable when disentangling with only weak labels. The problem occurs when a decoder reproduces the data without satisfying the disentangling properties for the varying attribute $v$. Practically, this means not all the factors of variation can provably be disentangled from weakly labeled data (i.e. when only $c$ is known for image pairs).    
%-- [16] end -------------------------------

\par \textbf{Disentangling Factors of Variation by Mixing Them.} Hu~\textit{et al.}~\cite{DisentFacOfVarByMixTh} present a novel unsupervised method to disentangle factors of variation without any data knowledge. The factors of variation correspond to image attributes such as the pose or color of objects. The disentangled representation consists of a fixed number of feature chunks where each chunk represents a factor of variation. The disentanglement is attained by a neural architecture including mixing autoencoders, a classifier and a GAN. A sequence of 2 mixing autoencoders is used to enforce disentanglement and invariance by mixing the feature chunks of input images $x_1$ and $x_2$ to generate a new image $x_3$, re-encode $x_3$ and using the obtained representation and its chunks, respectively, to reconstruct the representations of $x_1$ and $x_2$ to finally re-decode them for a reconstruction loss. This two-time cycle of encoding and decoding helps establish the disentanglement and invariance by "channeling" each image attribute into a distinct feature chunk. Invariance means each image attribute is represented by one feature chunk which is invariant to changes in other chunks and for the decoding part affects only one image attribute. Invariance allows the system after disentangled encoding to create mixes of factors of variation and decode them into new images semantically consistent with the dataset. The GAN establishes the adversarial training to ensure that the decoded images from the mixed features follow approximately the data generating distribution. To avoid the shortcut problem a classification constraint ensures that each feature chunk corresponds to a discernible image attribute such that degenerate encodings cannot occur.

% TODO evtl. diagram machen zum aufzeigen der beziehungen zwischen: image attribute - factor of variation - feature vector - feature chunk

In the experiments they show qualitative results on 2 datasets including Sprites and and CelebA with attribute transfer of factors of variation such as pose, vest, hair color, leg (Sprites) and glasses, hair style, brightness, background (CelebA).

Next we outline the differences between \cite{DisentFacOfVarByMixTh} and our method in Table~\ref{tab:diffHuVsLorbms}.
\begin{table} [H]
\centering
\begin{tabular}{l|c|c}
\Xhline{0.8pt}
\textbf{Category} & \textbf{Theirs} & \textbf{Ours}\\
\hline
Feature space & attribute transfer & object transfer\\
\hline
Feature mixing & random mask & scene mixing algorithm (sec. \ref{subsec:img_mixing_algo}) \\
\hline
Input  & 2 images & 5 images (sec. \ref{fig:model_arch}) \\
\hline
Data & single object, aligned & multiple object, unaligned\\
\hline
Dataset & Sprites, CelebA & MS COCO \\
\hline

Classifier & per feature match image & per image match quadrant (sec. \ref{fig:model_cls}) \\
\Xhline{0.8pt}
\end{tabular}
\caption{Differences between \cite{DisentFacOfVarByMixTh} and our method.} \label{tab:diffHuVsLorbms}
\end{table}
Notice that their system is designed for attribute transfer which disentangles and swaps attributes in aligned single object images whereas our system aims at object transfer that disentangles and swaps objects in unaligned natural image data. While the architecture as a whole is roughly the same in both works, the individual components differ substantially in their architecture as the challenges of a natural unaligned dataset call for exploration of various approaches.
 

%=========================================================================
% PROBLEM STATEMENT AND METHOD:
%
% LEARNING OBJECT REPRESENTATIONS BY MIXING SCENES
%=========================================================================
% \chapter{Learning object representations by mixing scenes}
\newpage
\section{Learning Object Representations by Mixing Scenes}\label{sec:lorbms_method}
\subsection{Concept}\label{subsec:concept}
Our main objective is the following: We want to build an unsupervised system that learns object representations by mixing objects from different scenes. To this end, the system identifies and extracts objects in images and represents them in a lower dimensional latent space in a disentangled and invariant state. By editing the latent space representation, we want to move objects around in a non-linear way. That is the basic motivation for our system. Eventually the mixed representation is transformed back to image space and thereby a new scene is created, a mixed scene with a combination of objects from the input images. For example, the system could extract an aircraft object from an airport, extract a lawn mower object in a public park, swap the 2 objects in latent space and create 2 new mixed scenes with a lawn mower in an airport and an aircraft in a public park. 
The system should therefore enable us to move objects around or swap them independently of their background and merge them into a new scene. We want to transfer objects between images simply by swapping the respective feature chunks. Ideally, the system should be able to cut-and-paste objects between different scenes where an object and its attributes (e.g. size) and its surrounding background are adapted in a non-linear way in the target image. The function of the system should occur as a spatial object-to-object translation. The model should learn not only to identify objects in scenes but also to extract and merge them into different scenes with the result of generating a new scene. 

While the synthesis of realistic mixed scenes is one goal, the ultimate ambition of our method is the object representation. An object representation is a high dimensional vector in the latent space and represents raw image data in a numeric form. The purpose in the end is to have a representation where a feature chunk (i.e. an object) can be taken from one representation and plugged into another and the rendering would adapt the content accordingly. We aim at an architecture where we can take images and mix the content in a selective way such that it gives us control over what we mix and eventually allows us to produce output images that correspond well with the mixing. Therefore, we want to move objects around such that if we move an object in the latent space we want to see it moved in the output picture. 

The model is built on the assumption that each of the quadrants of an image (i.e. spatial slots) contains at most one object as exemplified in Figure~\ref{fig:livingroom_4quad}. We call it the \textit{4 object assumption}. TODO: mention sth like "this is the inductive bias we impose"? (cf. https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html e.g. "the theoretical result that the unsupervised learning of disentangled representations without inductive biases is impossible") This assumption restricts the model to coarse movements. For instance, it would swap 2 persons between 2 images rather than swap part of the clothes 2 persons wear. The notion of coarseness depends on the size of the objects in an image. Note that the aim is not to segment objects but to move objects around wile making sure the result is realistic. Perhaps a parked car is copied onto a street but the car may not be in the right "placement". As the car is copied onto that street we expect the model to automatically put it in the right placement (e.g. pose, size) subject to the street. Or the system copies a person into a scene where the placement is further away, so in the target image the scale of the person has to be smaller. When a quadrant contains more than one object or just part of an object, the resulting mixed scene might suffer realism and not be as realistic as expected depending on what is mixed together. Whether the objects in an image belong to the same category or different categories, is irrelevant to the model.

\begin{wrapfigure}{r}{0.2\textwidth}
\includegraphics[width=0.9\linewidth]{images/living_room_sliced_4pcs.png} 
\caption{An image split into quadrants.} % An image as quadrants with one or zero objects each.
\label{fig:livingroom_4quad}
\end{wrapfigure}

The system will face multiple key challenges in learning good object representations based on the problem definition and assumptions made:
\begin{enumerate}
  \item many prior works have shown that disentanglement works with aligned data. This means spatial location matters for disentangling factors of variation. As we use a natural dataset, the data is inherently non-aligned and spatial location of objects highly stochastic.
  \item  in most datasets used by other methods the objects have little variation in spatial size. Our system however faces natural data where objects take on any size in any spatial location.
  \item an extra source of difficulty in extracting and merging an object into a new scene is its impact on the proximity pixels of its position such as shading or partly occluded other objects.
\end{enumerate}
% TODO 04.03 somehow somewhere talk about GENERALIZATION
Essentially, our unsupervised model tries to discover structure in the dataset in order to learn its manifold. We then try to interpolate through it when mixing objects. We expect this to be a very difficult task as the model faces a natural dataset with a complex, high-dimensional data distribution and an accordingly difficult to learn manifold. 

The input dataset is defined to be a natural dataset with a wide variety of objects and scenes. It should be as natural, unrefined and diverse as possible as we want the model to understand the real world, the definite target for our method. To that end, we should mix objects only if they are sufficiently similar to support the learning process. If we swap a person on a chair in a kitchen with a closeup view of a vegetable market stand, we cannot expect a realistic and meaningful outcome. The requirement thus is to mix meaningful or compatible pairs of objects only, then we should be able to see realistic output. We might expect at least to swap 2 different people, cars, buildings or animals with perhaps distinct poses and out should come a rendered image with that change. The method to determine compatible pairs of objects is elaborated in detail in Section~\ref{subsec:img_clustering}. 

Technically, the model learns the latent representation as a concatenation of 4 feature chunks where each chunk exposes one object from the input and represents a disentangled factor of variation, respectively. While \cite{DisentFacOfVarByMixTh} considers an \textit{image attribute} as a factor of variation in single-object input data, our model views an \textit{image object} as a factor of variation in multi-object input data~\footnote{the disentangling of object attributes within disentangled objects provides an interesting avenue for future work.}. The latent representation can also be considered as a \textit{compositional representation}~\cite{SpatialBDecoder} consisting of components that can be recombined. % ", and such recombination underlies generalization"
Our model architecture is designed to encourage such representations. See Section~\ref{subsec:model_arch} for more details. 

In theory, given images with compatible pairs of objects and under the assumption that our system produces realistic mixed scene outputs, the model should learn robust object representations where the description of an object and its attributes such as pose and scale are clearly disentangled (i.e. separated). As mentioned earlier, however, the object attributes are not given an explicit structural state by the architecture of our system unlike the object itself which is exposed in the form of a feature chunk. Therefore, a feature chunk will contain both the object description and its attributes.

To increase the realism of the generated scenes the model uses a GAN. Here we make the assumption that a GAN is capable of learning the input data distribution of the dataset used. Otherwise our method will inherently not be able to approximate the data distribution with the mixed scenes. The GAN generator is composed of an autoencoder where the encoder computes the low-dimensional latent representation given an input image and the decoder generates the output image given the representation. The proposed method is a conditional generative model where both the generator and the discriminator are conditioned on the input dataset. We describe the model and its architecture in detail next.


% TODO: make simple system overview diagram using images as in [25] Figure 2
% TODO: use images in D:\learning-object-representations-by-mixing-scenes\src\results\dump_mixes\20190221_133431_dump_mixes5cols or gen. new
% what is the theoretical groundwork for the proposed method? 
% In high dimensional spaces, embeddings?
% What architectural priors do I have in LORBMS? "Network architectures certainly reflect strong prior beliefs about the nature of the problem they will be applied to."
%%%%%%
% TODO: Put good effort in realizing all explicit and implicit assumptions that you make, and clearly state them.
%%%%%%
% TODO: how do we avoid learning trivial features?

%=========================================================================
% Model architecture
\subsection{Model architecture}\label{subsec:model_arch}
In this section, we formally present our method. A fundamental assumption is that a real world image $I_{ref}$ contains 4 objects, ideally one in each quadrant (see Figure~\ref{fig:livingroom_4quad}). The method considers $I_{ref}$ as made up of 4 quadrants $I^{q1}_{ref}, I^{q2}_{ref}, I^{q3}_{ref}, I^{q4}_{ref}$ where each quadrant contains either one object or none. In addition to $I_{ref}$, the method receives 4 other "quadrant replacement" images $I_{q1},I_{q2},I_{q3},I_{q4}$ where each represents a candidate for the replacement of the respective quadrant in $I_{ref}$. See section \ref{subsec:img_clustering} for more details on "quadrant replacement" images. A replacement of one or more quadrants in an image is considered a mixing of 2 scenes with the result of a new scene. Note that the replacement of quadrants and mixing of scenes, respectively, exclusively takes place in the latent space of the model as is described later. These per-quadrant replacement images have been preselected in the algorithm outlined in section \ref{subsec:img_clustering} and underlie the same assumptions as $I_{ref}$. In total the model receives 5 input images simultaneously.

The model architecture is depicted in Figure~\ref{fig:model_arch}. Its main components are:

\begin{itemize}
   \item a GAN comprised of a generator $G$ and a discriminator $D$ for adversarial training
   \item an autoencoder including encoder $Enc$ and decoder $Dec$ for image generation
   \item a classifier $Cls$ to encourage disentanglement within the feature vector of an image
   \item a latent space scene mixing algorithm (see section \ref{subsec:img_mixing_algo})
\end{itemize}
 
The GAN and the adversarial training, respectively, is used to generate realistic images representing the mixed scenes. Note that the generator $G$ is synonymous for the entire autoencoder. The autoencoder functions as the generator in the GAN framework. That means in training when $G$ is updated during backpropagation, what actually gets updated is the encoder and the decoder. The GAN generator therefore is used twofold: Firstly, using $Enc$ it encodes the input image into a disentangled feature vector that is then mixed with other feature vectors according to the scene mixing algorithm in \ref{subsec:img_mixing_algo}. Secondly, given the mixed feature vector, the generator, using $Dec$, generates a new image $I_{mix}$. Ideally, $I_{mix}$ represents a valid image according to the input data distribution. It is then evaluated by the discriminator as real or fake. Additionally, the classifier receives $I_{mix}$ to decide for each quadrant whether its content originates from a replacement image or not and thereby provide a feedback to the autoencoder to what degree it's quadrant disentanglement encoding and decoding is coherent in the output. Only if the encoder well disentangles the quadrants in the feature space can the decoder generate an image that represents a semantically correct scene where the respective quadrants can be rediscovered (by the classifier) at the same location as in the original position.

The generator is composed of the encoder and the decoder. The encoder $Enc$ maps an image $A$ into a latent space representation
\begin{equation} \label{eq:2}
    Enc(A) = [a_1, a_2, a_3, a_4] = f_A
\end{equation}
where $f_A$ represents the feature vector of image $A$ which is a concatenation of a fixed number of feature chunks $a_i$. See Figure~\ref{fig:encoder} for a schematic representation. Each chunk represents the respective image quadrant $q_i$ in the feature space. $a_i$ is meant to exclusively contain a lower dimensional representation of the object in $q_i$ such that this chunk is invariant to modifications in the neighboring chunks. Hence $a_i$ represents a disentangled factor of variation of the input image. At the same time it can be viewed as the DNA part of one quadrant of the output image.

\begin{figure}[ht]
\centering
\includegraphics[width=0.6\textwidth]{images/model_encoder.png}
\caption{The encoder maps image $A$ to feature vector $f_A$. Chunk $a_i$ represents the quadrant $q_i$ in latent space.}
\label{fig:encoder}
\end{figure}

By swapping some of the feature vector chunks we obtain a new, mixed representation $f_{I_{mix}} = [a_1, b_2, c_3, d_4]$ where $a_1,b_2,c_3,d_4$ represent quadrants from possibly different input images. Using decoder $Dec$, said representation is projected back into image space as
\begin{equation} \label{eq:3}
    Dec(f_{I_{mix}}) = I_{mix}
\end{equation}
where $I_{mix}$ represents a new, realistic scene containing a mixture of objects from one up to 4 images. See section \ref{subsec:img_mixing_algo} for more details on how the mixing takes place.

The decoder is not only used to generate new images $I_{mix}$. It is also used to reconstruct images $\hat{A} = Dec(Enc(A))$ to facilitate a reconstruction constraint on the autoencoder to the help guide the learning process of the generative model.

The reconstruction constraint on the autoencoder is twofold. The first constraint is based on $\hat{A}$ just introduced. For the second, $I_{mix}$ is encoded again as $Enc(I_{mix}) = \hat{f}_{I_{mix}}$ to obtain a reconstruction of its original feature vector, $\hat{f}_{I_{mix}}$. The original feature vector $f_A$ is then reconstructed as $\hat{f}_A$ using as many chunks from $\hat{f}_{I_{mix}}$ as possible. In fact, as many as were used from $f_A$ in the mixing algorithm to create $f_{I_{mix}}$. The remaining chunks are taken directly from $f_A$. Finally, using the decoder, $\hat{f}_A$ is decoded into a second reconstructed image $A' = Dec(\hat{f}_A)$ on which the second reconstruction constraint is imposed. Both constraints contribute an equally weighted training signal to the autoencoder.

To establish the adversarial training, $I_{mix}$ and the original image $A$ are passed to the discriminator $D$. $D$ decides on the realism of the fake image $I_{mix}$ thereby issuing a training signal for improvement to the autoencoder (generator loss). Additionally, $D$ decides on the realism of the real image $A$ and the fakeness of the fake image $I_{mix}$ together thereby issuing a training signal to itself (discriminator loss).

\begin{figure}[ht]
\centering
\includegraphics[width=1\textwidth]{images/lorbms_model.png}
\caption{Schematic of our model architecture. Same color denotes same weights.}
\label{fig:model_arch}
\end{figure}

Figure~\ref{fig:model_arch} shows the sequence of 2 mixing autoencoders ($Enc-Dec-Enc-Dec$) that helps to encourage disentanglement of image objects in latent space and data space alike. In latent space when the model encodes each image quadrant into a feature chunk, each chunk should be invariant to modifications in the other chunks. And in data space when the model decodes the feature vector into an image, each quadrant should represent the object its corresponding feature chunk carried. Intuitively, if we decode and re-encode the mixed feature vector $f_{I_{mix}}$, the resulting vector $\hat{f}_{I_{mix}}$ should preserve the original chunks copied into it and keep the same order of chunks as in $f_{I_{mix}}$.

To further foster the disentanglement of factors of variation, a classification challenge is posed to a classifier $Cls$ that receives a pair of images including the mix image $I_{mix}$ along with one of the original "quadrant replacement" images $I_{qi}$ with $i \in \{1,2,3,4\}$. For each $I_{qi}$, the classifier has to decide if that image occurs in $I_{mix}$ and if so in which quadrant. Therefore, if the autoencoder does a thorough job in encoding (and disentangling, respectively) the objects from the various input images and equally in decoding the mixture of the chunks to a new image, the classifier should be able to recognize the original objects in the generated, mixed scene. The classifier is depicted in more detail in Figure~\ref{fig:model_cls}.
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{images/lorbms_model_cls.png}
\caption{$Cls$ decides if an image $I_{q_i}$ occurs in $I_{mix}$ and in which quadrant. $x_i \in \{0,1\}$, $y_i \in [0,1]$.}
\label{fig:model_cls}
\end{figure}
Note that the challenge for the classifier is twofold. It not only has to decide if an image occurs in $I_{mix}$ at all but also in which quadrant. The classifier thus consists of 4 binary classifiers. It outputs 4 variables $[v_1,v_2,v_3,v_4]$, one for each quadrant of $I_{mix}$, that decide if image $I_{q_i}$ occurs in quadrant $q_i$ ($i \in \{1,2,3,4\}$). The output along with the ground truth $[x_1,x_2,x_3,x_4]$, the actual occurrence of the "quadrant replacement" images in $I_{mix}$, is then used to define the classifier loss.

%=========================================================================
% Loss functions
\subsection{Loss functions}
\subsubsection{Generator loss}
The encoder and decoder together as the generator $G$ receive a composite loss that is a weighted sum of 3 losses: the reconstruction loss, the adversarial loss and the classifier loss. 

\textbf{Reconstruction loss.} The reconstruction loss imposes a constraint on the autoencoder that steers it to produce images that closely match the originals $\hat{I}_{ref} \approx I_{ref}$ and is given by
\begin{equation} \label{eq:4}
    \mathcal{L}_{rec} = \mathbb{E}_{I_{ref}\sim p_{data} (I_{ref})}\big[ \Vert I_{ref} - \hat{I}_{ref} \Vert_p + \Vert I_{ref} - I'_{ref} \Vert_p \big]
\end{equation}
where $p = 1$ or $p = 2$. However, the well known deficiency of $l_1$ or $l_2$ (i.e. mean squared error) as a loss function is blurry predictions that $D$ can discriminate easily. 

\textbf{Adversarial loss.} As in \cite{DisentFacOfVarByMixTh} and \cite{1511.05440}, to counteract this flaw we use an adversarial loss to guide $G$ towards sharp predictions as follows:
\begin{equation} \label{eq:5}
    \mathcal{L}^G_{adv} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[ \mathcal{L}_{bce}(D(G(I_{ref},I_{q1},I_{q2},I_{q3},I_{q4})), 1)\big]
\end{equation}
where $j \in \{ref,q1,q2,q3,q4\}$ and $\mathcal{L}_{bce}$ is the binary cross entropy loss:
\begin{equation} \label{eq:6}
    \mathcal{L}_{bce}(p, y) = -(ylog(p) + (1 - y)log(1-p))
\end{equation}
where $y$ is the ground truth and $p$ the prediction. Equation~(\ref{eq:5}) is equivalent to the non-saturating GAN loss in Equation~(\ref{eq:g_loss}). Note that the adversarial loss is computed using the discriminator $D$. The generator's objective is to fool $D$ into thinking that the fake images $G$ generated are real.

\textbf{Classifier loss.} Thirdly, to encourage disentanglement in the latent space representation of the images, we impose the classifier constraint defined as
\begin{equation} \label{eq:cls}
    \mathcal{L}_{Cls} = \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[\sum_{j} \lambda_j \sum_{i} \mathcal{L}_{bce}(\hat{y}^j_i, y^j_i)\big]
\end{equation}
where $j \in \{q1,q2,q3,q4\}$, $i=1,2,\cdots,4$ and the classifier prediction $\hat{y}^j = Cls(I_{mix}, I_j) = [\hat{y}^j_1, \hat{y}^j_2, \hat{y}^j_3, \hat{y}^j_4]$. $Cls$ thus consists of 4 binary classifiers, one for each quadrant of $I_{mix}$, the generated image. Each classifier decides if a quadrant in $I_{mix}$ was generated using the quadrant from the replacement image $I_j$ or from $I_{ref}$. The main challenge for $Cls$ is it does not know which image $I_j$ it is given besides $I_{mix}$ and therefore cannot simply remember what to predict. This loss can be minimized only if the disentanglement of the 4 quadrants as well as the decoding thereof works well.

\textbf{Combined final loss.} To combine the 3 losses we introduce coefficient parameters $\lambda_{rec}$, $\lambda_{adv}$ and $\lambda_{Cls}$. By means of these we can optimize the tradeoff between original image similarity (\ref{eq:4}), sharp predictions (\ref{eq:5}) and disentanglement of the 4 quadrants (\ref{eq:6}). Thus we get the final loss on $G$ as follows:
\begin{equation} \label{eq:g_loss_comp}
    \mathcal{L}_{G} = \lambda_{rec} \mathcal{L}_{rec} + \lambda_{adv} \mathcal{L}^G_{adv} + \lambda_{Cls} \mathcal{L}_{Cls}
\end{equation}

\subsubsection{Discriminator loss}
The discriminator $D$ is trained with the standard GAN discriminator loss defined as
\begin{equation} \label{eq:lorbms_D_real}
    \mathcal{L}^D_{real} = \mathbb{E}_{I_{ref}\sim p_{data} (I_{ref})}\big[ \mathcal{L}_{bce}(D(I_{ref}), 1)\big]
\end{equation}
\begin{equation} \label{eq:lorbms_D_fake}
    \mathcal{L}^D_{fake} =  \mathbb{E}_{I_{j}\sim p_{data} (I_{j})}\big[ \mathcal{L}_{bce}(D(G(I_{ref},I_{q1},I_{q2},I_{q3},I_{q4})), 0)\big]
\end{equation}
\begin{equation} \label{eq:lorbms_D}
    \mathcal{L}_{D} = \mathcal{L}^D_{real} + \mathcal{L}^D_{fake}
\end{equation}
The discriminator's objective is to classify real images as real and images generated by $G$ as fake, both as good as possible.

\subsubsection{Classifier loss}
The classifier is part of the generator and consequently the classifier loss in Equation~(\ref{eq:cls}) contributes to the generator loss. Moreover, the classifier loss is used to train the classifier separately with frozen generator and discriminator weights.


%=========================================================================
% Finding visually similar images
\subsection{Finding visually similar images} \label{subsec:img_clustering}
A "quadrant replacement" image is loosely defined as an image that has a quadrant which is very similar visually, as perceived by the human eye, to the same quadrant of another image. For instance, this could be a blue sky in the top right quadrant of 2 images, or 2 baseball players in the bottom left quadrant. For our model the 2 quadrants can appear very similar semantically but should not be the same pixel-wise. Intuitively, we want to replace a quadrant with a pixel-wise different but semantically similar looking quadrant to give the model a challenge in generating a new image with different content, yet at the same time this task should not be easy thus the quadrants must not be too close visually (i.e. pixel-wise).

In unsupervised learning there is no human intervention. There are no human annotated labels. Likewise we cannot rely on human labor to find and group similar images. The problem arises how we can provide the model with meaningful "quadrant replacement" images since annotations in the dataset are inexistent and random selection will work poorly. We therefore have to rely on another mechanism to help us detect visually similar images that satisfy the above requirements of a "quadrant replacement" image. To this end, we devise a novel visual similarity detection algorithm that leverages the clustering method \textit{DeepCluster} described in \ref{subsec:deepcluster}.

\textbf{Visual similarity detection algorithm} As noted in \ref{subsec:deepcluster}, we exploit the fact that images whose features are close in latent space will also look similar in image space. The closer 2 features are in latent space the more visually similar 2 images will be in data space. "Visually similar" means both images contain an object of the same type such as a person, plane, dog, etc., yet the objects themselves may look substantially different on the pixel level.

Given a dataset $X = \{x_1,x_2,\cdots,x_N\}$ of $N$ images. The algorithm first slices every image from $X$ into 4 quadrants and creates 4 extra datasets $X_{q1}, X_{q2}, X_{q3}, X_{q4}$. Subsequently each dataset $X_{qi}$ is processed separately. To produce the features for all images from $X_{qi}$, the algorithm uses a convnet pretrained with \textit{DeepCluster}. The features are then clustered using the \textit{k}-means algorithm to obtain groups of features. For each feature of a cluster the 10 nearest features are calculated using the \textit{k}-nn algorithm with the L2 distance metric as provided by Johnson \textit{et al.}~\cite{FaissKnn}. Finally, each image in $X$ is annotated with 4 lists containing the ids of the 10 nearest images, one list per quadrant. The algorithm is sketched in Algorithm~\ref{alg:vis_sim_det_algo}.

\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwIn{a dataset $X$}
\KwOut{dataset $X'$ with nearest neighbor annotations}
\Begin{
\ForEach{image $x \in X$} {
    slice $x$ into 4 quadrants $x_{q1}, x_{q2}, x_{q3}, x_{q4}$\;
    create datasets $x_{q1} \in X_{q1}, x_{q2} \in X_{q2}, x_{q3} \in X_{q3}, x_{q4} \in X_{q4}$
}
\ForEach{$X_{qi}$} {
    compute features for all images in $X_{qi}$\;
    cluster features with \textit{k}-means\;
    \ForEach{cluster $c$} {
        \ForEach{feature $f \in c$} {
            compute 10 nearest features $tnf$ with \textit{k}-nn and L2 distance\;
            create list $tni$ of 10 nearest neighbor images using $tnf$\;
            annotate image $x_{qi}$ belonging to $f$ with $tni$\;
        }
    }    
}
\ForEach{image $x$ in $X$} {
    \ForEach{$X_{qi}$} {
        annotate $x$ with 10 nearest neighbor list $tni$ from $x_{qi}$
    }
}
}
\caption{Visual similarity detection algorithm}\label{alg:vis_sim_det_algo}
\end{algorithm}

In Appendix~\ref{app:vsda_examples} we show examples of visually similar looking images as determined by the algorithm. Note that the \textit{visual similarity detection algorithm} is used only to preprocess and annotate the dataset, respectively, prior to the model training. It is not employed during training or test of the model. The list of the 10 nearest image ids per quadrant is later utilized during training by the \textit{latent space scene mixing algorithm} as described next.


\subsection{Latent space scene mixing algorithm} \label{subsec:img_mixing_algo}
As a result of the clustering algorithm in section \ref{subsec:img_clustering}, each $I_{ref}$ from the dataset has 4 ordered lists of 10 nearest neighbors (10NN-list), one for each quadrant $I^{qi}_{ref}$ with $i \in \{1,2,3,4\}$. The 10NN-list contains the 10 images whose corresponding quadrant is closest in latent space to $I^{qi}_{ref}$. The \textit{latent space scene mixing algorithm} described here takes place in the latent space of the autoencoder where the feature encodings of the input images are available. This process is summarized in Algorithm~\ref{alg:img_mixing_algo}. 

\begin{algorithm}[H]
\DontPrintSemicolon
\LinesNumbered
\KwIn{a reference image $I_{ref}$, its feature vector $f_{I_{ref}}$, a threshold $\tau$}
\KwOut{A mixed feature vector $f_{I_{mix}}$ from 2 to 4 images}
\Begin{
\ForEach{quadrant $I^{qi}_{ref}$ of $I_{ref}$} {
    $I_{qi}$ $\leftarrow$ choose a replacement candidate uniformly from 10NN-list of $I^{qi}_{ref}$\;
}
\ForEach{quadrant $I^{qi}_{ref}$ of $I_{ref}$} {
    \If{$I_{qi}$ has the greatest L2 distance among all replacement candidates}{
        select feature of quadrant $I^{qi}_{ref}$ and ignore $I_{qi}$
    }
    \If{$I_{qi}$ has the lowest L2 distance among all replacement candidates}{
        replace feature of quadrant $I^{qi}_{ref}$ with feature of candidate $I_{qi}$
    }
    \uIf{L2 distance of $I_{qi}$ is greater than $\tau$}{
        select feature of quadrant $I^{qi}_{ref}$ and ignore $I_{qi}$
    }
    \Else{
        replace feature of quadrant $I^{qi}_{ref}$ with feature of candidate $I_{qi}$
    }
} 
}
\caption{Latent space scene mixing algorithm}\label{alg:img_mixing_algo}
\end{algorithm}

The algorithm ensures the resulting mixed feature vector $f_{I_{mix}}$ has the following properties:
\begin{itemize}
   \item at least 1 quadrant remains from $I_{ref}$ (lines 6-7)
   \item at least 1 quadrant of $I_{ref}$ is replaced (lines 9-10)
   \item only replacements occur which are "sufficiently similar" to the original (lines 12, 15)
\end{itemize}
Therefore, the number of replaced quadrants and features, respectively, in $f_{I_{mix}}$ is always between 1 and 3. In other words, the mixed scene will always be made up of between 2 and 4 images. See Figure~\ref{fig:quad_repl_ex} for visual depictions of mixed feature vectors. Quadrants are replaced only at the same position in the image, i.e. a replacement involving $I^{qi}_{ref}$ and $I_{qj}$ where $i \neq j$ does not occur. Due to the limitations of the algorithm there are also mixed scenes that don't match well semantically unlike the examples shown.
\input{includes/quadrant_replacement_ex.tex}


%=========================================================================
% see paper "Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles" sec 3.4 for an example
% NOTE: AS OF 13.03 THE ARCHITECTURE REFERENCE IS EXP70
%=========================================================================
\subsection{Implementation}
As in \cite{DisentFacOfVarByMixTh} and \cite{InfoGAN}, we use a GAN architecture derived from DCGAN. This means both the generator and the discriminator use convolutional layers to process their input. The generator which is implemented as an autoencoder uses a handcrafted architecture as depicted in Table~\ref{tab:arch_encoder} and Table~\ref{tab:arch_decoder}. For the reconstruction loss we choose the $l_1$-norm over $l_2$ encouraged by \cite{CondGAN_PatchGAN}. We do not use dropout in any of the networks. The weights for the combined final loss are $ \lambda_{rec} = 0.904$, $\lambda_{adv} = 0.052$ and $ \lambda_{Cls} = 0.044$. The system processes batches of images of size $64 \times 64$ and is trained on a dataset of 338,865 color images. The batch size is 12. For each iteration and batch, respectively, the generator is optimized twice in a row and the discriminator and the classifier once. The model is trained using the Adam optimizer. The learning rate for the generator is $0.0002$ and for the discriminator $0.0004$. TODO-START DESCRIBE WITH GAN LOSS VERSION WE'RE USING TODO-END The training is performed on 2 Nvidia GeForce GTX TITAN X separately with 12GB memory each. The training time for 50 epochs is approximately 13 days. One reason for this extensive duration is that for each image in the batch the network has to load an additional 4 images (i.e. the quadrant replacement candidates) directly from the file system causing a lot of I/O processing. As for another reason, we conjecture the deep autoencoder with 95 layers consumes another non-negligible amount of time to perform backpropagation even though the number of parameters is only a fraction of the total network. In the 44 layers of the decoder, spectral normalization is in place which in itself is a computationally light operation according to \cite{SNGAN}, however spread across 44 layers it could add up to another non-negligible cost. On top of this all, the generator (i.e. the autoencoder) is updated twice in each training iteration.
\input{includes/architecture_encoder.tex}
The encoder as shown in Table~\ref{tab:arch_encoder} uses dense blocks~\cite{DenseNet} and is inspired by the \textit{FC-DenseNet103} architecture from Jégou~\textit{et al.}~\cite{Tiramisu}. Using only dense blocks yields a deep architecture that consists of convolutional and pooling layers exclusively without any fully connected layers. This saves parameters and memory. More importantly, we argue that this is beneficial for the disentanglement of objects as the spatial information is preserved by the convolutional as well as the pooling layers. A fully connected layer at the end would squash the spatial information of different objects and likely discourage disentanglement. To further encourage the encoder to disentangle objects along with spatial information, we prepend a CoordConv layer~\cite{CoordConv} which simply adds 2 channels along the depth dimension of the incoming tensor. The 2 channels represent hard-coded $(x,y)$ Cartesian space coordinates. The CoordConv layer gives the subsequent convolutional layers a chance to know where they are in Cartesian space and may contribute to the conservation of spatial location information up to the encoded object representation. 

Note in Table~\ref{tab:arch_encoder} the drastic increase in output filters as the network goes deeper. As argued in \cite{DenseNet}, the output filters (i.e. feature maps) can be seen as the global state of the network. Each dense block layer contributes new state and the number of filters thus increase. The rate of increase is defined by the growth rate $k$. At the final layer, the encoder outputs the latent representation that consists of 4 feature chunks of size 192, each representing a distinct image quadrant and ideally an image object, respectively.

Our encoder has a receptive field of $660 \times 660$ pixels\footnote{See Tensorflow \texttt{tf.contrib.receptive\_field.compute\_receptive\_field\_from\_graph\_def}}. We also try with an encoder that has a small receptive field of 46 pixels with the intuition that in order to build good disentangled object representations the last layer of the encoder, that is the neurons in the last layer, should roughly only see one quarter of the input image, equivalent to one quadrant with one object as opposed to the entire image with 4 objects. However, the experiments conducted do not yield better visual results (see Appendix~\ref{app:failed_exps} for more details).

The decoder architecture is shown in Table~\ref{tab:arch_decoder}. The design rationale for the decoder is to be approximately symmetric to the encoder yet to give it more capacity to support the generation of the mixed scene images as this is likely the more difficult task than to reconstruct the original image. Encouraged from evidence in \cite{SAGAN}, we add spectral normalization in the decoder for each convolutional layer. A peculiarity in the upsampling process is the use of the resize convolution~\cite{ResizeConv} in place of the more frequent transpose convolution. An autoencoder calibration experiment we conduct shows that resize convolution in the decoder outperforms transpose convolution in terms of better reconstruction quality and higher PSNR values, respectively. TODO see figure XYZ (FIGURE erstellen WITH PSNR values from AE CALIBRATION EFFORTS).

\input{includes/architecture_decoder.tex}

For the classifier, analogous to \cite{DisentFacOfVarByMixTh}, we use AlexNet with batch normalization after each convolutional layer as described in Section~\ref{subsubsec:alexnet}. The 2 image inputs for the classifier are concatenated along the RGB channels (see Figure~\ref{fig:model_cls}).

The discriminator is implemented as a PatchGAN discriminator with a receptive field of 94 pixels TODO 14.03 SHOULD BE 46 PIXELS!. It is defined in Table~\ref{tab:arch_dsc}. The discriminator does not use CoordConv layers.

\input{includes/architecture_patchgan.tex}

In Table~\ref{tab:params} we show the number of parameters of each network. The generator is given more capacity than the discriminator as it faces the demanding task of understanding and generating realistic images compared to the discriminator deciding a yes/no question.
\begin{table} [ht!]
\centering
\begin{tabular}{l|c}
\Xhline{0.8pt}
\textbf{Network} & \textbf{Parameters}\\
\hline
Discriminator & 2,995,063 \\
\hline
Generator & 8,100,851 \\
\hline
Encoder & 2,800,512 \\
\hline
Decoder & 5,300,339 \\
\hline
Classifier & 45,355,044 \\
\hline
\textbf{Total} & \textbf{56,450,958} \\
\Xhline{0.8pt}
\end{tabular}
\caption{Number of parameters of each network.} \label{tab:params}
\end{table}

TODO: add param values for visual similarity detection algorithm, e.g. tau=16000

%=========================================================================
% EXPERIMENTS
%=========================================================================
% \chapter{Experiments}
\newpage
\section{Experiments}\label{sec:experiments}
In this section, we first perform different kinds of experiments on a real-world dataset to validate the
effectiveness of our method. We then perform ablation studies on our proposed
method. 
We experimentally show...

% Paolo: - SO ANOTHER THING THAT ONE WOULD DO AFTER DOING THIS IS TAKE THE ENCODER AND SEE HOW WELL THAT GUY TRANSFERS TO OTHER TASKS (possible continuation)

From [34]: It is in general difficult to evaluate how ‘good’ the generative model is. Indeed, however, either subjective or objective, some definite measures of ‘goodness’ exists, and essential 2 of them are ‘diversity’ and the sheer visual quality of the images.

\par Show what is used during training and what not during testing (e.g. discriminator)
    
\par A baseline could also be pure chance s.a. 50 percent chance..

\par In \cite{1611.03383} Vielleicht als Inspiration für 'Qualitativ evaluation': "we performed nearest neighbor retrieval in the learnt embedding spaces... We computed the corresponding representations for all samples (for the unspecified component we used the mean of the approximate posterior distribution) and then retrieved the nearest neighbors for a given query image"

\par Idea: for comparison with other meethods, use encoder to build a classifier and apply to MSCOCO or ImageNet => see paper https://arxiv.org/pdf/1902.09229v1.pdf: "unsupervised representation learning: using unlabeled data to learn a representation function f such that replacing data point x by feature vector f(x) in new classification tasks reduces the requirement for labeled data"

\par IDEE FID/IS 08.03 (siehe https://arxiv.org/pdf/1903.02271.pdf Seite 22): C. FID and IS training curves
    
%-----------------------------------------------------------
% Datasets
\subsection{Datasets}
For the training and evaluation of our model we use 2 datasets which we describe next.

\subsubsection{Microsoft COCO}
The Microsoft Common Objects in Context (COCO) dataset \cite{MsCoco} is a large, natural multi-object image dataset for object detection, segmentation, and captioning.
As the name suggests, COCO images are taken from everyday scenes that add the "context" to the objects captured. COCO provides about 330,000 images in total of which more than 220,000 are labeled. On average, COCO contains 3.5 categories and 7.7 instances per image. In other words, most often an image contains more than 3 categories and more than 7 objects (instances). Unlike the ImageNet dataset that contains one object per image, COCO is thus a reasonable choice for training of our method which assumes an image contains 4 objects, one per quadrant. Another interesting observation is only 10\% of the images in COCO have only one category per image. This means only few images have all objects of the same type and therefore the images are highly diverse, which makes for a complex, natural dataset and a difficult learning challenge for our method. We use the 2017 update of COCO which comes with 118,288 labeled training images, 5001 validation images and 40,670 test images. As our method is unsupervised, we do not use any labels. However, to ramp up the initial training experiments, we utilize the bounding box annotation to create datasets where each image has at least 4 objects (i.e. 4 bounding boxes). We thus create different datasets out of the available COCO data. The versions are listed in Table~\ref{tab:coco_datasets}. We use data augmentation to increase the dataset size and support the learning process, respectively.

\begin{table}[ht!]
\centering
\begin{tabular}{c|c|c|c|c|c}
\Xhline{0.8pt}
\textbf{Source} & \textbf{Purpose} & \textbf{Id} & \textbf{Image constraints} & \textbf{Data} & \textbf{Size} \\
\textbf{dataset} &  &  &  & \textbf{augmentation} &  \\
\hline
training & training & tr\_v4 & have at least 4 bounding boxes, & horizontal flip & 135,554 \\
 &  &  & size at least 200 pixels &  &  \\
\hline
training & training & tr\_v5 & have at least 4 bounding boxes, & horizontal flip, & 338,865 \\
&  &  & size at least 200 pixels & random crops &  \\
&  &  &  & with scaling &  \\
\hline
training & FID/IS & tr\_v6 & size at least 200 pixels & - & 10,482 \\
\hline
test & FID/IS, & te\_v2 & size at least 200 pixels & - & 10,492 \\
 & testing &  &  &  &  \\
\Xhline{0.8pt}
\end{tabular}
\caption{COCO training and test datasets used for LORBMS.} \label{tab:coco_datasets}
\end{table}
Notice the creation of a dataset involves the algorithm described in Section~\ref{subsec:img_clustering} and listed in Algorithm~\ref{alg:vis_sim_det_algo}, respectively. As indicated in Table~\ref{tab:coco_datasets} for dataset tr\_v5, we augment each original image in 4 ways using horizontal flip and random crops with different scale as depicted in Figure~\ref{fig:dataaug_elephant}. To some extent, these augmentations will increase the size of objects and move some objects entirely into one quadrant as desired by the method. However, objects spanning more than one quadrant can result as well.
\begin{figure}[ht]
\centering
% see figure_generator.py
\includegraphics[width=0.5\textwidth]{images/elephant_dataaug_2.png}
\caption{Augmentations on each original image including horizontal flip and random crops with scaling for training set tr\_v5.}
\label{fig:dataaug_elephant}
\end{figure}


\subsubsection{STL-10}
The STL-10 \cite{singleLayerNetworks} dataset is a single-object image recognition dataset for single-label classification. It is intended also for unsupervised feature learning and provides a set of 100,000 unlabeled images. We use it only for transfer learning experiments in a supervised setting. To that end, STL-10 provides 5000 labeled training images with 500 images per class and 8000 test images with 800 images per class. The 10 classes are airplane, bird, car, cat, deer, dog, horse, monkey, ship, truck. Each image contains one object only which is roughly center aligned. The images are of size $96 \times 96$ pixels and in our experiments we downsize them to $64 \times 64$, the input size of our model. STL-10 is similar to the CIFAR-10 dataset. However, CIFAR-10 comes with image size $32 \times 32$ only which is the main reason we use STL-10. STL-10 is a complex dataset with high inter-class diversity as well as high intra-class diversity.


\subsubsection{PASCAL Visual Object Classes 2012}
The PASCAL Visual Object Classes dataset~\cite{PascalVocDataset} is a realistic scenes dataset for multi-label classification, detection, and segmentation. The 2012 version comes with 11,540 labeled training images with 20 object classes. Unlike STL-10, PASCAL VOC images can have multiple classes per image and are thus suitable for multi-label classification tasks. The 20 classes are person, bird, cat, cow, dog, horse, sheep, aeroplane, bicycle, boat, bus, car, motorbike, train, bottle, chair, dining table, potted plant, sofa, tv/monitor. The images are of varying size and in our experiments we randomly crop them at $64 \times 64$ to match our model.


\subsubsection{ImageNet ILSVRC2012}
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset \cite{ILSVRC2012} is the basis for the ILSVRC, an annual computer vision competition. The dataset contains 1,281,167 labeled training images with 1,000 object categories for single-label image classification, object detection and segmentation. It comes with 50,000 validation images and 100,000 test images. 
% http://www.image-net.org/challenges/LSVRC/2012/

% \subsubsection{Places}
% A 10 million Image Database for Scene Recognition. The Places dataset contains significantly more complex factors % of variation than MNIST.

\subsection{Qualitative Evaluation}
TBD

\subsubsection{Mixed scene renderings}
Using our method in inference mode, we showcase renderings of mixed feature vectors and scenes, respectively, along with the corresponding input images in Figure~\ref{fig:results_lorbms_mixed_scenes}. One row represents one forward pass through our model and thus one distinct example. The examples do not always meet the \textit{4 object assumption} made in Section~\ref{subsec:concept} that the reference image in column $I_{ref}$ consists of up to 4 objects spread evenly across its quadrants. We can observe how the model copes with this misalignment in column $I_{mix}$. For more specific object transfer experiments in line with the model assumptions see Section~\ref{subsubsec:objTransfExp}. 
\input{includes/results_lorbms_mixed_scenes.tex}
To produce these results only the autoencoder of our method, that is the encoder and the decoder as described in Section~\ref{subsec:model_arch}, is used. Note that the column $I_{f_{I_{mix}}}$ exists solely for the purpose of visualizing the mixed feature vector $f_{I_{mix}}$. The renderings in column $I_{mix}$ show that the model has learnt to take the mixed feature vector and create a scene that clearly correlates with the feature mixing. Moreover, the model fuses the quadrant boundaries as seen in column $I_{f_{I_{mix}}}$ smoothly. This demonstrates that the model adds a degree of realism to the rendering other than a simple reverse encoding of the feature vector as depicted in column $I_{f_{I_{mix}}}$. Even in case where the mixed feature vector does not meet the \textit{4 object assumption} well, the model often manages to render a somewhat meaningful output. Yet as we show in Figure~TODO-ADD-REF-TO-FID-CHART, the FID values of these renderings are still relatively high compared to other generative models even though many of them use much simpler data. Thus the realism of the renderings still offers room for improvement.
Please see Appendix~\ref{app:samples} for more rendering examples.


\subsubsection{Object Transfer Experiments}\label{subsubsec:objTransfExp}
In this experiment we investigate visually the capability of the system to transfer objects from one scene to another. To that end, we take an image that contains an object and a second image as the background scene we want the object to transfer to. To implement the object transfer we use only a subset of our system including the autoencoder and feed it the 2 images along with a mask to define in which quadrants the transfer object is located and which quadrants to mix, respectively. For instance, as schematized in Figure~\ref{fig:objectransfer_arch}, we take an image $I_{obj}$ with a car object located in the bottom left quadrant together with a scene $I_{ref}$ depicting a street and pass the pair along with the mask $0010$ to the subsystem. Based on the feature mixing defined by the mask, the subsystem generates a new scene $I_{mix}$ as shown on the right.
\begin{figure}[ht]
\centering
% see https://www.draw.io/#G1JXyUs-KLxHF4mB5HcVqmNBkUprhLCuAK
% see evaluation/lorbms_objectransfer_model.py
% "image_ref_path": Master Thesis/Experiments/0404_ObjectTransfer/I_ref/street1.png",
% "image_obj_path": Master Thesis/Experiments/0404_ObjectTransfer/I_obj/car6.png",
% "ae_checkpoint_name_home": /src/data/models/exp73/DCGAN.model-62",
\includegraphics[width=0.8\textwidth]{images/object_transfer/object_transfer_model.png}
\caption{Object transfer of a car onto another street scene using a subsystem of our method.}
\label{fig:objectransfer_arch}
\end{figure}
Note that in this experiment we handpick the 2 input images from random sources by human judgment only and there is no \textit{visual similarity detection algorithm} in place as described in Section~\ref{subsec:img_clustering}. Therefore, in these samples we can observe how well our model generalizes to unseen data which otherwise might not have been selected by the training process at all.

In Table~\ref{tab:objectransfer_ex} we show object transfer renderings for a variety of handpicked objects and scenes. We observe that transferred objects do not always appear in the right pose. This shows that the system has no notion of object pose thus far. Further, the boundaries of the transferred objects are sometimes distorted when the object consists of lighter colors than its surroundings. It seems that the system is better at handling objects with darker colors (i.e. keeping its boundaries) and good at distorting lighter colors such as drawing a sky.
\input{includes/object_transfer_renderings.tex}

% TODO show examples where 1) quadrants contain more than one object, 2) quadrants contain only part of an object, 3) quadrants are without object


\subsubsection{Nearest Neighbor Retrieval}
TBD
See [70] figure 8
idea: search 5 nearest neighbors in feature space and show images in grid
% consider from Hu paper: Hence, a disentangled representation is also useful for image retrieval, where we can search for nearest neighbors of a specified attribute. Invariance is also beneficial for classification, where a simple linear classifier is sufficient to classify each attribute based on its corresponding feature chunk. This observation inspired previous work [18] to quantify disentanglement performance using linear classifiers on the full features f.

\subsubsection{Image Caption Experiments}
TBD


%-----------------------------------------------------------
% Quantitative Evaluation
\subsection{Transfer Learning} % aka Quantitative Evaluation
TBD
\subsubsection{Predicting image rotations using LORBMS as feature extractor}\label{subsec:predRotnet}
Gidaris \textit{et al.}~\cite{RotNet} propose a transfer learning experiment called \textit{RotNet}. ...


\subsubsection{Classifying STL-10 using LORBMS as feature extractor}\label{subsec:linearclassifier}
% Linear Classification
A common method to evaluate the quality of representations\footnote{e.g. the latent space vectors of an autoencoder or the output of a discriminator} learnt in an unsupervised algorithm is described by Coates \textit{et al.}~\cite{singleLayerNetworks} and applied for instance in \cite{DCGAN}, \cite{AE_Interp} . The idea is to use the representations as input to a one-layer linear classifier and train it on a supervised learning task. The performance of the model, after fitting it on top of the representations, is then evaluated. The rationale behind this evaluation procedure is that a simple classifier should achieve better performance given representations consisting of well disentangled factors of variation. For the experiments with our method, we use either the encoder or the discriminator to map images to their representations. In case of the discriminator, we use the output of the penultimate layer as input to the classifier. The experiments are performed on the STL-10 dataset with a 10 class single-label classification challenge. To that end, analogous to \cite{SpotArtifacts} we follow the testing protocol as suggested by \cite{singleLayerNetworks}. We train the classifier for 200 epochs on the 10 predefined folds using the same data augmentations as in \cite{SpotArtifacts} and report the average accuracy on the test set. We compare our model with other training procedures including supervised learning methods. The results are shown in Table~\ref{tab:comparison_stl10}.
% as in Simons paper do the experiment the same way and compare with results in table 2.
% see lorbms_main_linearcls.py
% 25.03: results based on exp70
% 26.03: NOTE: ONLY 50 EPOCHS USED FOR TIME REASONS!
% 02.04: see ~/src/evaluation
\begin{table}[ht!]
\centering
\begin{tabular}{l|c|c}
\Xhline{0.8pt}
\textbf{Model} & \textbf{Accuracy} & \textbf{SD} \\
\hline
Random & 10.0\% & - \\
Random encoder & 35.5\% & $\pm$0.3 \\ % 28.04: log_linearcls_random_encoder_No1.out
PASCAL encoder & tbd\% & $\pm$0.2  \\ % 28.04 TBD 14.04: 38.1% definitiv (cf. log_pascal4.out, log_pascalvoc_4th_as3_withMomOptim.out)
PASCAL encoder (AlexNet) & tbd\% & $\pm$tbd  \\
ImageNet encoder (AlexNet) & 51.6\% & $\pm$0.2  \\ % 29.04: pretraining: log_alexnet_No2_60ep.out, eval: log_linearcls_stl10_alexnet_after_pretrain_No1.out
STL-10 encoder & 77.2\% & $\pm$0.1 \\ % 27.04: pretraining: log_stl-10-pretraining__encoder_No2.out; eval: log_linearcls_stl10_encoder_after_pretrain_No1.out
STL-10 encoder (AlexNet, conv1-conv5) & 57.5\% & $\pm$0.1 \\ % 27.04: pretraining: log_stl-10-pretraining__alexnet_No1.out; eval: log_linearcls_stl10_alexnet_after_pretrain_No2.out
\hline
Swersky \textit{et al.}~\cite{Stl10TlExp2Comp} & 70.1\% & $\pm$0.6 \\
Jenni \& Favaro~\cite{SpotArtifacts} (frozen) & 76.9\% & $\pm$0.1 \\
\hline
(Ours) LORBMS encoder (frozen) & TBD\%  & $\pm$0.7  \\
(Ours) LORBMS encoder (finetuned) & TBD\% & $\pm$1.0  \\
(Ours) LORBMS discriminator (frozen) & TBD\%  & $\pm$TBD  \\
(Ours) LORBMS discriminator (finetuned) & TBD\%  & $\pm$TBD  \\
\Xhline{0.8pt}
\end{tabular}
\caption{Comparison of average test-set accuracy on STL-10 with different models and other published results.} \label{tab:comparison_stl10}
\end{table}
Note that the \textit{STL-10 encoder} model uses supervised training for both our encoder (initialized randomly) and the classifier on the STL-10 training dataset. It unsurprisingly achieves better performance than our unsupervised model. For the \textit{PASCAL encoder} we first train our encoder according to \cite{PascalVocTraining} using the momentum optimizer for 80,000 iterations on the PASCAL VOC 2012 dataset. It is a 20-way multi-label classification task where we use a single-layer classifier appended to the encoder. The encoder is then frozen and transferred to the evaluation on STL-10. For the \textit{Random encoder} model we initialize the encoder with random weights and keep them fixed while training only the classifier. The classifier performs better when using the pretrained encoder from our method. For the training of the classifier we use default parameters for the Adam optimizer and do not use data augmentation. Note that the referenced approaches have different architectures (e.g. \cite{SpotArtifacts} uses an entire AlexNet without the FC layers equalling 3.7M parameters) from ours (i.e. TODO parameters) and are therefore just indicative.

We observe that TODO ... This empirical evidence demonstrates that our method TODO ...
% Notice TODO...EIN PAAR FOLGERUNGEN ANFUEGEN HIER SOBALD ALLE ZAHLEN VERFÜGBAR...
% TODO from [70]: "To demonstrate that the learnt representations generalize to other input data.."

\subsection{Ablation Analysis}
TBD
E.g see [70] Table 1:
Table 1. Influence of different architectural choices on the classification
accuracy on STL-10 [4]. Convolutional layers were pretrained
on the proposed self-supervised task and kept fixed during
transfer for classification.
OR SEE SN paper [31] table 1 and figure 1: you just add a table with the different ablation experiments and then 2 figures where you show the FID and the IS for the different ablation experiments on COCO.



%=========================================================================
% DISCUSSION AND CONCLUSION
%=========================================================================
% \chapter{Conclusions}
\section{Conclusions}\label{sec:conclusion}
% "Often the writer uses the conclusions/implications section to merely restate the research findings. Don't waste my time. I've already read the findings and now, at the Conclusion/Implication section, I want you to help me understand what it all means... ..what are the key ideas that we can draw from your study to apply to my areas of concern?"
The results in this thesis suggest that ...
None of our image preprocessing attempts
We show ...
The first is
to elaborate on the impacts of using your approach. The
second is to state limitations or disadvantages of your solution,
thus enabling you to provide directions for future
research in the field.

% The conclusions section, similar to the introduction and
% related work sections, serves 2 purposes. The first is
% to elaborate on the impacts of using your approach. The
% second is to state limitations or disadvantages of your solution,
% thus enabling you to provide directions for future
% research in the field.

%=========================================================================
% FUTURE WORK
%=========================================================================
\section{Future work}
In general, run the experiments.... Do random search for a good set of hyperparameters. A lot of time was spent running experiments with varying sets of hyperparameters using sequential optimization.

\par One could automate the time consuming hyperparameter search by using Bayesian optimization or genetic algorithms. Note that these methods are also inherently sequential by nature. https://vimeo.com/250399261
Another promising approach to consider is random search where the hyperparamters are drawn from a distribution and the training happens in parallel where at the end of the run the best performing set of hyperparameters is picked. However, this approach also requires a substantial amount of work to put in place around a highly non-standard model architecture such as LORBMS. An even more interesting approach to consider is the recently proposed method called 'population based training' by Jaderberg \textit{et al.}~\cite{PopBasedTraining} which is based on random search but where sets of hyperparameters and their performances are compared to the population repeatedly during training and then exploited and built upon as training continues. However, for an efficient execution of this method it requires many GPUs. 

\par The disentangling of image attributes within disentangled objects provides an interesting avenue for future work. how do I enforce invariance between object representations and object attribute representations, respectively?

\par "Note that this assumption restricts the model to coarse movement" (see sec concept) -> Refine the system to more fine grained object translations.

\par Greff inspired: our system does not have an explicit notion of object location (or any other attributes). The system models an object in a coarse fashion where an object is exposed as a single entity and the location information, which would be a very important piece of information to have access to explicitly, is somewhere hidden in that entity.

\par change architecture to produce resolution of 128 x 128

% Read and reread this section until you are sure that you have made suggestions that emanate from your experiences in conducting the research and the findings that you have evolved. Make sure that your suggestions for further research serve to link your project with other projects in the future and provide a further opportunity for the reader to better understand what you have done.
% The best way to neutralize the concerns of critical reviewers is to acknowledge the holes in your arguments, and propose experiments for future work which will place these arguments on stronger experimental footing

%=========================================================================
% BIBLIOGRAPHY
%=========================================================================
\printbibliography


%=========================================================================
% APPENDIX
%=========================================================================
\begin{appendices}
\section{Appendix}
\subsection{Network Structure}
Here we give the network structures..

% TODO add SAGAN exp: While experiments conducted with the self-attention module in the discriminator shows promising results, other methods, namely the PatchGAN discriminator, performs even better in the context of our method.

\subsection{Experiments with non-significant results}\label{app:failed_exps}
\begin{flushright}
\rightskip=2.8cm\textit{``I have not failed. I've just found 10,000 ways that won't work.''} \\
\vspace{.2em}
\rightskip=1.8cm---Thomas A. Edison
\end{flushright}
Here we list the conducted experiments that did not produce significant results. TODO:
\begin{enumerate}
  \item add self-attention layer (cf. SAGAN) in decoder before last DenseBlock. Rationale: ...
  \item use SqueezeNet instead of AlexNet for classifier because of lack of memory/increase batch size. 12.02: 
\end{enumerate}

\subsubsection{Squeezenet}
We take SqueezeNet instead of AlexNet as the classifier network implementation with the aim to save memory consumption and in favor of a bigger batch size for faster training. In the training process however we noticed that Squeezenet would permanently make an assignment prediction of 0.5 even after the first iteration no matter the input whereas AlexNet would take up the training process as expected. A look into Squeezenet's penultimate layer before the final average pooling layer revealed that its activation was all 0 after the first backpropagation already. We conjecture that with a cross entropy loss function where most ground truth values are 0 (i.e. actual assignments), SqueezeNet suffers from a lack of numerical stability around 0 and therefore collapses to 0 from which is does not recover. This behavior can be reproduced consistently with the test program \texttt{test\_squeezenet\_0\_collapse.py}. We dismissed the idea of using SqueezeNet altogether.

\subsection{Visual similarity detection algorithm examples}\label{app:vsda_examples}
In Figure~\ref{fig:results_vis_sim_det_algorithm} we show results from the \textit{visual similarity detection algorithm} as described in Section~\ref{subsec:img_clustering} on the MS COCO dataset.
\input{includes/results_visual_sim_det_algorithm.tex}

\subsection{Sheets of samples}\label{app:samples}
...
Figure 9: WGAN algorithm: generator and critic are DCGANs
...
Figure 10: Standard GAN procedure: generator and discriminator are DCGANs

\subsection{Training vs. Test FID}\label{app:train_vs_test_FID}
TBD
% TODO: see paper https://arxiv.org/pdf/1811.11212.pdf table 2: their methods also have high FID values (150, 200...)
% TODO see StyleGAN [50]: "In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training"

\end{appendices}


\end{document}
