\documentclass[11pt,a4paper]{article}

\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{appendix}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii,
sorting=nty,
%style=alphabetic
%style=reading
style=ieee,
citestyle=ieee
]{biblatex}
\addbibresource{bibliography.bib}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{  \tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{Learning object representations by mixing scenes}

\author{Lukas Zbinden\\
Computer Vision Group\\
Computer Science Department, University of Bern\\ 
2018\\
{\tt\small lukas.zbinden@unifr.ch}
}

\maketitle
%\thispagestyle{empty}

%=========================================================================
%%%%%%%%% ABSTRACT
\begin{abstract}
   Our project 
   abstract: in 4 sentences: state the problem; say why it's an interesting problem; say what your solution achieves; say what follows from your solution
   Do not use custom terminology in the abstract;
use terms and concepts that are widely accepted.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The dawn of deep learning in 2012 with the introduction of the groundbreaking AlexNet neural network has reignited the idea of an intelligent computer that is on a par with the human brain. After much disappointment in the late 21st century, the decade old research field of artificial intelligence and machine learning in particular was revived around the globe. While today research is still far away from the lofty aim of a truly intelligent machine, impressive achievements have been published continuously (cite?). One research area in machine learning where progress has stood out in recent years is computer vision. The field of computer vision aims at equipping a machine with artificial eyesight to process and interpret visual signals such as images and video. For instance, the computer has learned to classify objects in images with an accuracy that exceeds human performance or detects skin cancer with a higher precision than doctors. Other significant applications such as autonomous driving or medical image analysis have emerged from the ability to artificially understand visual data and presumably many more are yet to be discovered. Within computer vision, one area of research focuses on the representation of visual data such as objects or attributes thereof in lower dimensional space, the latent or feature space. Considering that an image resides in a high dimensional space, the aim of representation learning in the context of computer vision is to learn a model that successfully transforms a given image into a lower dimensional representation that is meaningful and captures the significant image features in condensed form. This type of dimensionality reduction opens up an array of new possibilities for downstream applications that make use of latent space representations. Furthermore, some research imposes additional constraints on the low dimensional representation to lay the groundwork for more fine-grained methods. This research is called disentangling factors of variation. A factor of variation represents an image object or image attribute in latent space. This research aims to learn a latent space representation that is not only low dimensional but has an inherent structure that includes disentangled factors of variation. With such a structure at hand, latent space interpolations become possible where objects or attributes are swapped, altered or used to perform basic arithmetic operations. This work centers around the research field of disentangling factors of variation given image data.

In this work we propose a novel machine learning model capable of learning object representations by mixing scenes. The solution receives five input images of scenes and based on them generates a new scene that contains a mixture of some of the input content including objects and background. The model is able to perform this operation because it has learned to extract object representations from images and which are well disentangled and permit to selectively choose factors of variation to compose a representation that is then decoded into a new scene. The model thus encodes an image and extracts a feature representation that has such well disentangled factors of variation (i.e. image objects and attributes in latent space) which makes it possible to take this representation and swap or move objects around within it and thereby manipulating the content of the image to generate. Given two real images containing scenes, the model generates a new realistic scene that mixes the content of the two inputs. For instance, the two scenes could be pictures taken in a park with multiple persons and dogs in each. The generated picture then also represents a scene in a similar looking park with people and dogs mixed together from both input scenes. The model extracts objects from both scenes and incorporates them in a new made-up scene. The mixing of the objects is done in the latent space based on the extracted object representations.

The learned object representations open up a variety of possible downstream applications such as object interpolation. The model is trained in an unsupervised fashion without any human interference. Our work aims to produce a representation that is made up of disentangled factors of variation representing image objects and object attributes, respectively. Experiments showcase the potential of the model through qualitative and quantitative evaluation of the generated samples.

The object representations could be used for fine-grained image search where the search would receive an image showing some objects and it would extract those objects as features in the feature space and then run the search in the feature space space for images having a similar feature i.e. features that are close based on L2 distance and kNN. This search would work without requiring any labels and find images containing similar objects as the query image.


...........................................................................................
The introduction serves a twofold purpose. Firstly, it
gives the background on and motivation for your research,
establishing its importance. Secondly, it gives a summary
and outline of your paper. When you write the background review, you should
consider including technological trends of the area, open
problems and recent promising developments.
A proper flow is to first set the context,
then present your proposal, then provide the verification,
and lastly wrap up with conclusions

- says "I am going to look at the following things"
- introduce area of study - representation learning, disentangling factors of variation
- Describe the problem, state your contributions and the implications
- say what is really new in your work
- tell what your work is about, what problem it solves, why the problem is interesting, what is really new in your work, why it's so neat
- set the proper context for understanding the rest of your paper!
- introduce your idea using EXAMPLES and only then present the general case, explain it as if you were speaking to someone using a whiteboard;
...........................................................................................

learned representations, data space

disentangling factors of variation in image data. 

feature = numeric representation of raw data
feature space is a vector space
feature = high dimensional vector
a collection of data points then becomes a point cloud in the feature space 
model = mathematical "summary" of features (summary -> a gemoetric shape)
model = geometric summary of point cloud
point cloud = approximate geometric shape
disentangling image features into semantically meaningful properties

"a separate set of dimensions" = a feature chunk

Devise a manifold learning algorithm that can discover and capture independent factors of variation.

\par Concretely, we used a ... to model the distribution over image features and the latent factors of variation.

% - from [12]
Representation learning: Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsupervised learning algorithms such as autoencoders seek efficient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables. 

% -- from [26]
While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. For example, for a dataset of faces, a useful disentangled representation may allocate a separate set of dimensions for each of the following attributes: facial expression, eye color, hairstyle, presence or absence of eyeglasses, and the identity of the corresponding person. A disentangled representation can be useful for natural tasks that require knowledge of the salient attributes of the data, which include tasks like face recognition and object recognition.








%=========================================================================
\section{Related work}
Secondly, it provides a critique of the
approaches in the literature—necessary to establish the
contribution and importance of your paper. 
Critiquing the major approaches of the background
work will enable you to identify the limitations of the
other works and show that your research picks up where
the others left off.

This work builds on the results of \cite{1711.07410} and \cite{1711.02245}.
\cite{1711.02245} identifies two main challenges in disentangling factors of variation: the shortcut problem and the reference ambiguity. In the shortcut problem the model learns degenerate encodings in which all information is encoded only in one part of the feature, i.e. either in the component $c$ for common attributes shared by both images or the component $v$ for varying attributes. For an image pair there are two $v$ and one $c$. The encoder maps a complete description of its input into vector $N_v$ and the decoder completely ignores vector $N_c$.
\newline The second challenge is named reference ambiguity where the attribute representation for an image is not guaranteed to follow the same interpretation on another image. In other words, the reference in which a factor is interpreted may depend on other factors which makes the attribute transfer ambiguous. For example, the viewpoint angle of a vessel gets interpreted in two different ways depending on the boat type.
%-- [16] start -------------------------------
\par[Understanding Degeneracies and Ambiguities in Attribute Transfer] addresses these challenges. They introduce an adversarial weakly supervised training method that uses image triplets and fully tackles the shortcut problem by means of a composite loss function consisting of an autoencoder loss and an adversarial (i.e. GAN) loss. Used in conjunction they provably avoid that challenge. 
Furthermore, they analyze the reference ambiguity and prove that it is unavoidable when disentangling with only weak labels. The problem occurs when a decoder reproduces the data without satisfying the disentangling properties for the varying attribute $v$. Practically, this means not all the factors of variation can provably be disentangled from weakly labeled data (i.e. when only $c$ is known for image pairs).    
%-- [16] end -------------------------------

%-- from [12]
In previous approaches, content vs style, form vs motion, facial expression vs identity were explored.

%-- [15] start -------------------------------
\par\cite{1206.5538} elaborates on representation learning whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors. In 3.5 they distinguish between learning invariant features and learning to disentangle explanatory factors. They conclude that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical. Doing so they propose should give rise to a good representation significantly more robust to the complex and richly structured variations extant in natural images for AI-related tasks. The manifold hypothesis is introduced which makes the assumption that the data lies along a low-dimensional manifold where the probability mass is highly concentrated. They argue that a representation being learned can be associated with an intrinsic coordinate system (on the embedded lower-dimensional manifold). For instance, this can be demonstrated well by a variational autoencoder.
%-- [15] end -------------------------------

%-- [27] start (nicht sicher ob inkludieren soll) ------------------------
[Semi-supervised learning with deep generative models] utilizes a variational autoencoder in a semi-supervised learning paradigm which is capable of separating content and style in data.
%-- [27] end -------------------------------

%-- [12] Discovering Hidden FoV in DN -- start -------------------
\par\cite{1412.6583} augments autoencoders with regularization terms during training. They use an unsupervised cross-covariance penalty (XCov) as a method to disentangle class-relevant signals (observed variables) from other factors in the latent variables along with a standard supervised cross-entropy loss. In case of MNIST they consider the class label as a high-level representation of its corresponding input. Their model learns a class invariant smooth continuous latent representation $z$ that encodes digit style (slant) whereas the observed variable $\hat{y}$ represents the digit itself. On the TFD dataset, the observed variable $\hat{y}$ encodes the facial expression while the autoencoder is able to retain the identity of the faces through latent variable $z$. Here the XCov penalty prevents expression label variation from ‘leaking’ into the latent representation.
%-- [12] end -------------------------------

%-- [20] start -------------------------------
\par\cite{1701.00160} tutors GAN in detail. 
%-- [20] end -------------------------------

%-- GAN start -------------------------------
Good basic desc of GAN working here:
https://arxiv.org/pdf/1812.00964.pdf

- This results in a minimax game in which the generator is forced by the discriminator to produce ever better samples
- One big drawback of GANs is the notorious instability of the discriminator during training => used spectral norm
- DSC: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution
- model/generative/ distribution
- The generator G implicitly defines a probability distribution $p_g$ as the distribution of the samples
G(z) obtained when z ~ pz.


"Note that this loss indicates convergence in the
training because the discriminator is no more able to teach the generator how to improve, but it does
not ensure that the generator can generate good-quality reconstructions. In the following sections
we describe some ways to balance generator and discriminator in order to avoid convergence to
sub-optimal perform"
%-- GAN end -------------------------------

%-- [24] DCGAN start -------------------------------
\par\cite{1511.06434} introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate for the first time that they are a strong candidate for purely unsupervised learning. The authors propose a more stable set of architectures for training generative adversarial networks and give evidence that adversarial networks learn good representations of images. According to Ian Goodfellow, DCGAN defines a quasi-standard generator network architecture. 
%-- [24] DCGAN end -------------------------------

%-- [26] INFOGAN start -------------------------------
\par\cite{1606.03657} describes InfoGAN, a type of GAN that learns disentangled representations in an unsupervised manner (no supervision of any kind) with the addition of maximizing the mutual information between a small subset of the latent variables and the observation. InfoGAN can disentangle both discrete and continuous latent factors, scale to complicated datasets, and typically requires no more training time than regular GANs.  The effective results suggest that generative modelling augmented with a mutual information cost be a fruitful approach for learning disentangled representations. They used DCGAN for implementation and stable GAN training, respectively. Gist: concat noise vector $z$ with latent vector $c$ to form input for generator G. Additionally, extend discriminator D with Q-network on top to generate $\hat{c}$ to then calculate the reconstruction loss with $c$ and train entire network accordingly using backprob (aside the standard GAN training).
%-- [26] INFOGAN end -------------------------------

%-- [32] SAGAN start -------------------------------
\par \cite{1805.08318} introduces self-attention to the GAN framework, enabling both the generator and the discriminator to efficiently model relationships between widely separated spatial regions. Convolution processes the information in a local neighborhood only, thus using convolutional layers alone is computationally inefficient for modeling long-range dependencies in images.
%-- [32] SAGAN end -------------------------------

%-- CNN start -------------------------------
\par CNNs: As most of parameters exist in fully connected (FC) layers
%-- CNN end -------------------------------

\par Many prior works on DFoV are fully supervised, i.e. they use labels for all factors of variation to be disentangled.

%=========================================================================
% Problem statement
\section{Disentangling factors of variation}
\par the ULTIMATE OBJECTIVE of this is: we want to encode an image and get a feature representation that has such well disentangled features (2D) and attributes (3D) that it makes it possible to take this representation and move features or attributes around within in and therefore move or change objects in the feature space (manifold?)

\par image attribute - factor of variation - feature vector - feature chunk
an object representation is learned as a concatenation of feature chunks
uses high-dimensional feature chunks
feature space is only designed for attribute transfer but not for sampling
how is disentanglement achieved? 
-> invariance objective i.e. an architecture with mixing autoencoders encourages disentangled encoding of attributes and disentangled decoding of feature chunks, respectively
-> classification objective such that each feature chunk corresponds to a discernible attribute in the original image 
encoding of an attribute, decoding of a feature chunk

\par\cite{1412.6583} states in 2014 that there is a lack of standard benchmark tasks for evaluating disentangling performance. Their evaluation is based on examining qualitatively what factors of variation are discovered for different datasets. ALS INSPIRATION: In [12]: "To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with zni fixed to 0 and y fixed to one-hot vectors corresponding to each class label. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation". Auch für meinen Autoencoder machen um Beispiele aufzuzeigen?

\par "A disentangled representation is generally described as one which separates the factors of variation, explicitly representing the important attributes of the data" [a framework for the quantitative evaluation of disentangled representations]

\par image space vs feature space paper [42]

\par This work pushes the idea of \cite{1711.07410} further.

%=========================================================================
% Method
\section{Learning object representations by mixing scenes}
the intent of the system, the basic motivation for all of this, is that we would like to be able to move objects around by editing the feature space. the purpose in the end is to build a representation where I take a feature from one and plug it there and then the rendering adapts the content - that's the ultimate goal.
\par image attribute - factor of variation - feature vector - feature chunk
\par an object representation is learned as a concatenation of feature chunks

\par ***
Put good effort in realizing
all explicit and implicit assumptions that you make, and
clearly state them.
***

\par Show what is used during training and what not during testing (e.g. discriminator)

\par and a GAN is used to improve the realism of the results

\par Following is the minimax game applied to our method:
\begin{equation} \label{eq:1}
\min\limits_{G} \max\limits_{D} V(D,G) = \mathbb{E}_{x\sim p_{data} (x)}\big[log\, D(x)\big] + \mathbb{E}_{x\sim p_{tile} (x)}\big[log(1 - D(G(x)))\big]
\end{equation}
The optimal value for the mini-max game in Eq. \ref{eq:1} converges to the value $V(G,D)=-log\,4$.
Practically, the expectation value is the average over the training images x.

\par LEARN A MODEL THAT IDENTIFIES OBJECTS IN THE SCENE AND LEARNS TO COMPOSE THEM SO YOU CAN MERGE SCENES.

\par https://arxiv.org/pdf/1803.06414.pdf "Learning to Segment via Cut-and-Paste": We propose and formalize a new cut-and-paste adversarial training scheme for box-supervised instance segmentation, which captures an intuitive prior, that objects can move independently of their background.
-> perhaps use latter sentence as idea for the intuition that we're trying to "move objects around independently of their background"

\par what is the theoretical groundwork for the proposed method?

\par We use a GAN to check for realism of the generated images.

\par This subsection explicates the reason ...

\par A baseline could also be pure chance s.a. 50 percent chance..

\par an encoder for computing a low-dimensional latent representation, and a decoder for synthesizing the output image.

how do I enforce invariance between object representations and object attribute representations, respectively?
x3 should be a valid image according to the input data distribution.

The classifier consists of $3x3x8$ multi-class classifiers, one for each chunk, that decide for each chunk the original image ($x_1$, $x_2$) and the specific tile, respectively, that was used to generate a tile of the composite image.

(Hinweis: [12] has nice formulation of cost function, perhaps use as inspiration)

\par To generate the unbalanced mask $m$ with a bias towards the first of the two contexts, we used a Bernoulli distribution with $p=0.6$ and $q=1-p=0.4$, respectively, such that $Pr(X=1)=0.6$ and $Pr(X=0)=1-P(X=1)=0.4$ where $1$ represents the first context (i.e. image $x_1$) and $0$ the second context (i.e. image $x_2$).

\par During training of modified DCGAN encountered lack of GAN training, i.e. the generator loss was immediately 0, the discriminator loss 27.6 ($dsc_loss_real$ was 0 and $dsc_loss_fake$ was 27.6 (meaning the discriminator always output 1 no matter the input). To counteract this problem we implemented and used spectral normalization for GANs (also SNGAN, \cite{1802.05957}) to regularize the Lipschitz constant. With the latter in place, the GAN training set in. We first added to the discrminator, and later according to TTUR paper also in the autoencoder. Note that spectral normalization does not use extra weights, therefore comes at no memory cost.
Important: also mention link to Lipschitz continuity: "They applied spectral normalization to the weights in both generator and discriminator, unlike the previous paper which only normalizes the discriminator weights. They set the spectral norm to 1 to constrain the Lipschitz constant of the weights. It’s just used for controlling the gradients. This spectral normalization idea was first introduced by Miyato et. al." https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b

\par In high dimensional spaces, embeddings?

\par Some methods are based on new mathematical models and others based on intuition back up by experiments.

\par In \cite{1611.03383} Vielleicht als Inspiration für 'Qualitativ evaluation': "we performed nearest neighbor retrieval in the learned embedding spaces... We computed the corresponding representations for all samples (for the unspecified component we used the mean of the approximate posterior distribution) and then retrieved the nearest neighbors for a given query image"

\par We used the pretrained AlexNet model from the work of DeepCluster (\cite{1807.05520}) to cluster the dataset into groups of similar images.

\subsection{Architecture}
Training Procedure: Our models are implemented in TensorFlow \cite{1605.08695} and are trained using a batch size of 4 instances for the generator and 8 instances for the...
Cf. paper https://arxiv.org/pdf/1803.06414.pdf

\par A search on Google (21.11.18) for standard autoencoder architectures trained on MS COCO dataset did not yield any useful results. The idea was to draw inspiration from previous work and to ensure as much as possible that the autoencoder architecture would be a strong fit to synthesize images following the distribution of the MS COCO dataset.

\par The rationale behind the use of dense blocks is that multiple stacked CONV layers can develop more complex features of the input volume (before the destructive pooling operation). The core idea of dense blocks is that for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. A small growth rate $k$ is sufficient to achieve good results. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth
rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Cf. http://cs231n.github.io/convolutional-networks/ and DenseNet paper.

\par INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
Here we see two CONV layers stacked before every POOL layer. This is generally a good
idea for larger and deeper networks, because multiple stacked CONV layers can develop
more complex features of the input volume before the destructive pooling operation.

%=========================================================================
\section{Experimental Results}
We first evaluate the performance of our learned representations on different
transfer learning benchmarks. We then perform ablation studies on our proposed
method. We also visualize the neurons of the intermediate layers of our network.
We experimentally show...

\subsection{Experiments}
We now experimentally evaluate the method. An online serarch regarding self-attention layer (from SAGAN paper) and autoencoder does not yield any results.

\subsection{Findings in experiments}
21.11.18: Considering exp10 and exp11, it seems that the results of exp11 (i.e. without instance normalization) suggest that the use of instance norm yields better results (exp10) in that the output is not polluted with recurring artifacts/textures.

\par For so and so We use the publicly available code of Kraehenbuehl cf [43]

\par Oftentimes changes in hyperparameters only have a marginal effect on the results.

\subsection{Datasets}
\subsubsection{Places}
A 10 million Image Database for Scene Recognition. The Places dataset contains significantly more complex factors of variation than MNIST.

\subsubsection{Microsoft COCO}
On average the Microsoft COCO dataset \cite{1405.0312} contains 3.5 categories and 7.7 instances per image. Another interesting observation is only 10\% of the images in MS COCO have only one category per image.
validation set: total: 5000, greaterThan300: 4916, moreThan4: 2430
training set: total: 118287, greaterThan300: 116510, moreThan4: 57180
For data augmentation we use random crops four times per image, resulting in a dataset size of 228720.

\subsection{Training}
We train the model to maximize the data log-likelihood using SGD.
Training strategies for disentangling:...
The training with ... did not yield well-disentangled features as the generated images were blurry and not representing objects and sceneries well. 

\par How do we find a good set of hyperparameters? sequential optimization (hand tuning) by running a series of experiments that build on top of each other. https://vimeo.com/250399261

\subsection{Model Performance}
We evaluated the model using the MS Coco dataset first.
We hypothesize...
We find empirically that...
Thus, both quantitative and qualitative results demonstrate...
We found that the model is not sensitive to this hyperparameter
The model achieves strong performance in ...

\par Paper https://arxiv.org/pdf/1806.05575.pdf says "The evaluation metric used for the hyperparameter search was the Frechet Inception Distance (FID)"

\subsection{Qualitative and quantitative evaluation}
See paper [a framework for the quantitative evaluation of disentangled representations]: "visual inspection remains the standard evaluation metric"; "current research generally lacks a clear metric for quantitatively evaluating and comparing disentangled representations."
Evtl. "Disentanglement metric score" verwenden aus Paper 41 für Comparison mit anderen Models?

\subsubsection{Qualitative Evaluation}
Qualitative ...

interpolation in latent/feature space
"latent space interpolations"
"Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects." Cf. 3dgan.csail.mit.edu/papers/3dgan\_nips.pdf

\subsubsection{Quantitative Evaluation}
For quantitative assessment of generated examples, we used inception score (Salimans et al., 2016)
and Frechet inception distance (FID) (Heusel et al., 2017). See paper [31]


\subsection{Ablation Study}
cf. e.g. [44]

%=========================================================================
\section{Conclusion}
None of our image preprocessing attempts
We show ...
The first is
to elaborate on the impacts of using your approach. The
second is to state limitations or disadvantages of your solution,
thus enabling you to provide directions for future
research in the field.

\section{Future work}
In general, run the experiments.... Do random search for a good set of hyperparameters. A lot of time was spent running experiments with varying sets of hyperparameters using sequential optimization.

\par One could automate the time consuming hyperparameter search by using Bayesian optimization or genetic algorithms. Note that these methods are also inherently sequential by nature. https://vimeo.com/250399261
Another promising approach to consider is random search where the hyperparamters are drawn from a distribution and the training happens in parallel where at the end of the run the best performing set of hyperparameters is picked. However, this approach also requires a substantial amount of work to put in place around a highly non-standard model architecture such as LORBMS. An even more interesting approach to consider is the recently proposed method called 'population based training' (TODO ref to paper) which is based on random search but where sets of hyperparameters and their performances are compared to the population repeatedly during training and then exploited and built upon as training continues. However, for an efficient execution of this method it requires many GPUs. 

\printbibliography

%=========================================================================
\begin{appendices}
\section{Appendix}
\subsection{Network Structure}
Here we give the network structures...
\end{appendices}


\end{document}
