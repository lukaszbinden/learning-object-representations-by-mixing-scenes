\documentclass[11pt,a4paper]{article}

\usepackage{cvpr}
\usepackage{float}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{appendix}
\pgfplotsset{compat=newest}
\usepgfplotslibrary{units}
\pgfplotsset{width=10cm,compat=1.9}
\usepgfplotslibrary{external}
\tikzexternalize
\usepackage[backend=bibtex,
style=numeric,
bibencoding=ascii,
sorting=nty,
%style=alphabetic
%style=reading
style=ieee,
citestyle=ieee
]{biblatex}
\addbibresource{bibliography.bib}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{  \tt\raisebox{-.5ex}{\symbol{126}}}}

\begin{document}

%%%%%%%%% TITLE
\title{Learning object representations by mixing scenes}

\author{Lukas Zbinden\\
Computer Vision Group\\
Computer Science Department, University of Bern\\ 
2018\\
{\tt\small lukas.zbinden@unifr.ch}
}

\maketitle
%\thispagestyle{empty}

%=========================================================================
%%%%%%%%% ABSTRACT
\begin{abstract}
   Our project 
   abstract: in 4 sentences: state the problem; say why it's an interesting problem; say what your solution achieves; say what follows from your solution
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
The present thesis aims at devising a novel machine learning model capable of disentangling factors of variation in image data. 


It could be used for fine-grained image search where the search would receive an image showing some objects and it would extract those objects as features in the feature space and then run the search in the feature space space for images having a similar feature i.e. features that are close based on L2 distance and kNN. This search would work without requiring any labels and find images containing similar objects as the query image.


...........................................................................................
- says "I am going to look at the following things"
- introduce area of study - representation learning, disentangling factors of variation
- Describe the problem, state your contributions and the implications
- say what is really new in your work
- tell what your work is about, what problem it solves, why the problem is interesting, what is really new in your work, why it's so neat
- set the proper context for understanding the rest of your paper!
- introduce your idea using EXAMPLES and only then present the general case, explain it as if you were speaking to someone using a whiteboard;


disentangling image features into semantically meaningful properties

"a separate set of dimensions" = a feature chunk

Devise a manifold learning algorithm that can discover and capture independent factors of variation.

\par Concretely, we used a ... to model the distribution over image features and the latent factors of variation.

% - from [12]
Representation learning: Supervised algorithms approach this problem by learning features which transform the data into a space where different classes are linearly separable. However this often comes at the cost of discarding other variations such as style or pose that may be important for more general tasks. On the other hand, unsupervised learning algorithms such as autoencoders seek efficient representations of the data such that the input can be fully reconstructed, implying that the latent representation preserves all factors of variation in the data. However, without some explicit means for factoring apart the different sources of variation the factors relevant for a specific task such as categorization will be entangled with other factors across the latent variables. 

% -- from [26]
While unsupervised learning is ill-posed because the relevant downstream tasks are unknown at training time, a disentangled representation, one which explicitly represents the salient attributes of a data instance, should be helpful for the relevant but unknown tasks. For example, for a dataset of faces, a useful disentangled representation may allocate a separate set of dimensions for each of the following attributes: facial expression, eye color, hairstyle, presence or absence of eyeglasses, and the identity of the corresponding person. A disentangled representation can be useful for natural tasks that require knowledge of the salient attributes of the data, which include tasks like face recognition and object recognition.








%=========================================================================
\section{Related work}
This work builds on the results of \cite{1711.07410} and \cite{1711.02245}.
\cite{1711.02245} identifies two main challenges in disentangling factors of variation: the shortcut problem and the reference ambiguity. In the shortcut problem the model learns degenerate encodings in which all information is encoded only in one part of the feature, i.e. either in the component $c$ for common attributes shared by both images or the component $v$ for varying attributes. For an image pair there are two $v$ and one $c$. The encoder maps a complete description of its input into vector $N_v$ and the decoder completely ignores vector $N_c$.
\newline The second challenge is named reference ambiguity where the attribute representation for an image is not guaranteed to follow the same interpretation on another image. In other words, the reference in which a factor is interpreted may depend on other factors which makes the attribute transfer ambiguous. For example, the viewpoint angle of a vessel gets interpreted in two different ways depending on the boat type.
%-- [16] start -------------------------------
\par[Understanding Degeneracies and Ambiguities in Attribute Transfer] addresses these challenges. They introduce an adversarial weakly supervised training method that uses image triplets and fully tackles the shortcut problem by means of a composite loss function consisting of an autoencoder loss and an adversarial (i.e. GAN) loss. Used in conjunction they provably avoid that challenge. 
Furthermore, they analyze the reference ambiguity and prove that it is unavoidable when disentangling with only weak labels. The problem occurs when a decoder reproduces the data without satisfying the disentangling properties for the varying attribute $v$. Practically, this means not all the factors of variation can provably be disentangled from weakly labeled data (i.e. when only $c$ is known for image pairs).    
%-- [16] end -------------------------------

%-- from [12]
In previous approaches, content vs style, form vs motion, facial expression vs identity were explored.

%-- [15] start -------------------------------
\par\cite{1206.5538} elaborates on representation learning whose goal is to use unlabelled data to learn a representation that exposes important semantic features as easily decodable factors. In 3.5 they distinguish between learning invariant features and learning to disentangle explanatory factors. They conclude that the most robust approach to feature learning is to disentangle as many factors as possible, discarding as little information about the data as is practical. Doing so they propose should give rise to a good representation significantly more robust to the complex and richly structured variations extant in natural images for AI-related tasks. The manifold hypothesis is introduced which makes the assumption that the data lies along a low-dimensional manifold where the probability mass is highly concentrated. They argue that a representation being learned can be associated with an intrinsic coordinate system (on the embedded lower-dimensional manifold). For instance, this can be demonstrated well by a variational autoencoder.
%-- [15] end -------------------------------

%-- [27] start (nicht sicher ob inkludieren soll) ------------------------
[Semi-supervised learning with deep generative models] utilizes a variational autoencoder in a semi-supervised learning paradigm which is capable of separating content and style in data.
%-- [27] end -------------------------------

%-- [12] Discovering Hidden FoV in DN -- start -------------------
\par\cite{1412.6583} augments autoencoders with regularization terms during training. They use an unsupervised cross-covariance penalty (XCov) as a method to disentangle class-relevant signals (observed variables) from other factors in the latent variables along with a standard supervised cross-entropy loss. In case of MNIST they consider the class label as a high-level representation of its corresponding input. Their model learns a class invariant smooth continuous latent representation $z$ that encodes digit style (slant) whereas the observed variable $\hat{y}$ represents the digit itself. On the TFD dataset, the observed variable $\hat{y}$ encodes the facial expression while the autoencoder is able to retain the identity of the faces through latent variable $z$. Here the XCov penalty prevents expression label variation from ‘leaking’ into the latent representation.
%-- [12] end -------------------------------

%-- [20] start -------------------------------
\par\cite{1701.00160} tutors GAN in detail. 
%-- [20] end -------------------------------

%-- [24] DCGAN start -------------------------------
\par\cite{1511.06434} introduces a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate for the first time that they are a strong candidate for purely unsupervised learning. The authors propose a more stable set of architectures for training generative adversarial networks and give evidence that adversarial networks learn good representations of images. According to Ian Goodfellow, DCGAN defines a quasi-standard generator network architecture. 
%-- [24] DCGAN end -------------------------------

%-- [26] INFOGAN start -------------------------------
\par\cite{1606.03657} describes InfoGAN, a type of GAN that learns disentangled representations in an unsupervised manner (no supervision of any kind) with the addition of maximizing the mutual information between a small subset of the latent variables and the observation. InfoGAN can disentangle both discrete and continuous latent factors, scale to complicated datasets, and typically requires no more training time than regular GANs.  The effective results suggest that generative modelling augmented with a mutual information cost be a fruitful approach for learning disentangled representations. They used DCGAN for implementation and stable GAN training, respectively. Gist: concat noise vector $z$ with latent vector $c$ to form input for generator G. Additionally, extend discriminator D with Q-network on top to generate $\hat{c}$ to then calculate the reconstruction loss with $c$ and train entire network accordingly using backprob (aside the standard GAN training).
%-- [26] INFOGAN end -------------------------------

%-- [32] SAGAN start -------------------------------
\par \cite{1805.08318} introduces self-attention to the GAN framework, enabling both the generator and the discriminator to efficiently model relationships between widely separated spatial regions. Convolution processes the information in a local neighborhood only, thus using convolutional layers alone is computationally inefficient for modeling long-range dependencies in images.
%-- [32] SAGAN end -------------------------------

%-- CNN start -------------------------------
\par CNNs: As most of parameters exist in fully connected (FC) layers
%-- CNN end -------------------------------

\par Many prior works on DFoV are fully supervised, i.e. they use labels for all factors of variation to be disentangled.

%=========================================================================
% Problem statement
\section{Disentangling factors of variation}
\par the ULTIMATE OBJECTIVE of this is: we want to encode an image and get a feature representation that has such well disentangled features (2D) and attributes (3D) that it makes it possible to take this representation and move features or attributes around within in and therefore move or change objects in the feature space (manifold?)

\par image attribute - factor of variation - feature vector - feature chunk
an object representation is learned as a concatenation of feature chunks
uses high-dimensional feature chunks
feature space is only designed for attribute transfer but not for sampling
how is disentanglement achieved? 
-> invariance objective i.e. an architecture with mixing autoencoders encourages disentangled encoding of attributes and disentangled decoding of feature chunks, respectively
-> classification objective such that each feature chunk corresponds to a discernible attribute in the original image 
encoding of an attribute, decoding of a feature chunk

\par\cite{1412.6583} states in 2014 that there is a lack of standard benchmark tasks for evaluating disentangling performance. Their evaluation is based on examining qualitatively what factors of variation are discovered for different datasets. ALS INSPIRATION: In [12]: "To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with zni fixed to 0 and y fixed to one-hot vectors corresponding to each class label. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation". Auch für meinen Autoencoder machen um Beispiele aufzuzeigen?

\par "A disentangled representation is generally described as one which separates the factors of variation, explicitly representing the important attributes of the data" [a framework for the quantitative evaluation of disentangled representations]

\par image space vs feature space paper [42]

\par This work pushes the idea of \cite{1711.07410} further.

%=========================================================================
% Method
\section{Learning object representations by mixing scenes}
the intent of the system, the basic motivation for all of this, is that we would like to be able to move objects around by editing the feature space. the purpose in the end is to build a representation where I take a feature from one and plug it there and then the rendering adapts the content - that's the ultimate goal.
\par image attribute - factor of variation - feature vector - feature chunk
\par an object representation is learned as a concatenation of feature chunks

\par LEARN A MODEL THAT IDENTIFIES OBJECTS IN THE SCENE AND LEARNS TO COMPOSE THEM SO YOU CAN MERGE SCENES.

\par https://arxiv.org/pdf/1803.06414.pdf "Learning to Segment via Cut-and-Paste": We propose and formalize a new cut-and-paste adversarial training scheme for box-supervised instance segmentation, which captures an intuitive prior, that objects can move independently of their background.
-> perhaps use latter sentence as idea for the intuition that we're trying to "move objects around independently of their background"

\par what is the theoretical groundwork for the proposed method?

\par We use a GAN to check for realism of the generated images.

\par This subsection explicates the reason ...

\par A baseline could also be pure chance s.a. 50 percent chance..

\par an encoder for computing a low-dimensional latent representation, and a decoder for synthesizing the output image.

how do I enforce invariance between object representations and object attribute representations, respectively?
x3 should be a valid image according to the input data distribution.

The classifier consists of $3x3x8$ multi-class classifiers, one for each chunk, that decide for each chunk the original image ($x_1$, $x_2$) and the specific tile, respectively, that was used to generate a tile of the composite image.

(Hinweis: [12] has nice formulation of cost function, perhaps use as inspiration)

\par To generate the unbalanced mask $m$ with a bias towards the first of the two contexts, we used a Bernoulli distribution with $p=0.6$ and $q=1-p=0.4$, respectively, such that $Pr(X=1)=0.6$ and $Pr(X=0)=1-P(X=1)=0.4$ where $1$ represents the first context (i.e. image $x_1$) and $0$ the second context (i.e. image $x_2$).

\par During training of modified DCGAN encountered lack of GAN training, i.e. the generator loss was immediately 0, the discriminator loss 27.6 ($dsc_loss_real$ was 0 and $dsc_loss_fake$ was 27.6 (meaning the discriminator always output 1 no matter the input). To counteract this problem we implemented and used spectral normalization for GANs (also SNGAN, \cite{1802.05957}) to regularize the Lipschitz constant. With the latter in place, the GAN training set in. We first added to the discrminator, and later according to TTUR paper also in the autoencoder. Note that spectral normalization does not use extra weights, therefore comes at no memory cost.
Important: also mention link to Lipschitz continuity: "They applied spectral normalization to the weights in both generator and discriminator, unlike the previous paper which only normalizes the discriminator weights. They set the spectral norm to 1 to constrain the Lipschitz constant of the weights. It’s just used for controlling the gradients. This spectral normalization idea was first introduced by Miyato et. al." https://towardsdatascience.com/not-just-another-gan-paper-sagan-96e649f01a6b

\par In high dimensional spaces, embeddings?

\par Some methods are based on new mathematical models and others based on intuition back up by experiments.

\par In \cite{1611.03383} Vielleicht als Inspiration für 'Qualitativ evaluation': "we performed nearest neighbor retrieval in the learned embedding spaces... We computed the corresponding representations for all samples (for the unspecified component we used the mean of the approximate posterior distribution) and then retrieved the nearest neighbors for a given query image"

\par We used the pretrained AlexNet model from the work of DeepCluster (\cite{1807.05520}) to cluster the dataset into groups of similar images.

\subsection{Architecture}
Training Procedure: Our models are implemented in TensorFlow \cite{1605.08695} and are trained using a batch size of 4 instances for the generator and 8 instances for the...
Cf. paper https://arxiv.org/pdf/1803.06414.pdf

\par A search on Google (21.11.18) for standard autoencoder architectures trained on MS COCO dataset did not yield any useful results. The idea was to draw inspiration from previous work and to ensure as much as possible that the autoencoder architecture would be a strong fit to synthesize images following the distribution of the MS COCO dataset.

\par The rationale behind the use of dense blocks is that multiple stacked CONV layers can develop more complex features of the input volume (before the destructive pooling operation). The core idea of dense blocks is that for each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. A small growth rate $k$ is sufficient to achieve good results. One explanation for this is that each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth
rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
Cf. http://cs231n.github.io/convolutional-networks/ and DenseNet paper.

\par INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC
Here we see two CONV layers stacked before every POOL layer. This is generally a good
idea for larger and deeper networks, because multiple stacked CONV layers can develop
more complex features of the input volume before the destructive pooling operation.

%=========================================================================
\section{Experimental Results}
We first evaluate the performance of our learned representations on different
transfer learning benchmarks. We then perform ablation studies on our proposed
method. We also visualize the neurons of the intermediate layers of our network.
We experimentally show...

\subsection{Experiments}
An online serarch regarding self-attention layer (from SAGAN paper) and autoencoder does not yield any results.

\subsection{Findings in experiments}
21.11.18: Considering exp10 and exp11, it seems that the results of exp11 (i.e. without instance normalization) suggest that the use of instance norm yields better results (exp10) in that the output is not polluted with recurring artifacts/textures.

\par For so and so We use the publicly available code of Kraehenbuehl cf [43]

\par Oftentimes changes in hyperparameters only have a marginal effect on the results.

\subsection{Datasets}
\subsubsection{Places}
A 10 million Image Database for Scene Recognition. The Places dataset contains significantly more complex factors of variation than MNIST.

\subsubsection{Microsoft COCO}
On average the Microsoft COCO dataset \cite{1405.0312} contains 3.5 categories and 7.7 instances per image. Another interesting observation is only 10\% of the images in MS COCO have only one category per image.
validation set: total: 5000, greaterThan300: 4916, moreThan4: 2430
training set: total: 118287, greaterThan300: 116510, moreThan4: 57180
For data augmentation we use random crops four times per image, resulting in a dataset size of 228720.

\subsection{Training}
We train the model to maximize the data log-likelihood using SGD.
Training strategies for disentangling:...
The training with ... did not yield well-disentangled features as the generated images were blurry and not representing objects and sceneries well. 


\subsection{Model Performance}
We evaluated the model using the MS Coco dataset first.
We hypothesize...
We find empirically that...
Thus, both quantitative and qualitative results demonstrate...
We found that the model is not sensitive to this hyperparameter
The model achieves strong performance in ...

\par Paper https://arxiv.org/pdf/1806.05575.pdf says "The evaluation metric used for the hyperparameter search was the Frechet Inception Distance (FID)"

\subsection{Qualitative and quantitative evaluation}
See paper [a framework for the quantitative evaluation of disentangled representations]: "visual inspection remains the standard evaluation metric"; "current research generally lacks a clear metric for quantitatively evaluating and comparing disentangled representations."
Evtl. "Disentanglement metric score" verwenden aus Paper 41 für Comparison mit anderen Models?

\subsubsection{Qualitative Evaluation}
Qualitative ...

\subsubsection{Quantitative Evaluation}
For quantitative assessment of generated examples, we used inception score (Salimans et al., 2016)
and Frechet inception distance (FID) (Heusel et al., 2017). See paper [31]


\subsection{Ablation Study}
cf. e.g. [44]

%=========================================================================
\section{Conclusion}
None of our image preprocessing attempts
We show ...

\section{Future work}
In general, run the experiments

\printbibliography

%=========================================================================
\begin{appendices}
\section{Appendix}
The contents...
\end{appendices}


\end{document}
