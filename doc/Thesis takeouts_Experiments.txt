%-----------------------------------------------------------
% Training
\subsection{Training}
We train the model to maximize the data log-likelihood using SGD.
Training strategies for disentangling:...
The training with ... did not yield well-disentangled features as the generated images were blurry and not representing objects and sceneries well. 

\par Training Procedure: Our models are implemented in TensorFlow \cite{1605.08695} and are trained using a batch size of 4 instances for the generator and 8 instances for the...
Cf. paper https://arxiv.org/pdf/1803.06414.pdf

\par How do we find a good set of hyperparameters? sequential optimization (hand tuning) by running a series of experiments that build on top of each other. https://vimeo.com/250399261

\subsubsection{Training convergence}
TODO / IDEA: See [50] appendix D für ein Beispiel dh verwende FID zum Aufzeigen der Training convergence.


\subsubsection{Hyperparameters and training details}
As noted in \cite{GANLandscape}, the search space for optimal GAN architectures is humongous: exploring all combinations of all losses, different normalization and regularization schemes, and the plethora of possible architectures is outside of the feasible realm. We therefore take a more pragmatic approach where we conduct a number of experiments with intuitive reasoning for hyperparameter values and combinations thereof (e.g. according to other papers, experiences from fellow lab researchers).

\textbf{Hyperparameter search} As \cite{SpatialBDecoder} and \cite{ChallCmonAssInUnLearOfDR} confirm, finding good parameter sets for hyperparameter-sensitive networks is still a very challenging and time consuming problem. Similar to \cite{AreGANsEqual}, we perform hyperparameter optimization and, for each experiment, look for the best FID across the training run (simulating early stopping). To choose the best model, every epoch we compute the FID between the 10k samples generated by the model and the 10k samples from the test set.

We choose $\beta_1 = 0.5$ and $\beta_2 = 0.999$ as recommended in \cite{GANLandscape}.

% ---------------- nur Notizen START
We use the two learning rate rule for GAN \cite{TTUR} with $0.0005$ for the generator and $0.0002$ for the discriminator.
Beispiel: We use the same feature map counts in our convolution layers as Karras \textit{et al.} [26]. We use batch normalization [25], spectral normalization [38] and attention mechanism [55].
We initialize all weights of the convolutional, fully-connected, and affine transform layers using $N(0, 1)$. The biases are initialized to zero?.
We do not use dropout.
Batch normalization: [51] It normalizes the pre-activations of nodes in a layer to mean $\beta$ and standard deviation $\gamma$, where both $\beta$ and $\gamma$ are parameters learned for each node in the layer.
% ---------------- nur Notizen END

%-----------------------------------------------------------
% Quantitative Evaluation
\subsection{Quantitative Evaluation}
Beispiel aus [50]: Figure 9 shows how the FID and perceptual path length metrics evolve during the training of our configurations B and F with the FFHQ dataset. With R1 regularization active in both configurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images.

Beispiel aus [31]: We computed the Fréchet inception distance between the true distribution and the generated
distribution empirically over 10000 and 5000 samples.

Here we calculate for the generated test images Inception Scores (IS) and FID. The FID is computed as in 
\subsubsection{Evaluation metrics}
For quantitative assessment of generated examples, we used inception score (Salimans \textit{et al.}, 2016)
and Frechet inception distance (FID) (Heusel \textit{et al.}, 2017). See paper [31]. The inception score measures the “objectness” of a generated image (cf. [30]). This metric allows us to avoid relying on human evaluations. Higher Inception score indicates better image quality. We include the Inception score because it is widely used and thus makes it possible to compare our results to previous work. FID measures the sample quality and diversity. FID calculates the Wasserstein-2 distance between the generated images and the real images in the feature space of an Inception-v3 network (cf [32], [33]). Lower FID values mean closer distances between synthetic and real data distributions. The FID is a distance, while the Inception Score is a score. In contrast to FID, inception score is
measured on the set of generated images independently from the original images.
FID implementation taken from https://github.com/bioinf-jku/TTUR.

Idee: Diagram mit IS vs Training iteration erstellen -> siehe paper [31] Figure 15b

Aus https://arxiv.org/pdf/1802.03446.pdf: It has been shown that FID is consistent with human judgments and is more robust to noise than IS [37] (e.g. negative correlation between the FID and visual quality of generated samples)

Aus https://arxiv.org/pdf/1802.03446.pdf: According to this paper, FID and IS do not specifically evalute disentanglement in latent space.

From paper [33]: A well-performing approach to measure the performance of GANs is the
“Inception Score” which correlates with human judgment [53]. Generated samples are fed into an
inception model that was trained on ImageNet. Images with meaningful objects are supposed to
have low label (output) entropy, that is, they belong to few object classes. On the other hand, the
entropy across images should be high, that is, the variance over the images should be large. Drawback
of the Inception Score is that the statistics of real world samples are not used and compared to the
statistics of synthetic samples.
Siehe: For formula of IS see (8) in [33].
    
Some sample FID and IS values here: https://nealjean.com/ml/frechet-inception-distance/    
    
\par 06.03: wichtig zu erwähnen: die datasets für die FID berechnung sollten keine data augmentation verwenden.. hatte dies zuerst so gemacht (mit crop) vgl. te-v1, dann aber te-v2 und tr-v6 erstellt 
    
\par \textbf{PSNR} The proposal is that the higher the PSNR, the better degraded image has been reconstructed to match the original image and the better the reconstructive algorithm.
http://www.ni.com/white-paper/13306/en/
    
\par From [54]: when evaluating disentangled representations we both employ standard metrics [Kim and Mnih, 2017, Chen \textit{et al.}, 2018, Locatello \textit{et al.}, 2018] and (in view of their limitations) emphasize visualization analysis    
    
\subsubsection{FID/IS}    
TBD...
    
%-----------------------------------------------------------
% Qualitative Evaluation
\subsection{Qualitative Evaluation}
We show examples of images generated by our method in Figs. 3 and 4.
TBD... show examples of different experiments/methods

\par Perhaps to a system diagram again with sample images as in [55] figure 2 (b)

interpolation in latent/feature space
"latent space interpolations"
"Here we show interpolations both within and across object categories. We observe that for both cases walking over the latent space gives smooth transitions between objects." Cf. 3dgan.csail.mit.edu/papers/3dgan\_nips.pdf
 
\subsection{Ablation Study}
cf. e.g. [44] oder appendix D in [51].

TODO perhaps show examples like in [1] Figure 3: just MIX, MIX+G, MIX+C, MIX+G+C







%-----------------------------------------------------------
% Discussion
\subsection{Discussion}
% The discussion should start with a paragraph which succinctly states your motivation, and the conclusions that you draw from your data that do not require discussion and ends with an introduction to the weaker conclusions that you would like to draw, but do require discussion. This paragraph should not be a rehash of your results.
% The following paragraphs should discuss the conclusions that you would like to draw from your data, but require discussion because the inferences are indirect, or your data or the data in the literature are contradictory. The best way to neutralize the concerns of critical reviewers is to acknowledge the holes in your arguments, and propose experiments for future work which will place these arguments on stronger experimental footing. Describing experimental tests which will falsify new hypotheses that you draw from your data is the most rigorous means of presenting a new hypothesis.
% The last paragraph should summarize all major conclusions from the discussion of your results and indicate future research directions


-----------------------------------------------------------------------------------------
\newline

\subsection{Experiments}
We now experimentally evaluate the method. An online serarch regarding self-attention layer (from SAGAN paper) and autoencoder does not yield any results.

Idea: do a "nearest neighbor classification" as in paper [1] table 2.

%-- [12] Experiment Idee -------------------------------
\par\cite{DiscHiddenFoViDN} ALS INSPIRATION: In [12]: "To visualize the transformations that the latent variables are learning, the decoder can be used to create images for different values of z. We vary a single element zi linearly over a set interval with zni fixed to 0 and y fixed to one-hot vectors corresponding to each class label. Moving across each column for a given row, the digit style is maintained as the class labels varies. This suggests the network has learned a class invariant latent representation". Auch fuer meinen Autoencoder machen um Beispiele aufzuzeigen?
%-- [12] -----------------------------------------------

\subsubsection{Experiments using LORBMS model on downstream tasks}
See paper [63] for some ideas... "We also studied the effect of improved interpolation on downstream tasks, and showed that ACAI led to improved performance for feature learning and unsupervised clustering"

\subsubsection{Traversing image manifolds}
Idea see paper [25]: Once you have a trained model, you can do traversing of image manifolds.

\subsection{Findings in experiments}
\paragraph{Exp10 vs Exp11} 21.11.18: Considering exp10 and exp11, it seems that the results of exp11 (i.e. without instance normalization) suggest that the use of instance norm yields better results (exp10) in that the output is not polluted with recurring artifacts/textures.

\paragraph{Exp60 vs Exp62} 20.02.19: exp62 with 2-img-CLS achieves better FID in first 50 epochs! Exp60 deteriorates after e=30
% cf. https://docs.google.com/document/d/1wWYBJTLVzWeR9h27UrUB7qrNzHgkUHUzoJjyZyYL-gY/edit

\paragraph{Exp62 vs Exp67} 20.02.19: Analyze impact of CoordConv layer in ENC + DSC: TBD

\par For so and so We use the publicly available code of Kraehenbuehl cf [43]

\par Oftentimes changes in hyperparameters only have a marginal effect on the results.

\subsection{Results on MS COCO}
(siehe [31] für eine coole Grafik Figure 1 für FID score und verschiedenen Hyperparametern..., auch Table 2)
Samples from both models are provided in Figure 13.

\subsection{Model Performance}
We evaluated the model using the MS Coco dataset first.
We hypothesize...
We find empirically that...
Thus, both quantitative and qualitative results demonstrate...
We found that the model is not sensitive to this hyperparameter
The model achieves strong performance in ...

\par Paper https://arxiv.org/pdf/1806.05575.pdf says "The evaluation metric used for the hyperparameter search was the Frechet Inception Distance (FID)"

\subsection{Qualitative and quantitative evaluation}
See paper [a framework for the quantitative evaluation of disentangled representations]: "visual inspection remains the standard evaluation metric"; "current research generally lacks a clear metric for quantitatively evaluating and comparing disentangled representations."
Evtl. "Disentanglement metric score" verwenden aus Paper 41 für Comparison mit anderen Models?

\par TODO consider as quantitative : (paper [66]): use FCN-score to measure quality of generated images: if the generated images are realistic, classifiers trained on real images will be able to classify the synthesized image correctly as well => could also use it for ablation study